{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "% matplotlib inline\n",
    "% load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_path = \"../predata\"\n",
    "data_paths = os.listdir(parent_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1_0_Lounge_iphone7_10002',\n",
       " '1_0_Lounge_Iphone7_20028',\n",
       " '1_0_Lounge_Iphone7_20029',\n",
       " '1_0_Lounge_Iphone7_20030',\n",
       " '1_0_Lounge_Iphone7_20032',\n",
       " '1_0_Lounge_Iphone7_20033',\n",
       " '2_0_Lounge_Iphone7_20014',\n",
       " '2_0_Lounge_Iphone7_20015',\n",
       " '2_0_Lounge_Iphone7_20016',\n",
       " '2_0_Lounge_Iphone7_20019',\n",
       " '3_0_Lounge_HONOR7iAndroid_00024',\n",
       " '3_0_Lounge_HONOR7iAndroid_00027',\n",
       " '3_0_Lounge_iphone7_10003',\n",
       " '3_0_Lounge_iphone7_10006',\n",
       " '4_0_Lounge_HONOR7iAndroid_00029',\n",
       " '4_0_Lounge_iphone7_10006',\n",
       " '4_0_Lounge_Iphone7_20001']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../predata\\1_0_Lounge_iphone7_10002\\stage1_0000.png\n",
      "../predata\\1_0_Lounge_iphone7_10002\\stage1_0001.png\n",
      "../predata\\1_0_Lounge_iphone7_10002\\stage1_0002.png\n",
      "../predata\\1_0_Lounge_iphone7_10002\\stage1_0003.png\n",
      "../predata\\1_0_Lounge_iphone7_10002\\stage1_0004.png\n",
      "../predata\\1_0_Lounge_iphone7_10002\\stage1_0005.png\n",
      "../predata\\1_0_Lounge_iphone7_10002\\stage1_0006.png\n",
      "../predata\\1_0_Lounge_iphone7_10002\\stage1_0007.png\n",
      "../predata\\1_0_Lounge_iphone7_10002\\stage1_0008.png\n",
      "../predata\\1_0_Lounge_iphone7_10002\\stage1_0009.png\n",
      "../predata\\1_0_Lounge_Iphone7_20028\\stage1_0000.png\n",
      "../predata\\1_0_Lounge_Iphone7_20028\\stage1_0001.png\n",
      "../predata\\1_0_Lounge_Iphone7_20028\\stage1_0002.png\n",
      "../predata\\1_0_Lounge_Iphone7_20028\\stage1_0003.png\n",
      "../predata\\1_0_Lounge_Iphone7_20028\\stage1_0004.png\n",
      "../predata\\1_0_Lounge_Iphone7_20028\\stage1_0005.png\n",
      "../predata\\1_0_Lounge_Iphone7_20028\\stage1_0006.png\n",
      "../predata\\1_0_Lounge_Iphone7_20028\\stage1_0007.png\n",
      "../predata\\1_0_Lounge_Iphone7_20028\\stage1_0008.png\n",
      "../predata\\1_0_Lounge_Iphone7_20028\\stage1_0009.png\n",
      "../predata\\1_0_Lounge_Iphone7_20029\\stage1_0000.png\n",
      "../predata\\1_0_Lounge_Iphone7_20029\\stage1_0001.png\n",
      "../predata\\1_0_Lounge_Iphone7_20029\\stage1_0002.png\n",
      "../predata\\1_0_Lounge_Iphone7_20029\\stage1_0003.png\n",
      "../predata\\1_0_Lounge_Iphone7_20029\\stage1_0004.png\n",
      "../predata\\1_0_Lounge_Iphone7_20029\\stage1_0005.png\n",
      "../predata\\1_0_Lounge_Iphone7_20029\\stage1_0006.png\n",
      "../predata\\1_0_Lounge_Iphone7_20029\\stage1_0007.png\n",
      "../predata\\1_0_Lounge_Iphone7_20029\\stage1_0008.png\n",
      "../predata\\1_0_Lounge_Iphone7_20029\\stage1_0009.png\n",
      "../predata\\1_0_Lounge_Iphone7_20030\\stage1_0000.png\n",
      "../predata\\1_0_Lounge_Iphone7_20030\\stage1_0001.png\n",
      "../predata\\1_0_Lounge_Iphone7_20030\\stage1_0002.png\n",
      "../predata\\1_0_Lounge_Iphone7_20030\\stage1_0003.png\n",
      "../predata\\1_0_Lounge_Iphone7_20030\\stage1_0004.png\n",
      "../predata\\1_0_Lounge_Iphone7_20030\\stage1_0005.png\n",
      "../predata\\1_0_Lounge_Iphone7_20030\\stage1_0006.png\n",
      "../predata\\1_0_Lounge_Iphone7_20030\\stage1_0007.png\n",
      "../predata\\1_0_Lounge_Iphone7_20030\\stage1_0008.png\n",
      "../predata\\1_0_Lounge_Iphone7_20030\\stage1_0009.png\n",
      "../predata\\1_0_Lounge_Iphone7_20032\\stage1_0000.png\n",
      "../predata\\1_0_Lounge_Iphone7_20032\\stage1_0001.png\n",
      "../predata\\1_0_Lounge_Iphone7_20032\\stage1_0002.png\n",
      "../predata\\1_0_Lounge_Iphone7_20032\\stage1_0003.png\n",
      "../predata\\1_0_Lounge_Iphone7_20032\\stage1_0004.png\n",
      "../predata\\1_0_Lounge_Iphone7_20032\\stage1_0005.png\n",
      "../predata\\1_0_Lounge_Iphone7_20032\\stage1_0006.png\n",
      "../predata\\1_0_Lounge_Iphone7_20032\\stage1_0007.png\n",
      "../predata\\1_0_Lounge_Iphone7_20032\\stage1_0008.png\n",
      "../predata\\1_0_Lounge_Iphone7_20032\\stage1_0009.png\n",
      "../predata\\1_0_Lounge_Iphone7_20033\\stage1_0000.png\n",
      "../predata\\1_0_Lounge_Iphone7_20033\\stage1_0001.png\n",
      "../predata\\1_0_Lounge_Iphone7_20033\\stage1_0002.png\n",
      "../predata\\1_0_Lounge_Iphone7_20033\\stage1_0003.png\n",
      "../predata\\1_0_Lounge_Iphone7_20033\\stage1_0004.png\n",
      "../predata\\1_0_Lounge_Iphone7_20033\\stage1_0005.png\n",
      "../predata\\1_0_Lounge_Iphone7_20033\\stage1_0006.png\n",
      "../predata\\1_0_Lounge_Iphone7_20033\\stage1_0007.png\n",
      "../predata\\1_0_Lounge_Iphone7_20033\\stage1_0008.png\n",
      "../predata\\1_0_Lounge_Iphone7_20033\\stage1_0009.png\n",
      "../predata\\2_0_Lounge_Iphone7_20014\\stage2_0000.png\n",
      "../predata\\2_0_Lounge_Iphone7_20014\\stage2_0001.png\n",
      "../predata\\2_0_Lounge_Iphone7_20014\\stage2_0002.png\n",
      "../predata\\2_0_Lounge_Iphone7_20014\\stage2_0003.png\n",
      "../predata\\2_0_Lounge_Iphone7_20014\\stage2_0004.png\n",
      "../predata\\2_0_Lounge_Iphone7_20014\\stage2_0005.png\n",
      "../predata\\2_0_Lounge_Iphone7_20014\\stage2_0006.png\n",
      "../predata\\2_0_Lounge_Iphone7_20014\\stage2_0007.png\n",
      "../predata\\2_0_Lounge_Iphone7_20014\\stage2_0008.png\n",
      "../predata\\2_0_Lounge_Iphone7_20014\\stage2_0009.png\n",
      "../predata\\2_0_Lounge_Iphone7_20015\\stage2_0000.png\n",
      "../predata\\2_0_Lounge_Iphone7_20015\\stage2_0001.png\n",
      "../predata\\2_0_Lounge_Iphone7_20015\\stage2_0002.png\n",
      "../predata\\2_0_Lounge_Iphone7_20015\\stage2_0003.png\n",
      "../predata\\2_0_Lounge_Iphone7_20015\\stage2_0004.png\n",
      "../predata\\2_0_Lounge_Iphone7_20015\\stage2_0005.png\n",
      "../predata\\2_0_Lounge_Iphone7_20015\\stage2_0006.png\n",
      "../predata\\2_0_Lounge_Iphone7_20015\\stage2_0007.png\n",
      "../predata\\2_0_Lounge_Iphone7_20015\\stage2_0008.png\n",
      "../predata\\2_0_Lounge_Iphone7_20015\\stage2_0009.png\n",
      "../predata\\2_0_Lounge_Iphone7_20016\\stage2_0000.png\n",
      "../predata\\2_0_Lounge_Iphone7_20016\\stage2_0001.png\n",
      "../predata\\2_0_Lounge_Iphone7_20016\\stage2_0002.png\n",
      "../predata\\2_0_Lounge_Iphone7_20016\\stage2_0003.png\n",
      "../predata\\2_0_Lounge_Iphone7_20016\\stage2_0004.png\n",
      "../predata\\2_0_Lounge_Iphone7_20016\\stage2_0005.png\n",
      "../predata\\2_0_Lounge_Iphone7_20016\\stage2_0006.png\n",
      "../predata\\2_0_Lounge_Iphone7_20016\\stage2_0007.png\n",
      "../predata\\2_0_Lounge_Iphone7_20016\\stage2_0008.png\n",
      "../predata\\2_0_Lounge_Iphone7_20016\\stage2_0009.png\n",
      "../predata\\2_0_Lounge_Iphone7_20019\\stage2_0000.png\n",
      "../predata\\2_0_Lounge_Iphone7_20019\\stage2_0001.png\n",
      "../predata\\2_0_Lounge_Iphone7_20019\\stage2_0002.png\n",
      "../predata\\2_0_Lounge_Iphone7_20019\\stage2_0003.png\n",
      "../predata\\2_0_Lounge_Iphone7_20019\\stage2_0004.png\n",
      "../predata\\2_0_Lounge_Iphone7_20019\\stage2_0005.png\n",
      "../predata\\2_0_Lounge_Iphone7_20019\\stage2_0006.png\n",
      "../predata\\2_0_Lounge_Iphone7_20019\\stage2_0007.png\n",
      "../predata\\2_0_Lounge_Iphone7_20019\\stage2_0008.png\n",
      "../predata\\2_0_Lounge_Iphone7_20019\\stage2_0009.png\n",
      "../predata\\3_0_Lounge_HONOR7iAndroid_00024\\stage3_0000.png\n",
      "../predata\\3_0_Lounge_HONOR7iAndroid_00024\\stage3_0001.png\n",
      "../predata\\3_0_Lounge_HONOR7iAndroid_00024\\stage3_0002.png\n",
      "../predata\\3_0_Lounge_HONOR7iAndroid_00024\\stage3_0003.png\n",
      "../predata\\3_0_Lounge_HONOR7iAndroid_00024\\stage3_0004.png\n",
      "../predata\\3_0_Lounge_HONOR7iAndroid_00024\\stage3_0005.png\n",
      "../predata\\3_0_Lounge_HONOR7iAndroid_00024\\stage3_0006.png\n",
      "../predata\\3_0_Lounge_HONOR7iAndroid_00024\\stage3_0007.png\n",
      "../predata\\3_0_Lounge_HONOR7iAndroid_00024\\stage3_0008.png\n",
      "../predata\\3_0_Lounge_HONOR7iAndroid_00024\\stage3_0009.png\n",
      "../predata\\3_0_Lounge_HONOR7iAndroid_00027\\stage3_0000.png\n",
      "../predata\\3_0_Lounge_HONOR7iAndroid_00027\\stage3_0001.png\n",
      "../predata\\3_0_Lounge_HONOR7iAndroid_00027\\stage3_0002.png\n",
      "../predata\\3_0_Lounge_HONOR7iAndroid_00027\\stage3_0003.png\n",
      "../predata\\3_0_Lounge_HONOR7iAndroid_00027\\stage3_0004.png\n",
      "../predata\\3_0_Lounge_HONOR7iAndroid_00027\\stage3_0005.png\n",
      "../predata\\3_0_Lounge_HONOR7iAndroid_00027\\stage3_0006.png\n",
      "../predata\\3_0_Lounge_HONOR7iAndroid_00027\\stage3_0007.png\n",
      "../predata\\3_0_Lounge_HONOR7iAndroid_00027\\stage3_0008.png\n",
      "../predata\\3_0_Lounge_HONOR7iAndroid_00027\\stage3_0009.png\n",
      "../predata\\3_0_Lounge_iphone7_10003\\stage3_0000.png\n",
      "../predata\\3_0_Lounge_iphone7_10003\\stage3_0001.png\n",
      "../predata\\3_0_Lounge_iphone7_10003\\stage3_0002.png\n",
      "../predata\\3_0_Lounge_iphone7_10003\\stage3_0003.png\n",
      "../predata\\3_0_Lounge_iphone7_10003\\stage3_0004.png\n",
      "../predata\\3_0_Lounge_iphone7_10003\\stage3_0005.png\n",
      "../predata\\3_0_Lounge_iphone7_10003\\stage3_0006.png\n",
      "../predata\\3_0_Lounge_iphone7_10003\\stage3_0007.png\n",
      "../predata\\3_0_Lounge_iphone7_10003\\stage3_0008.png\n",
      "../predata\\3_0_Lounge_iphone7_10003\\stage3_0009.png\n",
      "../predata\\3_0_Lounge_iphone7_10006\\stage3_0000.png\n",
      "../predata\\3_0_Lounge_iphone7_10006\\stage3_0001.png\n",
      "../predata\\3_0_Lounge_iphone7_10006\\stage3_0002.png\n",
      "../predata\\3_0_Lounge_iphone7_10006\\stage3_0003.png\n",
      "../predata\\3_0_Lounge_iphone7_10006\\stage3_0004.png\n",
      "../predata\\3_0_Lounge_iphone7_10006\\stage3_0005.png\n",
      "../predata\\3_0_Lounge_iphone7_10006\\stage3_0006.png\n",
      "../predata\\3_0_Lounge_iphone7_10006\\stage3_0007.png\n",
      "../predata\\3_0_Lounge_iphone7_10006\\stage3_0008.png\n",
      "../predata\\3_0_Lounge_iphone7_10006\\stage3_0009.png\n",
      "../predata\\4_0_Lounge_HONOR7iAndroid_00029\\stage4_0000.png\n",
      "../predata\\4_0_Lounge_HONOR7iAndroid_00029\\stage4_0001.png\n",
      "../predata\\4_0_Lounge_HONOR7iAndroid_00029\\stage4_0002.png\n",
      "../predata\\4_0_Lounge_HONOR7iAndroid_00029\\stage4_0003.png\n",
      "../predata\\4_0_Lounge_HONOR7iAndroid_00029\\stage4_0004.png\n",
      "../predata\\4_0_Lounge_HONOR7iAndroid_00029\\stage4_0005.png\n",
      "../predata\\4_0_Lounge_HONOR7iAndroid_00029\\stage4_0006.png\n",
      "../predata\\4_0_Lounge_HONOR7iAndroid_00029\\stage4_0007.png\n",
      "../predata\\4_0_Lounge_HONOR7iAndroid_00029\\stage4_0008.png\n",
      "../predata\\4_0_Lounge_HONOR7iAndroid_00029\\stage4_0009.png\n",
      "../predata\\4_0_Lounge_iphone7_10006\\stage4_0000.png\n",
      "../predata\\4_0_Lounge_iphone7_10006\\stage4_0001.png\n",
      "../predata\\4_0_Lounge_iphone7_10006\\stage4_0002.png\n",
      "../predata\\4_0_Lounge_iphone7_10006\\stage4_0003.png\n",
      "../predata\\4_0_Lounge_iphone7_10006\\stage4_0004.png\n",
      "../predata\\4_0_Lounge_iphone7_10006\\stage4_0005.png\n",
      "../predata\\4_0_Lounge_iphone7_10006\\stage4_0006.png\n",
      "../predata\\4_0_Lounge_iphone7_10006\\stage4_0007.png\n",
      "../predata\\4_0_Lounge_iphone7_10006\\stage4_0008.png\n",
      "../predata\\4_0_Lounge_iphone7_10006\\stage4_0009.png\n",
      "../predata\\4_0_Lounge_Iphone7_20001\\stage4_0000.png\n",
      "../predata\\4_0_Lounge_Iphone7_20001\\stage4_0001.png\n",
      "../predata\\4_0_Lounge_Iphone7_20001\\stage4_0002.png\n",
      "../predata\\4_0_Lounge_Iphone7_20001\\stage4_0003.png\n",
      "../predata\\4_0_Lounge_Iphone7_20001\\stage4_0004.png\n",
      "../predata\\4_0_Lounge_Iphone7_20001\\stage4_0005.png\n",
      "../predata\\4_0_Lounge_Iphone7_20001\\stage4_0006.png\n",
      "../predata\\4_0_Lounge_Iphone7_20001\\stage4_0007.png\n",
      "../predata\\4_0_Lounge_Iphone7_20001\\stage4_0008.png\n",
      "../predata\\4_0_Lounge_Iphone7_20001\\stage4_0009.png\n"
     ]
    }
   ],
   "source": [
    "train_set = []\n",
    "time_length = 10\n",
    "width,height = 64, 64\n",
    "for path in data_paths:\n",
    "    data = np.zeros((time_length,width,height))\n",
    "    label = int(path[0]) - 1\n",
    "    stage_name = \"stage\" + path[0] + \"_\"\n",
    "    for i in range(time_length):\n",
    "        file_name = stage_name + str(i).zfill(4) + \".png\"\n",
    "        file_path = os.path.join(parent_path, path, file_name)\n",
    "        print(file_path)\n",
    "        img = cv2.imread(file_path)[:,:,0]\n",
    "        img = cv2.resize(img, (width, height))\n",
    "        # normalize image\n",
    "        data[i,:,:] = (img-np.mean(img))/np.std(img)\n",
    "    \n",
    "    train_set.append((np.expand_dims(data, axis=3), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 1, 0, 2, 0, 1, 0, 1, 3, 3, 0, 2, 2, 0, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(train_set)\n",
    "tmp = [x[1] for x in train_set]\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label of 10 th sample: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACwsAAARUCAYAAACDGm5TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzs3cmPZVl+H/bzMuaMKSMzK7Omri62\nBsMECMIAYXBvCNBOWtorLwxoYwM24I3+BK0MeOGNYAvkwrA3NiAuBFACZcAQQBgqsSmAg0RWEZXV\nVVmVmRGZMWZEZAzPCxaNtqT7/WXfU6+i+sXns2l2/frce+4Zf/e8W8HJdDptAAAAAAAAAAAAAMD8\nuXPTFQAAAAAAAAAAAAAAZsPHwgAAAAAAAAAAAAAwp3wsDAAAAAAAAAAAAABzysfCAAAAAAAAAAAA\nADCnfCwMAAAAAAAAAAAAAHPKx8IAAAAAAAAAAAAAMKd8LAwAAAAAAAAAAAAAc8rHwgAAAAAAAAAA\nAAAwp3wsDAAAAAAAAAAAAABzarGn8GQy+duttf+xtbbQWvufp9PpP0j/+/v3709/9KMfjbrXdDod\nVa611iaTyej49fX16Pv2uLq6Gl32zp38DXh6pp52vkmp3lUfprLV2Elle/qhqnOqV1Xny8vLwVhV\n54uLixhfWloajFXP9ObNm1HXra69sLAQy469bmt1W49VzcOe8V71cTLLuZSuPcs1LZWt1uE0LtM8\nq+pV3TeVTfc9OztrFxcXsxm0t9AvmgM9ePBgdA7Usxb1rAmzzL3SOK/q3LOu96yf6Zl61u3eayc3\nlcdW0jPNMhdN465nr2mtLzdLOdDiYn5dS89UzZVZ5cA9e/4sx3tPDtSzN/e0ZY+eHKhnHa5yoLF1\nquLpPeX8/FwO9B37RfKgWeZAPXnMTZ0D3dS73Kz23p41v7r3LNfPVK+eZ5rVGUFrfXlMKtvbzune\nVf+nPaPKY3rOEHrmf08f98z/pMpTUlv2zuGees9q/vf0YdWWaWz15ItVndO1U8w50HdvzFnQRx99\nNOpePXlBT9lqLPe8+85q/63W/Z53lKR3DZ3Vvau9O707VWcQPW15U2eQqc69fZj22Krs+fn5YGx5\neTmWTe3Vk0NVbur341m9y/Scm7Q2u/PcWUrzv6pzWh9+iGdB8qDv1nedA83qN61Zlv0hvkfOcg+c\n5fra80xJzxlUz7V7vkXr/W0xuanfw6o6p9/DenKgyqz27VmO2Z7zi1mefffMh5s6C+7JgdI3QWk8\nt9bXhyme6nx6etrevHlTDszRHwtPJpOF1tr/1Fr7W621L1tr/3IymfzOdDr9k6EyP/rRj9rv/u7v\njrpfT6JZveCm+PHxcde1k/RM1X2T1dXVGD87OxtVp7eJ35TqA7kkxau2TPetyqY+ruqcxl01Jvf3\n9wdjVZ2fPXsW448fPx6MVc/0+eefD8bee++9WDZde319PZYde93W6o+Yx6o+yk4bw+npaSy7trY2\nqk5vc+2euZTmQ1XnVK9qzUr3rdbhd999dzC2u7sby6bDwKOjo1g2PVO677/6V/8qXpe3NzYH+qf/\n9J+Oul8aL63ldb9ax9LcnGXudXBwMBir6tyzrlc/pCbpmao+qtpyVoet1TP1fGg69rqt5Weq2rJH\nWterferk5CTGV1ZWBmPVfPjqq68GYw8ePIhlDw8PB2NbW1uxbGrrnh8IU1u0ltuy51+YrPbtjY2N\n0ddOa1ZruT2qtkzztKcfqrZM963GexpbL1++jGWTNJ5by3V+/vz5YOwP//APR9eJf98vmgdVOVDa\nb6qx2JPHpLWqum+PVK/qvj050Kz23ipf6MmRetbAqv9TvXqeqeeMsJLOcqo9LpWt6lz1YRqX1Zje\n29sbjPXkMVUuks5yqvx5Vjlydd+kylO2t7cHY1X/V33YU+9Z5U89Z3lprrSWx3tVNv2AVNU59XHK\ngT/55JN4XX4xY86CPvroo/bP//k/H3W/njPenrLVuWSan9VZeapXz7yv3ufTO0rPv3BRPW/Vlj3S\nvavfFl68eDEY29nZiWVfvXqVKxbc1Blket5q7FTvqOnMpur/zz77bDBW/cuWadxubm7Gsq9fv47x\nsfedpTQ+qvE+9rqt1Wtpzzzs0TNf0tlJlY+88847g7FqbUh1rto5xZ8+fToYcxb03ZlFDpTW7pvK\ngXrmfFV2Vt8TVddNOVCVx8xyfU3X7vn2omc9qdoy7etV2Z79omfNT3lMz7cXrfU9U/o97IMPPohl\nZ5UzzjIXTXraucpTU9mefLC1PB961pZZrkspB6qum76BS+O5tTw+qt/SUjunOv/+7/9+vO5fGf9n\nSVr7T1trn06n07+YTqdvWmv/e2vt73RcDwDgl4EcCAC4reRBAMBtJAcCAG4jORAAzJmej4U/aK39\n7Of++5ff/jMAgHkmBwIAbit5EABwG8mBAIDbSA4EAHOm52PhyX/gn03/vf/RZPL3JpPJJ5PJ5JP0\n/14OAOCXhBwIALityjxIDgQAzKFf+Cxod3f3e6gWAMBMyYEAYM70fCz8ZWvtRz/33z9srT39d/9H\n0+n0H06n09+YTqe/8eDBg47bAQD8IMiBAIDbqsyD5EAAwBz6hc+CHj58+L1VDgBgRuRAADBnej4W\n/pettb8xmUx+ZTKZLLfW/vPW2u98N9UCAPjBkgMBALeVPAgAuI3kQADAbSQHAoA5szi24HQ6vZxM\nJv9Na+13W2sLrbV/NJ1O/ziVmUwmbXFx3C3Pzs5ifHV1dTB2eXk56p5vc9979+4Nxr755ptYdmxb\ntNb3TKlsasfWcnv01KlXzzP11Pv4+Hj0dVP/b2xsxLKpH6oxO7ZOrfWNj8rHH388GKvaMtWreqbz\n8/PBWNUPS0tLg7GLi4tY9uDgYDC2sLAQy1bxJI3ZSvVMPetDauurq6vR9z05OYll19fXB2Opf1vL\nbZnGVWutraysDMZ62jnNhTt3ev79IH7emByotbweLS8vD8aq8ZTWhJ5co5o/aRynNa4qW9U5rQnV\n+pieKa0HVdneHKinfFozqn276uMkjcuqD1O8J3/qeZ5ePc+U/upmtQemudSjqvP29vZgrKcfenKc\natz15ECVNB+qtkxrT7X+p/FRrcP3798fdd3WWnv9+vVgrKpzz9ip1mm+H2PyoDS30zpWjcUe6do9\n75/VWpPKVmt6z7lHml/VfdO8rub8LM/ybvIMakhPHtNznrK/vz/6vtXYqcZ0T717/vJ4msPVu3zK\nn6tcJJ1dVPMhxas9rufsKuUEVf40y3nW866a+v/ly5exbMqBetbS6+vrWLbnedP4SP8vnqfTf+//\nOzQdxuZAOzs7g/FZ/dZSrYPp2tV919bWBmPV/Etlq/0kvYNUczetGZubm7Hs0dHRYKzaI9Pztpb3\nnCoPPj09jfFkVr+l9uwpVdnUD1Wd37x5Mxir+r+S+qF6pr/21/7a6PumZ+rpw2rcPXr0aDCW+qi1\n3FY9OXR131nqmYdp7PW8Bz1//nz0fav5kNbpqi3Stavfw9JammKTySRel7f3fX8TVEl5Ts/8qdaT\nnnGc4j1rYI8qT0l1ruZ8lYv2fF/Rk0+ka/e0R0/ZSk/uPcv3+fRMVf8/fvx4Jvet+iG1R9VH6Ryh\nmg89OVBPH6b3p+q6PTlO1ZbV+EhSvXvOgqrnra49VvUem+JffPHFYOxtz4K6dpfpdPpPWmv/pOca\nAAC/bORAAMBtJQ8CAG4jORAAcBvJgQBgvvgzgwAAAAAAAAAAAAAwp3wsDAAAAAAAAAAAAABzysfC\nAAAAAAAAAAAAADCnfCwMAAAAAAAAAAAAAHPKx8IAAAAAAAAAAAAAMKcWb7oCb+vy8nJ02dXV1dFl\n7927F+P7+/szue/GxkaMHx8fD8bOzs5i2dSW6bqVnuet7l09U7p3NXY+++yzwdiv//qvz+y+Jycn\ng7FXr17Fsu++++5gbHExT+ulpaXB2Pn5eSxbjcukGlsLCwuDsbW1tVh2d3d31HVba217e3sw9uWX\nX8ayaX1I/Vupxntqy54+qu6bxk5ruV6bm5uxbOrDnmeq5lJSrWmpvVZWVmLZWa3DV1dXg7HpdDr6\nusze69evB2PVflKNtyStkevr67FsmgPVfpL2quq+SU++2FO2qnN17dReFxcXsWxam6uyT58+HYz9\n6Ec/imWTaj9Jz3twcBDLPnjwYDDWMxcqPeOymg9J9UzpXSTlOFX8Zz/7WSyb8pxUp9byvl61Vdoj\nqxw4Sftna3U+meJVP6R5mMZ7a33r1suXL0eXTao6pz6s+j/Nh+Xl5cHYnTv+HembNJlMRs/Paoz3\nzPs0b6v3gFSvnhyo50ylaqsUr9a4nv21epdLOUNP/1dlnzx5Mhj7yU9+EssmPedxPWtgTzv3nLdV\nqlw0qc4fUr2rOqf2+uKLL3LFgsPDwxhPfViVrfop6Zkr1Trbcz5V5f1Jastqzeppy6S6b885YVof\nUh86B7p5l5eX7cWLF4PxNMd61tCqbDprr9bQnnol1X1TvHq3SblOOo+rPHr0KMara6f1qDrT3tnZ\nGYxVa/u//tf/ejBW/R6Wxuzp6Wksm+p1dHQUy6bfFqrfjpJqPG9tbcV4eqaqD1NbVs+Uzl2qcXn3\n7t3BWJUH9ZxB9Kwd6dyk+t2pZ8xW107re1W255mSqo968qDUXtWY7Zn/aez0jEl+ec0qF7l//36M\nV2vGWNWcT+O8Z++t3kGTnr23tVzv6pnSvXvOgn7t134tlk05QZWLpjpXZdM+VuUpSdVW1ZlMMsux\nVe0ZSWrLvb29WDa1RzVm03173hd68oVKz75enSOlPqzK9rw/p7lW3Tddu2qrND6qvCy1Vbru9fV1\nvO5f8asZAAAAAAAAAAAAAMwpHwsDAAAAAAAAAAAAwJzysTAAAAAAAAAAAAAAzCkfCwMAAAAAAAAA\nAADAnPKxMAAAAAAAAAAAAADMKR8LAwAAAAAAAAAAAMCc8rEwAAAAAAAAAAAAAMypxZuuwNt6+PBh\njF9eXg7Gzs7OYtnFxeFmSLEqvrGxEcvu7u4Oxvb392PZ1dXVwVjP81ZSO6fY28STnn6o7vvrv/7r\ng7GqH3qe6fT0dDD27rvvxrLn5+ej77uwsDAYu7q6Gn3d1vIzbW9vx7Jp3L569SqWXV9fzxUbqer/\nzc3NwdjFxUUsm+ZwVXZnZ2cwlvqgtdzHS0tLsWwlrXnVM6Wy1ZqWnqlah9O4/Oabb2LZ1A/VmE3P\ndHx8HMum9S71//X1dbwuNyutCdUe2LMnpPlTrdsnJyeDsUePHsWyaQ5Uz7OysjIY69nHqj097TWp\nLX7IPv7448FYT1tWfZj6vxo7qa1nleP2lk9jtipbja0PPvhgdNl03729vVg2zYcqn0j37ckXevbP\n3hw4Xbvqh573usPDw8FYNabff//9wdjz589j2bt37w7GXrx4Ecumd5Gqzqk9Xr9+PRiTA92syWQS\n+z158OBBjB8cHAzGqvGU5u3jx49j2c8++2x02bRWVevYrM6BetbAqm+rNbBn7+7xq7/6q4OxNK4q\n1fOkM4Z79+6Nvm/V/0k1V6rxkcZA9T7ekwOlXKTKJ1Kde9qymg89Z5c97yLp2r1zMK1L1ZqWnqlq\nj2fPnuWKBWmuVeeAqc5V2dTWVe7NL687d+7EeZKsra2Nvm+1Dib379+P8U8//XQwVuVBR0dHg7Ge\n3C29n7SW3xWq99e0D6brtta3p1RnIz19/Ju/+ZuDseq3hdQPs3zeNHaqOifpnbq1vrGVfjtoLY/5\n6pl68saeM5meXKbHRx99NPq+L1++HIxV87/6jSdJY7a1PD6qZ0r1qvaO9JtmaqvWcp2rs6CkGnc9\naws/XD2/R89qvanmT9ozqrI939ck1X178oWktw9S/1djo+eZfu3Xfm30dVO85726yr3TuKvyhVn1\nf2t5DGxtbcWyqd5VbtbzjpTqXO3b1Rn12PtWuWZqy6r/e+ZpVbYn/+7JJ9OYrsZGassqB0rz9MmT\nJ7FsqlfPHP4ufg/zl4UBAAAAAAAAAAAAYE75WBgAAAAAAAAAAAAA5pSPhQEAAAAAAAAAAABgTvlY\nGAAAAAAAAAAAAADmlI+FAQAAAAAAAAAAAGBO+VgYAAAAAAAAAAAAAObU4k1X4G0dHx/H+OLi8KNc\nXl7Gsqurq6Pvm+JnZ2ex7L179wZju7u7sWy6dnqet6nXWFU7V/GesumZ0thorbX9/f3R903Xrvow\n9X9PW/VYWVmJ8Z56VePu4OBgMLa+vh7LXl1dDcYuLi5Gl33nnXdi2fRMS0tLo+9blX3x4sWoOrWW\n14c0Jlvr6/+jo6MYf/jw4WAsjY3K5uZmjKdrr62txbLVMyXvvffeYOybb76JZVM/pP6fTqd1xZip\n1HdpPzk/P5/JPStPnz6N8YWFhcHYyclJLJvW9Wod62mPHmm9qHKNnn6o9oRqn0vevHkzGOup897e\nXow/ePBgMDbL8Z76qTd/rsZAkp65um6aaz19+OjRo9H3TWtDpRrvqR+qdSf18d27d2PZw8PDGE+q\nMX3//v3BWNWH6Zmr/Pn58+eDseqdILVHVXZra2swVu07aT6ktkh5N7M3nU5jH6Rx3jP3etb1zz77\nbPR9nz17FuPb29uDsarOSbVf9MyDWeZeszqfqqQ1oyevq95dNzY2BmM97/I9e3417qr2SPXuOcur\n6pVy4GrMpvnw/vvvjy7bM4d75mh1dp3q1dO/Vfnq2mP39db62jq9q1R57MuXLwdjVQ6UztyePHkS\ny4793cM50M2bTqejz4KqNbTnLCBdu8rJ0ztbmiOt5XPa6nlPT09jPElrbPUuOMtcJbVl1b+pvaq2\n7DnT7hl3qf+r6/acuSRV/1eWl5dHl53VmK6um8ZHOp9oLe+T1R7aM3bS+UU1ntO4q3476qlz1Q/p\n3tXZWOrDqs7VOp28evVqMFbN0XTOWOVBqT3Sb6XOgm7WZDIpx/KQWX4jkcZqdQbVs25XvzePve/Y\nNn4bqR+q9bMn1+hp52oNTNfuGXdVnVM/9ZStnjeVneVvmj375yx/D01t/fHHH48u29OWr1+/Hl22\nJ+fb2dmJZatrpzWtZw5XbdnT/2mN71nTqrLp97Dqd4RZrvGt+cvCAAAAAAAAAAAAADC3fCwMAAAA\nAAAAAAAAAHPKx8IAAAAAAAAAAAAAMKd8LAwAAAAAAAAAAAAAc8rHwgAAAAAAAAAAAAAwp3wsDAAA\nAAAAAAAAAABzysfCAAAAAAAAAAAAADCnFm+6Aj/v8vJydNn9/f3B2MbGRiz75Zdfjq7Tw4cPB2Nn\nZ2exbLr24mLumnTt6nmT1dXVGE/tXNW5unZ6pqrs8fHxYGyW7dEzZpOqLa+urkZfuxqXydLSUoyv\nra0Nxi4uLmLZ7e3tUXVqrbWTk5PR903tce/evVg29cPu7m4s+/jx49Fl33333cFYT/9+/vnnMf7h\nhx/G+KtXrwZj1Vyqnnmso6OjGK/qNdb5+XmMp32nkuqc1uiedYPZS+tYtdf07J/p2tV919fXR9Wp\niqe2qFTPu7KyMirWWp5DVVtV1055TCXVa2FhYfR1qzqndS6Njdbynt+TP/fo2T9b68vrU7zqh3Tf\nai719GGyt7cX4w8ePBiMpX2stb58Mc2Hly9fdt03la/68NmzZ4OxKvdO9Xr+/HksW43LsWWrOfqz\nn/1s9H3TPJ3V2kC/6+vruB71jKeecZxUuXxax3rqVO1Fac+v1u30TFW+0JMvVu3RM3dT2SonTO3R\nkxNUa346n6rywVSvqp3T8/bk3pWqPdI+V53lJFV7pGfe2toafd+qLXuu3ZObJSkPaS2fXbVW525J\nGtPVujT2XKS6dpV7pTFdjdnPPvtsMNazd7x582YwNp1OR1+X78b19XU7PT0djPecpfecBaX3l+q+\naV1Iz1rdt+fccmdnJ8Z79pSkqnPP72GVnjOIVK9Z5Wattba5uTkYq97Je94ZZvmu2FOvtB9VYytd\nuxrvad2ppHql34Zay7+lV3v3o0ePBmPV3p36KO3N1X1by3O4auc05tNcqVS/hyVVWybVeP+3//bf\njr52mitp37m+vh59T/pVOVBS7WNpnFdlDw8PR9Wptdbu378/GKuetWfdTs+b6lSp3k/TOlXt29Ua\n2NOHqS2r+6ay1RqY+rhnL6rq3HPfnhx4Vvlza7PL63pyvp78qSrb876f5mnPdZ8+fRrjVQ6UxuUs\n53C6b7UOp7asyvbMh/T9Vc+YTXno254F+cvCAAAAAAAAAAAAADCnfCwMAAAAAAAAAAAAAHPKx8IA\nAAAAAAAAAAAAMKd8LAwAAAAAAAAAAAAAc8rHwgAAAAAAAAAAAAAwp3wsDAAAAAAAAAAAAABzavH7\nvNl0Om2Xl5eD8bGx1lpbXV29kbJnZ2eDsePj41g2Xbsqu7g43HU9de5RXXdjY2Mm920t92HP81b9\nkDx8+DDGT05OBmMrKyuj73txcRHjS0tLg7HT09PRZStV2d3d3cHY+vr66PumsVHFUx+1lutVjfc0\nhyupLXvWjvfee290naprV9K6Vc2Hg4ODwdj5+Xksu7m5ORir5lKaLz3rTtWO6dqpHafT6eg60e/q\n6iquKT25SBozPWOxmnvpeao69+RAa2trg7GFhYVYNu016bqt5TpX68Ws1vzW/nJsjbW8vDwYe/Xq\n1ejrPn78OMbT2Kn6cFbjvdovqvnQM4d72iP1f0/+VLVlunbPfavxnPqpaqvk/v37MV7VK42Pnvlf\njbunT58Oxqpxl565et40ZnvmYVXnVC850A/XZDIZ/X57UzlQtY49e/ZsMLa9vR3LpnpV75/Vu+3Y\n+1bXrfqhR7p2T72qOqc9o+cM8d69e7FsynOrM4RUtspjevbIyqzOJ2dZ5wcPHgzG9vf3Y9nU/1Wd\nU9mXL1/GsrNqj9QWrfXN/2oO96yle3t7g7Hq3Sy1ZVU2zbWe87jK2P3u+vp69D35bkwmk/hOP6vf\nw6o15e7du4OxdFb6NtdOUltUuVvP7xI9v/FU60JSrYM99Urn0tV6k9ryxYsXsWx6puq+PWMnjY83\nb96Mvm4lnZu1ludpT3tUZ5Q9ZxDp2lUfpXx1Z2cnlk1mmW+kaz969Gj0fVub3XlvtR4eHR2NLvvO\nO+8Mxqqz4J68v6etnAX9cppMJqP7PY3x1vLe3LNvV7lG2nur7yvSWK3Kpnas2jjdt1rze9pyludI\ns+r/qj3S/tnz22IlPVN13VSvnrZqLc+Xnu8rKlVbJ2m+VHVK70jV2Bn7Ptha3/6ZVL+HbW1txXi1\nTo9V5XVfffXV6GunsdMzJqs+7HmPnVX//xV/WRgAAAAAAAAAAAAA5pSPhQEAAAAAAAAAAABgTvlY\nGAAAAAAAAAAAAADmlI+FAQAAAAAAAAAAAGBO+VgYAAAAAAAAAAAAAOaUj4UBAAAAAAAAAAAAYE75\nWBgAAAAAAAAAAAAA5tTi93mz6XTazs7OBuMptrGxEa+9uro6GNvf349lLy8vR9WptdYWF4ebMNWp\nKpti1bWPj49H37d63h499aqkeqf+bS2PrZOTk1h2ZWVlMFaNu3Tf8/PzWDa5d+9ejD979mwwlp6n\ntdYuLi5iPD1T1f/r6+uDsWpcVnMtWVtbG4xV/Z/iVVvt7u4Oxqrn/dnPfjYYe+edd2LZqh+Sakwn\nqZ1bm926tLCwEOOvXr0ajFV1TvWq7tvTlkmag3fu+PeDblraj1KsWuPSeKvWsaRnDaz2k+3t7cFY\ntQdubW0Nxg4PD2PZtEdWz5vW9aWlpVi2Wnt79rGkao+evTeV3dvbi2VT/1f90DNXUtlqzFbSPlbl\nomlMX11dxbKpvVIftdaXx6a2rsZO9UxJGluPHj2KZQ8ODgZj1XtIKttafqZqbKW9uxo7qY+rOqf1\noZpLqY+rsj37Utrv5EA/XFdXV3E8pr6r5mbPGcLYvKznuq3lOVLNjzQHqjl/U+dAPe0xy/v2lE3x\nWZ7HpftW7589eUo1z3raOtW7GhspJ+jJgSpVWyfPnz8fjFXt/PTp08HYgwcPYtm0tvSM99byu1vV\nh+mdoGftqM7jUg5UjZ2e3y7SmO0590zvEj3jle/G1dVVOzo6GoynuV+dSyZpXLSW51habyrV2cjO\nzs5gLLVTa3k8V3VObVm9n/bsc9UzJT15cFXn1E/VfVPZ6iwo7VfV2v3mzZvBWDXek5vMg9O+UY3L\ntP/25Dk3Nd6r/v/iiy8GY7/yK78Sy6bzrdPT01g2/XbUWmubm5uDsVm957SW26uaw6mf0vO0VrdH\nksZWtXek9T/tK19++WVdMWamyoGSahz35Ehp3ldrQs/vQ6nO1W84qT3SO2Zr9fqapHlb9VF1357z\nvKTqw5Qz9ORe1VhP62vP3tvTjtWY7Rk71bXTM/fct2fcVf3Qs3ZU8STN8VnmGl999dXostUanfqp\n6oc0l6p2Tt/IVXVO5+7Ly8uxbKpXNWZ7cr634VczAAAAAAAAAAAAAJhTPhYGAAAAAAAAAAAAgDnl\nY2EAAAAAAAAAAAAAmFM+FgYAAAAAAAAAAACAOeVjYQAAAAAAAAAAAACYUz4WBgAAAAAAAAAAAIA5\ntfi933Bx+JZjY621dnZ2NhhbXV0dXfbhw4ex7OXl5WDs3/ybfxPLfvzxx4OxjY2NWHZWz1u18/7+\n/mCsqnOPdN/W+sbO8fHxqDpV115aWopl09ipXFxcDMZOTk5i2ZWVlcFYNXaurq5iPLVl9bxVPyVH\nR0eDsc3NzVi2p/8/+OCDwdjnn38++rq9/ZAsLCwMxqq2qOp1cHAwqk6VDz/8MMbT2Orp31evXsV4\nTz+kcVndNz1TWt+n02ldMWbmzp07cZ9M/bq9vR2vndb9nvwp7Rettba+vj4Y+9M//dPRZXued2tr\nK5Z9+fLlYKxqq7T3VqpnSv3/5s2b0fdN7dxaXseqNT+1V0+O05MP9JStpLnSWu7Dar9Ic62nbOX8\n/HwwVvX//fv3B2PPnj0bXaeUp7SWn7fqo1T28PAwlq3aI72rVLl56uP3338/lk1tXd03qdoytUc1\n/1MfV++TT58+HYylOXh9fR2vy2zduXMnjplZ5UDVWEzxaj95/PjxYOxP/uRPYtmPPvpoMFat6el5\nqz0/vatVa1xPW/XkBD1lK2kONSzKAAAgAElEQVTtrZ6pJwfqWT977pvi1dpbvVP3vBcn1flCao+q\nTj1nF+++++5g7M/+7M9i2bQH9rRjteen/q/2/J53pKqdU3tU5/GpvapnSqp3vp55mJ63WofTPEzv\nuLOan7y9hYWFeAZ4eno6+to9ZxSpbHrXay2P108++SSWTW1R3Ted//f8ltbzG071u0OP9Lyt5XpX\nz5T6v+dcZXl5OcZ7xvvdu3cHYz05Y9WHVT+k9nr9+nUsm9qrp616+mFtbS2WffDgwWDs66+/jmV7\nxmyKV/dNz1SN952dnRhPbdnznlTdN5V98uRJLJvaoxrvSc+eVJ0Fpjwo9YGzoJt1586dON7SmKnW\notTv1Z6QzoCr35bSnvHTn/40lv3rf/2vj75vj565OUuzOu+pxk5Pe1R7VZKet6pTz31nNc9ay/Wa\nZTun/bX3mZI0T9M7eXXfWf6mmVT37WnLqv97vusaW6fW8tiqyqb8uuc3/KqdU1t9F+uovywMAAAA\nAAAAAAAAAHPKx8IAAAAAAAAAAAAAMKd8LAwAAAAAAAAAAAAAc8rHwgAAAAAAAAAAAAAwp3wsDAAA\nAAAAAAAAAABzysfCAAAAAAAAAAAAADCnfCwMAAAAAAAAAAAAAHNq8fu82XQ6bWdnZ4PxxcXh6uzv\n78drr66uDsaOj49j2cvLy9Fl7927Nxh79913Y9mxbdFarnO6bnXtdN3WcjtXZXvqVbVHT71SHz9+\n/DiWrZ4pqZ4pWVhYmEnZq6ur0ddtrbWlpaXBWDWXktS/rfXV++LiYjDW087V8+7s7AzGqjGbpOdp\nLa+lPfO/tdYePnw4GNvd3Y1lV1ZWBmP/4l/8i1g2tWU1h5Nnz57FeJr/aS601trp6emoOrXWNz64\nOdfX13HMpPl1cnIy+r7VeEnrZ88a+P7778f44eHhYGxrayuWPT8/H4z17mNjVWtvlS+ktp7Vnt9a\n7oe0trbW19Y961hP2fX19cFYNc+qvC3Fq2v3zPE0H9Le2lruw57+rZ5nY2NjMFbNlZ56vXz5cjDW\n8+7VWs43qnwi5Qw//elPY9m0dzx69CiWTZ4/fz66bM++U/X/wcHBYCz14WQyidflZvXkQGm89bxv\nV9J9q/ePnnV77HVb6zszSap5W12753xq7HVby+vJgwcPYtk0Lqt96qaeN+29e3t7sWyVT6ZxW43L\n1F69e/OspHpV8yHloj2qtkrnU1UfVdI5UJpnlT/6oz+K8bSmpbP61vKY/uKLL2LZ5eXlwVjKNSs9\nZ9ez3O/od319Hc8A19bWBmNHR0fx2jfV92m8/uQnP4llX79+PRirzlKrc5ckXbu6blpves53W8t9\nWLVHKlvVK42t+/fvx7JprXvz5k0sm86Zqjr37Pvpvl999dXo67aW+yHtGa3lfuiZ39Xakdq6575V\nH21ubo4u++rVq1HXba21p0+fDsbSGvw20nxJ920t1/uP//iPR5f98Y9/HMumNe/FixexbFLlwWn9\nr6QxnWI39TsBf6knB6ry6p65W/32NNYHH3wwumy19lbretKTA81yT0jXrp63Zz9J5/Q9OVDVltW1\nk54cKN23yoGqeZaeucrrqjw3Sf1Q1XlW50jVmE39UO2PqS2rdkxtVeVPlVS+Zw3/gz/4g1g2ndlW\n/Z/aq/e9blaqM9te/rIwAAAAAAAAAAAAAMwpHwsDAAAAAAAAAAAAwJzysTAAAAAAAAAAAAAAzCkf\nCwMAAAAAAAAAAADAnPKxMAAAAAAAAAAAAADMKR8LAwAAAAAAAAAAAMCcWvw+b3bnzp22sbExquzi\n4viqVve8vLwcjJ2dncWyu7u7g7Gqzum+lVS2p62q5/2h9sOspP6tvHr1KsY//PDDwdj5+Xksu7Cw\nMBi7uroaXfb09DSWnaU0Pg4ODmLZi4uL0fddXV0djFXj7vPPPx9939TWPX1Yjdnj4+PB2MOHD2PZ\npaWlGE9jvmcOpz5qLT9TJZV9/Pjx6Ot+/fXXMZ6eqVorP/nkk8FYWkd79hz6LSwstO3t7cF4te4n\nPX2b1pOe+1bzNpU9OTmJZVMuUrVFKlvtgWtra6PLVu2R1shqT1heXh6MvXnzJpZdWVkZjP3hH/5h\nLPs3/+bfHIxVdU7tsbe3F8v2SGOrN9fsmYfr6+uDsWo+9OSEPXV++vTpYKwa70lVp7SOVvliao+q\nztVa+fLlyxhPevLY5PDwMMbTmN/a2oplX7x4MRjrGXdVDpTqnPbRqk7M1mQyiftN6rvq/KHnXa4n\nj0nXTs9a3XeWa+CsVH1UxVNb9pw/VdJ689Of/jSWff/990ffN42PKgdKZau26nlnrtbQFK/2uJQD\nzfIcsCd/+uyzz77r6rTW6vmf1qX9/f1YNq0PDx48yBUrpH6q+jD1w/X1dSz7+vXrXLEg1evRo0ex\nbMr5qnmY8vqqH/7sz/5sMOas54dtcXGx7ezsDMaPjo4GY1U+ks5pe843bkpqi9b6nrfnHGmWqmdO\n0jNV1039X50FffTRR7liQWrr6v01vaNWfZjao9r3q7mS9qPq2pubm4Oxnt/pqv2o575PnjwZfd/U\nT9WYTevoLNuqivfM4aRa/9MzV+2R+qF6z0lngT25WfW7Y2rnVHYymYyuE/0mk8nod/pq7e35nSbp\nKZvW1uraPXlbdSadyvb8llaZ5TdSPf2U2uPTTz+NZavz8rGqHKhaI5Ox62drdR+l855qDqdrz/I9\nJo3Lai6lfqrGe9ojq3ZO/VSduaW26hlXreWx1TO/q7ORND6qZ0plq7Og58+fD8Z6zi+r8ZyunZ6n\nOlP7K/6yMAAAAAAAAAAAAADMqfJj4clk8o8mk8nzyWTyRz/3z+5PJpN/NplM/vzb/xz+1/oAAH5J\nyYMAgNtIDgQA3EZyIADgNpIDAcDt8TZ/Wfi3Wmt/+9/5Z3+/tfZ70+n0b7TWfu/b/w4AMG9+q8mD\nAIDb57eaHAgAuH1+q8mBAIDb57eaHAgAboXyY+HpdPp/t9Ze/jv/+O+01n772//7t1trf/c7rhcA\nwI2TBwEAt5EcCAC4jeRAAMBtJAcCgNvjbf6y8H/I4+l0+nVrrX37n4+G/oeTyeTvTSaTTyaTySd7\ne3sjbwcA8IPxVnmQHAgAmDNyIADgNhr1e9ju7u73VkEAgBnwTRAAzKGxHwu/tel0+g+n0+lvTKfT\n33jw4MGsbwcA8IMgBwIAbiM5EABwW/18HvTw4cObrg4AwPfCWRAA/PIY+7Hws8lk8l5rrX37n8+/\nuyoBAPygyYMAgNtIDgQA3EZyIADgNpIDAcAcWhxZ7ndaa/9la+0ffPuf//htCk2n03Z2djbqhqur\nqzGerlvdM8UXF3MTVfGxquum9jg+Ph597Z7nre5b9eGsylb1Sq6urmJ8ZWVlMLazsxPLXl5ejqpT\na/WYThYWFkaXrcbH0tLS6LJJ9VcYTk9PB2P7+/uxbGqPi4uLWDY973vvvRfLfv3114Oxaryvra0N\nxqoxu7m5ORhL7dhrfX19dNlq7KS5VvXh9vb2YOzZs2exbHXtJD1T9f+e5+OPPx6MffPNN4Oxo6Oj\nsl68tV84D7q+vm4nJyeD8bQnVPOnZz9JdepZt6uyPc87th0r1frZo6ctl5eXv8Oa/P8dHBwMxn7y\nk5/Esufn54OxtLa2lvuwku5bmVXe3lreu6txmeJVW6acsNpP7t27Nxjraef79+/HeBp3PX2U8vIq\nXj1vzzvBLPXcN7X169evY9k0Zqvxnu5b5e2PHz8ejP3FX/xFLMt35hfOgabT6eg1pRrjPWtVktap\n1vrmXlpPqhyoqleS9qme84Wq7MbGRoz3nE+le1djI603VQ6U7lv1YapXNa7S2VY6m6iuXZ0R9eT1\nldSW1TMlh4eHMZ76qac9ZpkDp/tW8yzFq713lv2fVPOhZ01LZav2SKp1Jz1TNTZSfv306dPB2HQ6\njdflFzLq97Crq6t4HpfmUDrDbS2f41Zz9+XLl4Oxu3fvxrI956Fpbd/a2opl09p+U+dXvddO7VH1\nfxpX1R6axs77778fy6Y1tjq/SmOnGnfpvm/evIllq2vPSpVTpH5Iv/9Unj/P3/GlcVn9fpD2o+r3\nsN3d3VF1quLVeH/06NFgrFrPqpwi1av6jS/1f7XupGeqfuNL167GTrp2T+5ejbv0+9+LFy9G35e3\nNvqboDTe0niqzpZ7fstOOVA1jtPz9OQi1fPOKgfq0XP+W8V//OMfx7JfffXVYKxaT9K6X+WiSTV2\nUh9WZVOdq37oySd6+rAnf+6pc7U29LzHpPFRXXdW35P0zP+qj6o6p37qed6qXun3oeosMKnWjjS2\nqnGX5njak1pr7YMPPhiMffrpp7Hs2yj/svBkMvnfWmu/31r7jyaTyZeTyeS/an+ZEPytyWTy5621\nv/XtfwcAmCvyIADgNpIDAQC3kRwIALiN5EAAcHuUn5tPp9P/YiD0n33HdQEA+EGRBwEAt5EcCAC4\njeRAAMBtJAcCgNuj/MvCAAAAAAAAAAAAAMAvJx8LAwAAAAAAAAAAAMCc8rEwAAAAAAAAAAAAAMwp\nHwsDAAAAAAAAAAAAwJxavOkK/LzLy8vB2PHxcSy7sbEx6rpVfHV1dXTZs7OzWDZdu6pzsrg4vlur\n593f3x+M9bRzFe9pj8rV1dVgbGVlJZY9Pz8fjC0sLMSyPeM9qeqcnrdSjY/k9PQ0xqv5kqT536N6\n3lTn6nl7pPFxdHQUy/asd1UfpbFV1SvZ3d2N8aWlpdHXTs9czaWLi4tR120tt+X6+nosm8ZW6t/p\ndBqvy2xNp9PRe1naa1rL+/4sc5E0Rw4PD2PZra2twdjJyUksm9aaNC9by21V7SWpPaq9tWdfr649\nq329GnfJLHORlF/15MCV6tppfFRtmcpW82F7e3swVu2PPblXT9nUllX+nO5btVWPaj1M9+7p/2qu\npPumdba11u7fvz8Ye/nyZSyb+rDKYw4ODmJ87H354erJgXr6vPd8Yqxqzqd1bpbrWFo/q3buWfOr\nHCitcz1tWT1Tz9hKZXvOTHrGZM/5U6Wnraqxk8Z8tZ+ktq7mUk97zHKeJj1npmke9rwDVfHq3SzF\nq7JJNQ/T2Nrb24tlU/9X8zCtd7N6b3cOdPOur69j/6a+neXZcs9ZaipbjeX0jlKdI6VrV2vVrN4j\nZplvPn/+PMZ78pG0xs4yh0qqtkrvqD17WbV2V5aXlwdjPXO4Kru5uTkYW1tbi2V7+jD1Q3XfpFqT\n0m9L1bozy3OEVK+e/u/5jW9nZyeWTe1RzcM07np+/03Xrcqm+8qDbtZkMhk9/6rxlNaMnveIWZZN\nOVD1vGluVutnqle1bqd69bRVde1qDUzjqnqmnnr3rJ+pn6rn7enDHtXYqnL3sdeu+mhW+cYsnzeZ\n5Zlsmmc9v3dW8WpNS31YlU25aJVPjL1ua7nO1ftEOpOrxl1q5zQm3/Y3eH9ZGAAAAAAAAAAAAADm\nlI+FAQAAAAAAAAAAAGBO+VgYAAAAAAAAAAAAAOaUj4UBAAAAAAAAAAAAYE75WBgAAAAAAAAAAAAA\n5pSPhQEAAAAAAAAAAABgTvlYGAAAAAAAAAAAAADm1OL3fsPF4VuenZ0NxjY2NuJ1Ly8vR92ztdbe\nfffdwdg333wTy967d28wtrq6Gsvu7+/H+Niy1X1TPLVja7kfjo+PY9mea+/u7sayqY+r9hh73dZa\nOz8/H4yl8dxaft6VlZVcsaBq59Qea2trsezV1VWMpzGwsLAQyybV/D84OBiMVe1RPVOytLQ0GKvm\nQypbjbtU583NzVj24uJiMFaN2eqZ0npYjenUh+m6rbX26tWrwdiHH34Yy6a1pRobPWMn9XFqi9Zy\nP6XrTiaTumLMzJ07d+L6O7ZfW8vrXLV+pvtW62eyvr4e4ycnJ4Oxam6dnp6OqlNrfc+U9OxxreU9\nIbVVZXl5OcbTtbe3t0eXreqcxkc13lO8um96pp77ttba3t7eqPtWqv0zPXM1LtNcS/lCa33vBD3z\npWcO95StcqS0vlfvIqkPe9b/tK601trPfvazwVi1hic9a1a1/t+5M/zvOqe5Ige6WZPJJI7lNI6r\nedvz3pzmZnVWM/Z5Wus7Q0j37VkvKmNz2F4972PVXtNzhpT2uWrsVLn52LLVO2TKRaqxUfVxWver\ntkz7TbWPpWdO86xXqnPPfKj6IcV71o7qvLWS+rg6y0nzpTrbSv1f5TEpb69y4KQn5+vp/7Q29Pz2\nwHdjYWGhHM9DqrOPdI5flU11Ojo6Gn3f6lkPDw8HY7PMv3pUOVaPWdZ7rOr3oWp8JD1t+ejRo8FY\n+m2gtXw2Vp2bVXVO967eVXp+43n69OlgbJZjthofN2GWz1tJ7dEzdnZ2dmLZnrUj3XeW75Tp/axa\nV1J+dpP9z81J/V7l81tbW991dd5Kys1SftRannvV86Z1qirbM796f2tJUltW5wgpXpXteVdMqrZI\nfViNnZvat3vasqrzs2fPRtWptdbu3r07umzPu1dS7a1pfNzkb+WpD6v+T/t+lQP31Pv58+ejyybV\nu0hSrSupj9OYrH6j/Sv+sjAAAAAAAAAAAAAAzCkfCwMAAAAAAAAAAADAnPKxMAAAAAAAAAAAAADM\nKR8LAwAAAAAAAAAAAMCc8rEwAAAAAAAAAAAAAMwpHwsDAAAAAAAAAAAAwJxa/L5veHl5ORhbXV0d\njJ2dncXrbmxsDMb29/frig1YXMxNlOqVnrWKHx8fx7KprVKstdZ2d3cHY9Xzpms/fPhw9H1765VU\nbXl1dTWT+y4tLcV4NT7GWltbi/GqPZI0z1prbWFhYfS1U3tVdb64uBiMVe2RylZzKc3/nZ2dWPb0\n9HQw1tNH1bhKY7oqe+/evRjf29uL8aRq67GePHky+r5pbFSqtSP1cTWP0p6WYmmtY/aur6/jmtGT\nA6WyPXvgyspKLHtychLjyfn5+WCsmnvV/pqka1fXTf3w+PHjWLbKgXraMnn9+nWMp2daX18ffd9q\nDUz935MfVXVO86FaI6tnSvfueaaetbvaW9OeUeV8qQ97ys4yb099VM3Ban04ODgYXa/l5eVRsdZa\ne/HixWDsz//8z2PZnvZIY7qah6mtqhzo+fPng7HDw8PBmBzoZk2n0zhmet65k2rtHZuXtZbnSM+a\nX923px3T+lqt2z3PlNb81vr6IelZx6q1qKfOqWzVh+l9+8GDB7Fset6e89bWcnv1vFNXZdMzbW9v\nx7JpfFRtmfKnag9Me1UljY9q7KS26p3/33zzzWBsc3Mzlu2Z43fuDP/9k+p3gDdv3gzGen4H6OmH\nnvumtUEOdPOur6/jGXCaJz05Us+YqtbfnrW9Zw9Nc6hab16+fDkYu3//fiyb+q9aQ9P7Wmv1Hpyk\nPj46Ohp9357ftGa5DqbzrXfeeSeWndX+21o+O+1Zg6s+TL95Vee5PecqaT5UYyeVTbFZ6vktrbV8\nRlG1R/r9sCqb7vvZZ5/FsmnNq9pjVufqlTQfZvU7O/1meRbU0+9pHFdrURrnVdlZ/d5cfQOR6tXz\n/UQl5V6t5T6s6pX6odrze+7bM5579rnUDz15bNW/Pf3QM3Z6bG1txfis8o3qvimP7dkfK+kdqco1\nq7ZK8Wo+VO9uSVpbPv3001i2JwdKz1t9E5beCav8qec95m34y8IAAAAAAAAAAAAAMKd8LAwAAAAA\nAAAAAAAAc8rHwgAAAAAAAAAAAAAwp3wsDAAAAAAAAAAAAABzysfCAAAAAAAAAAAAADCnfCwMAAAA\nAAAAAAAAAHPKx8IAAAAAAAAAAAAAMKcWv8+bXV9ft7Ozs8H44uJwdTY2NuK1U9nLy8tYNl071fdt\n4mPve3x8PPq6ldXV1VGx1vLzfvnll7Fs6qPW6n5KUr2r+6ZnOjk5iWUXFhYGY1dXV7HswcHBqOtW\nlpaWYnxtbW0wdnp6Ovq+rdVtnfTMpc3NzcHYxcVFLJviu7u7sWw1X8ba2dmJ8bQ+VP3/6tWrwVjV\nf2nstJbbY3t7O5Z99uzZYKwalw8fPhyMVc+U5mE1dmbV/5V79+4NxtLYqNYzZmsymcTxmPquyoGS\nqt/X19cHY9X8SftctY+lNSGtB63lda5nH6qkZ3r69Onosq3lfkp91Fpry8vLMT5WNXbSvl2tjynn\nq/qwJ+dPVlZWuuI9a2xPe/SUTXM47Y+Vav9Mc7ha73ryxdRH5+fnsWw1D9Mcr8p+/fXXMZ6ktbS6\nb2qPnrlUtWVPH6Z2TnP0zh3/jvQPWRpv1X6SxvHe3l4s++DBg8FYdR6T6lztF2ntreZHzx6Y3iGq\nsum+1X5R7UWpj6t6Vetcktq655mqPux53rTmV3Ol5wzhplT7yazex6t3kTTHe84QKz25d8+5Z3WW\nk87jHj9+HMtW58hJWtOqZ7q+vh6M9eQplVmde6e18PXr16PvyXejOgtKZ57VOWwq++LFi1g2zd1K\n9ZvH2PseHR2Nvm61vt6/f38wVr2/pv6r2rmS7l3tzyle9W9aj6pz+Ddv3sR4ktayav29e/fuYKxn\nfa3Gc5XLpjO5qq3SvWf5Tp6eqZqHqWw1l9IzVeM9Xbsa7+n3sJ7531peW6r87IsvvojxpGcN7/0N\neMgsz+Rv6rcA+lQ5UFoTesZ49T6XbG1txfjLly8HYz2/r6frVteu1rFZ5V7VfSs9OXAVTw4PDwdj\nPc/Us2/37IE9fVjdtxrTvWNgSLVPpZywkuZ4GhuVnvfunvOcqg/T+OhZO6p4NUe/+uqrwVj1TNU6\nPVaVt6f3zZ5zpJ5vkVJbTSaTt7q/X80AAAAAAAAAAAAAYE75WBgAAAAAAAAAAAAA5pSPhQEAAAAA\nAAAAAABgTvlYGAAAAAAAAAAAAADmlI+FAQAAAAAAAAAAAGBO+VgYAAAAAAAAAAAAAObU4vd5szt3\n7rSNjY3B+PHx8ahYa62dnZ2NrldP2fQ8i4u5edMzVWWreHLv3r3BWNUWPW1VSc9UPW+q18OHD2PZ\n8/PzXLGRTk9PY3xlZWUwlsZVa61dXl4Oxqo+WlpaGoxV7Xx1dRXjBwcHg7HV1dWuayfpmSqpLat+\nmJWLi4uZXXuWz5TGdDUu0/io5lK6ds/8TmOjum8l1WuW/c/NmU6n5Zga0jMWq3X98PBwMLawsBDL\nbm1tjb7v8+fPB2PVftFjfX19MFatF2mPq563unaqV9UPb968GYwtLy/Hsj1tneqc2qq6b7pude1Z\n5s/VM6V52nPfaq9J167WjtQP1bhLqjqnfa4nH6yeN83DatxVUlumdba1PE+rsum+VT+M3ZN6yyY9\na1LPPOOHq2ccb29vx7J7e3uDsWo8pTWjKlvtJ0l63mpepvfA9B7XWs7bquet9pOTk5PBWLUmpLZ8\n8OBBLLu7uzsY68knetbHnnWsum/PeVtPPlHN4Z53kaTnmaqcIF07jefqvrPcx1K9qvnf0//VWX46\nn/7mm29i2XQOWPVDclPzsCo7dv2fTqfxusxez1nQ0dFRjKfrVnvo69evR9WptdY2NzcHY9UcevXq\n1ej7rq2tDcZ6zp0rT548GVWn1vrygqrOKT979OhRLNuzH929e3cwVo3Z9EzV86Y+TudirdVnY0n1\nTt7z20P6zaMaWz17Ss99U9me8d7ze0g17m7qrKCq187OzmDsxYsXo689q5yitTxPq7HTs/4naW2Q\nB92sWeZASfW9QFrHqjU/rSc962dVNq2R1fOmPqju+9VXX40uW0n5ZOXZs2eDsQ8++CCWTc/UU6fq\nO4bUXtU+lfq/Z4+rxk41D1P5al9Pcy397lyZZQ6U6tzznVI1dmaldw6ntu6ZD1XZdN+qH9I5cvW7\nZE979fTx2Pe2yWTyVtf3l4UBAAAAAAAAAAAAYE75WBgAAAAAAAAAAAAA5pSPhQEAAAAAAAAAAABg\nTvlYGAAAAAAAAAAAAADmlI+FAQAAAAAAAAAAAGBO+VgYAAAAAAAAAAAAAOaUj4UBAAAAAAAAAAAA\nYE4tfp83m06n7ezsbDC+uro6GFtczFVN8cvLy1j2+Ph4dNlU5/39/Vg2Xbt63qS6b4+Tk5PB2Pr6\nete1U1tW/ZCeuSr76tWrwdjDhw9j2TSer66uYtmNjY0YH3vfe/fuxbK7u7uj75vmSmt5DKSxU0lj\no7XcHgsLC7Hs2traYOzi4iKWTePunXfeiWXPz89jPKnqlaS2quZKNWbT2Kr68ODgYDC2vb0dy/b0\nf5LGRmutnZ6ejqpTa3muVGXTPEyx6+vreF1uVk8ek/abKp9I167W/HTtw8PDWLZn3i4tLQ3Gnj17\nFssmW1tbMd6zbq+srIwuW+UTaf2s+j+VrfK61Ic9465q51S22i+eP38e4z1SH/eMnaot07WrcZfi\n1fzvue/Lly8HY/fv3x9930paW6o8tZoPe3t7o+7bWmtv3rwZfd9Zja0qF6nWpSS9q1TvsanOqQ/l\nQL+8qhyoR8+7ayqb9tbW6vU1Se9j1X171u2x53hvI+3d1RqX2rLnDKEq2zMuZ3XfKl9IbZVy69bq\n84dU76oP09irxmV636jaI+Ub1XtMT+49q3PgSlqzqr13lnPp6OhoMLa5uRnLpnG5vLwcy75+/TrG\nk573mJ53/rHnz9PpNF6Xm5f6vmeP7Zm76X2ttdbu3r07GKveudPzpnPW1vI5bVU2/f5Trb9pn+zN\nVR89ejQYS2tkde+qH1J7VetvWkOrs/SePSO9N1fneV988cVgrHreSipf9WHVXkm6dnXdNHaqs4C0\n/1b37alzum9P/lX1UTU+0lzqWR961v/qvj2/h/bsHelsrDo3S+2R6iwPulmTySTOz+o9NEnvbNV6\nkuLVOO5Z81Odq7ZI7fjVV1/Fsr373JAq95rVfat7P3nyJJbtGTs9e2DPdx3peasc6PPPPx+MVb/D\nVOMy9XE1l5JqbKW2rNojla3q3JM/9/yW2iNdu2rnqi1n1cc9+XH1TOk9tlrDe5435WbV2pD64enT\np4Oxt/39zl8WBgAAAPOrsZcAACAASURBVAAAAAAAAIA55WNhAAAAAAAAAAAAAJhTPhYGAAAAAAAA\nAAAAgDnlY2EAAAAAAAAAAAAAmFM+FgYAAAAAAAAAAACAOeVjYQAAAAAAAAAAAACYUz4WBgAAAAAA\nAAAAAIA5tfh93mwymbTFxeFbHh8fD8bu3bsXr53KplhrrW1sbAzGzs7OYtmkKttz38vLy1Gxyurq\naow/ePBgMFa1c6Wn3mlcVdIzX11djb5uj562fPbsWYyvrKyMvvbS0tLosgsLCzHeM9d6nJ6eDsaq\ncZXWpRcvXsSyPWM29cPR0VEs2zPPqj5Kc+nhw4ej71tJz1S1czUuk7W1tdFlDw4OBmNpTLaWnzft\nK+fn53XFmJk7d+7EObK3tzcYS3tva3lMnJycxLJpjlRjPO0n1X1TW1TzNj3v1tZWLPvmzZvB2OvX\nr2PZnZ2dUddtrZ5/d+/ejfEktcf6+nos27MGJtVes729PRir2iqNj+fPn8eys3reSpV7HR4eji57\ncXExqk5V2SoHTvtN2uNay+Oyum9Pbt6Te1VrWlKt4dW4Hat6r0v9VJXteRdN+13POzA/XNU50P7+\n/mCsOgdKY6YaLz1rQrp22uOq+/aM8eq+6drVGcKPf/zjwVh1dnFTZyo9766V1IdVHpP2wJ4xm+ZR\na33nXtU5UKpXT17fM3aqtuw5f0pt2TOHZ3lWk1RzoWetrNaldH7V80ybm5sxnt7dqudNa16Vt/fk\nkz39wM26c+dOPFt5+fLlYCydQbTW2ldffTW6Xkm1LqR4tWekeM847zmjrc7S33nnndFlq7W9Kj9W\n1Zapvao6Ly8vD8aqs7F0Ztdz3+p3mLQvVOdE1XldOkvsyYN69IyrnjOmnt80eu7bo1pnK+mZqmu/\nevVq1HVby3O4WofTGVQ1ZlO8mitpv6ueNz1Tik0mk3hdblY6D79//34sm9abai1KY6Zai3rWsfRM\nPet29e4zNg9tLde5et6ePaGSylY5YTW2xqqetydXTWXTPGqttcePHw/Gqj6s9oTqmZOx63prud7V\nmO45k+3pw1Tnqp1T2Z7vtnrauSpfrUspF+k5G636KP0eVr0TpGv3zIXqedMcT23xtjmQvywMAAAA\nAAAAAAAAAHPKx8IAAAAAAAAAAAAAMKd8LAwAAAAAAAAAAAAAc8rHwgAAAAAAAAAAAAAwp3wsDAAA\nAAAAAAAAAABzysfCAAAAAAAAAAAAADCnFr/3Gy4O3/Ly8nIwdnx8PPqeDx8+jPH9/f3R1071Ojs7\ni2VXV1cHY6mdWsttVZUd2wdV2fPz81h2ZWUlxqvySWrLFGst91NV54uLi8HYwsLC6PsuLS3Fsqen\np4Oxqs49Y6e6dlI9U2qPqi2vrq5GxSo9Y3JjY2N02aofUr02Nzdj2aOjo8FYNf+rdTg98+effx7L\nJlV7rK2tDcbSXGktj4/UVq3ltq7aKo3pauykfkr7Wc9coN/19XWZFwyp5mYaM9X8OTg4GFWn1lp7\n+vTpYKxat9O8rcqm9qjGebp2dd83b96MqtPbeP369WCs2ouqPGds2Z49v3JycjIY62nLnjr35N6V\nnnyicu/evcFYteaMXZNay2vH/fv3Y9k0T1Nu3Vru42ou7O3tjarT29RrfX19MPb8+fNYNt27592s\nkq6d5mhrrW1vbw/Gqn2lZ826vr4ejKXnmU6no+/J7PXMgaTKqdOaUEnjvKpzz96b7ttzHvP48eNY\ntme/qKT+73n/nNX6WOnJY6v7prbqeYfsOUOq4lV79Eh7b3Xfap8bq+c8tmfs9Jx7Vqq1JdX7yy+/\njGVTvXryhSoX6fn9IeW5X3/9dSybnreaw6leve+izNZ0Oo3vEmlcVOeSW1tbg7FqPXr58uWoOrWW\nz4Iq6feBnrWsZx5UZ+mvXr0ajFXtXJ1Lp/ao3kF7cp2buG5ruZ/SuVhrOQ+q6pz2quXl5Vi2qtfY\n+7aW26Pq/57fQ3ry0XTfSpprPblK1VZJ1VaVtG71rJU9ep7pxYsXMf7+++8Pxqrnrc4KkydPngzG\nUv87C/phS3lOynFaq88wksPDw8FY9R1DKlvp+TYj6fmepFrz0/zqyXEqVQ6c1pOePWGWOVBPvWb1\nvlf1Uc/vElXZNH6qXOPu3buDsaqd031n+W1eT/7UU7Y3z0lSvarfw5JZ/k6b3ieqtkrrTrU3pGeq\n3kXH/gbxtjmQvywMAAAAAAAAAAAAAHPKx8IAAAAAAAAAAAAAMKd8LAwAAAAAAAAAAAAAc8rHwgAA\nAAAAAAAAAAAwp3wsDAAAAAAAAAAAAABzysfCAAAAAAAAAAAAADCnfCwMAAAAAAAAAAAAAHNq8fu8\n2WQyaYuLw7d8+PDhYOzy8jJeO8X39/dj2bOzs8HY6upqLJueZ2NjI5atnilJ9aqeN9W5574XFxej\nr9taaysrK4Oxb775JpbtGTupn3r6aG1tLcZTe/W2ZZLqdXp62nXtNJeWlpZi2dSHx8fHsezCwkKu\n2IzM6r7n5+ej71vN/zSmq/Wukq59dXUVy6Zn2tzcjGVfvXo1GEtjsrU816r2ODo6GoxV63+qV7VG\np3g1drg5CwsLbX19fTCexltPDvTy5ctYNq371Tje3t4ejFXrdrpvtX/2zIFUtlrTl5eXB2MnJyex\nbMpxqnr1XjtJY7KS1vVqHetZA2eVP1f7RVWvg4OD0WW3trYGY9WYTuOj6t9qbCV3794djFV7fmrr\nqn9TWz59+jSWTXM89UFrre3t7cV4GlvVOpzmcFp3Wmvt9evXg7GqLVM/VWMnjfceVe6Vnvfw8PC7\nrg7fk48++mgwVo21WZ0D9eyP1X6S6lzN2zRHqrZKa2DP3lvtJT3vmFVbpmtXfdhz/pDao2rLtK/P\n8lyj5xywKpv2yKpsep/omf+VVK+e61Z92JN7pTpXc6Xn3LunzlVOmO7dU6+qD3vao8rrklmdx88q\nL+O7MZlM4pl4yoPSuWNreUxVYzXtddWZTFL9ppHqVd03xV+8eBHLpraq7pvOpZ8/fx7LVr+HpPbq\nOaOonqlnr0t6csp0xlCVneU5UlWvNAaq/u85z+s5V01lZ7VXtZbXtKqPetalnutWv5emPu55t+v5\nPay6b3qm999/P5ZNa3hV53Se06OnLZitO3fuxDn2wQcfjL52GsfV+WBai6pxnNbIar1I9arOpVM7\nfv7557Fsunb1vOm+VQ50//79GE/zs8onU72qvXdsnSo9OV/P7yE9+3bv3pv2hKpejx8/HoxVc7gn\nB5qVKl9Mz9Qz7qq5kuLVulP1Qxrzs5zDPd+ypXpV30M8e/ZsMNYzV6qyaWxV6/DbKP+y8GQy+dFk\nMvm/JpPJn04mkz+eTCb/7bf//P5kMvlnk8nkz7/9z53u2gAA/EDIgQCA20gOBADcVvIgAOA2kgMB\nwO1RfizcWrtsrf330+n0P26t/WZr7b+eTCa/2lr7+62135tOp3+jtfZ73/53AIB5IQcCAG4jORAA\ncFvJgwCA20gOBAC3RPmx8HQ6/Xo6nf7Bt//3UWvtT1trH7TW/k5r7be//Z/9dmvt786qkgAA3zc5\nEABwG8mBAIDbSh4EANxGciAAuD3e5i8L/38mk8nHrbX/pLX2/7TWHk+n069b+8vkobX2aKDM35tM\nJp9MJpNP9vb2+moLAHAD5EAAwG0kBwIAbit5EABwG/XmQLu7u99XVQGAEd76Y+HJZLLRWvs/Wmv/\n3XQ6PXzbctPp9B9Op9PfmE6nv/HgwYMxdQQAuDFyIADgNpIDAQC3lTwIALiNvosc6OHDh7OrIADQ\n7a0+Fp5MJkvtL5OC/3U6nf6f3/7jZ5PJ5L1v4++11p7PpooAADdDDgQA3EZyIADgtpIHAQC3kRwI\nAG6Hxep/MJlMJq21/6W19qfT6fR/+LnQ77TW/svW2j/49j//cXWt6XTaLi8vx1V0MVd1f39/MPbh\nhx/Gsl9++eVg7Pj4uKte/y979xJiXbrf9/2/ateuXff7W9Wqar193DpGGCRigxAGz5QMQhKCBw4E\nQvBA4KlnCcks4IlG9tjYA81k44nBM0OsgScOCnISkubIp02/3W9Vv3W/3/eulYH6iGPUz+9X5/nX\neqt71/cDBh/9+1nrWc/1v569urrW7e1tKq5k6qzuOz09LcvOz89XX9v9G2jqmdzzXl1dFWNfffWV\nLLu9vV2M9Xo9Wfbh4UHGFVXnpaUlWfbk5KQYc33k+lj1oRuzg8GgGHPzUI0P9586GY1GxZjrQ9Ve\nZ2dnsuzNzU0xlpmjCwsL1fd1feTGh2prNy5VH7s+VHPJPVO/35dxZXl5uRj78ssvq697d3cn46qt\n5ubmijHV9/h+z50DqfVG9Z1bT1R8Y+N7/4tQf0HNkfNz/S+Nq7Ho9gvFPW8mF5mamqqqk6P67yn3\nVXuz2xPUuHLtodr666+/lmU///zzYszl+6pemRzXPa8as9mcXpV3+YQq6+ahGnvq/ShC55OLi4uy\nrGprN4fVmM1wdVb3dXVyOdD+fv0ZueoHt7aouebyCXdtZWtrqxj74osvqu/rxqxaH8iBntdz5kAv\nxc1btfa6fSyTi6j7uvdexd1Xxd3zZtZtt9bs7e0VY5l8Uq2tEboP3X8yXv2VSNeW6tzDrdtK5twr\nk6dE6Lnmzr0yY17Vq8szU9WHmfcYd181tjJ95PpAPW9ExMHBQTHmzluur6+LMdeH9/f3xVjmXcTJ\n5ECZdyDVT6qPVDuh7GPmQWq8urGs3hVXV1dlWTU/3Xi8uLgoxtz8UufW7nl3d3dlvKv7qrg7h3fr\n4PHxcfW1VT9k3n9cO6tzxi7XHPW8KysrsmymXm6Pzfy20FXu5/pfxWdmZmRZFVfjOUL3obtv5rfU\nTM7o+lc9Uya3y6zDrv/VHHZlP/vss2LsZz/7mSyr3jnc89auw+5MFX/Zx/wmSM0RNcYj9Fjd3NyU\nZdX5sRsz6vzXzXm1zrn7qnMTx62viuoH97tjhqtz5vsKVfbdu3eyrFoD3fqpxp3ba9T4cL+HdHkm\nrvopc9/MmHUyZ8Hq/SozR9191T6X+U3T5VZuXGZyMzU+MudI7vxa9aHbd9Q3gT//+c9lWTVP3VxR\n81+NDXcu/gtPGUF/KyL+x4j4f5qm+fff/d/+1/jzhOCfN03z+xHxdUT8d0+6IwAAwI8DORAAAHiN\nyIEAAMBrRR4EAABeI3IgAABeCfuxcNu2/zYimkL4P3/e6gAAAPwwkAMBAIDXiBwIAAC8VuRBAADg\nNSIHAgDg9Zh46QoAAAAAAAAAAAAAAAAAAAAA6AYfCwMAAAAAAAAAAAAAAAAAAABjio+FAQAAAAAA\nAAAAAAAAAAAAgDHFx8IAAAAAAAAAAAAAAAAAAADAmOJjYQAAAAAAAAAAAAAAAAAAAGBMTb50BX7Z\n4eFhMTYcDju5rrO8vCzjt7e3xZirsyqrYhERk5PlrnP3VWUvLy9lWWV6elrGXT/0+/1i7ObmRpYd\njUbF2NLSkiyrrK+vV5d1bTk/P1+Muf6fm5srxlz/q3ZWsQjfh6of3Pg4OzsrxlZWVmTZ09PTYsz1\n4dXVVTHm+lC1h3vervR6PRlX40OtDRG+/1Ufunmo+jDTlm4uffrpp8XY3t6eLKvGzmAwkGVVP2TW\n/7u7u2Ls8fFRXhfdappGzs/d3d1izI0J1e9uLKqyaq9x11bzI0KvF46aA5n905VVe9zU1JQse319\nLeOqLV1bZdpDefv2rYxncmC1rmfKun1b9aHbAzPvIo66tpuHaq659xiV97mxc3x8XIy5dUdRfZSV\nyYHcPFTttbi4KMseHBwUY24tzazDW1tbxZh7XhV3Y1b1g3sXqc2fyIFeVtM0co6p8eTGYmZeK65s\n5v1EPZN7l8uskZmcQNUrs09F6HMRt69n9mbVx5kzJNdH6r6u/1VZ185Kdu9VY8s9k+LKZt5jVNmj\noyNZVj2vWzsy61JG5qw3896WOQecnZ2VZTNn259//nkxpt7LI3QO7J43s2Z1+S6Cl3VxcVEVc3H3\nDqK4sfzw8FB9bVVn9y6g7uvKdiVT5wg9tzPzfmZmpvq+Gxsbsqz6nc7t3Wpcnp+fy7Lq3dfNFVWv\n7Hme6mPXD4prS9UPLt9Q9XLjTu2Dbj6o+75UjuTue3JyIuOZflA5x+rqqiyrru36IfNtgcuTlMz7\nqKLmf5dnjMhT64mj9gy39qp56/KnzN6s6pw5D3Xfz6h9ypXNrM1ub86UXVhYqL62Gh+fffaZLJs5\n01Zt6Z5XXdvlT5kc2eWx6t5uLmX2z8y7iGqPTA7kxqTq40zenvlN0913f39fxrt6J8yss65O6pkz\n796uLd08VdTZWGad/QX+sjAAAAAAAAAAAAAAAAAAAAAwpvhYGAAAAAAAAAAAAAAAAAAAABhTfCwM\nAAAAAAAAAAAAAAAAAAAAjCk+FgYAAAAAAAAAAAAAAAAAAADGFB8LAwAAAAAAAAAAAAAAAAAAAGOK\nj4UBAAAAAAAAAAAAAAAAAACAMTX50hX4ZfPz88XY7e2tLDs9PV2MDYfD6jpl7uvKquednNRdo66d\nKevaSl3bPW+/35fxubm5Yuzh4UGWXV5eLsZOT09lWdUPm5ubsqy6tuuHDHXty8tLWVaNWdfOqmxE\nxNXVlYwrvV6vGHP1uru7K8ZGo1EndYqIGAwGxVhm3XFlVdzNQxdXXP8rao46rs6qPRYWFmTZ9+/f\nF2NuzVIybZV5XvxwPT4+yrVK7YHO0tJSMebWZbWOnZ+fy7JqLLo9cHZ2VsaV+/v76rJqfrnnXVxc\nrK6TaucIvd9k9lY15iJ0H25sbMiyx8fHVXVyusyBVdy1letD1U9q7ETkchXVHm5dUc/kci81h11b\nqud17ayu7fZHlSO7PV+tsxF6Dqt3jQi9fmRyQtf/7p1BUXPJ5UCqzm4dVvdVz5t5VuS1bSv7PXOm\nsra2VoxlzjbcmMmci6h1zq2faq3J7CWurHsvzpRVben6MJMTqD5eX1+XZdU5kNs/u+pDt38qmbny\nlPJKVzlwpj3cnq/6qau9NUK3s+sD1ZYuX3BzWJ25uLbMnE+pueZyr/39/WIs0w+ZcefaosszZnSr\nbVs5XjPnhysrK8WYWxfU+8/FxYUsmzkLUvd1ZdUZxcnJiSx7c3NTjHX5u+PMzIyMq2dyZy7X19fF\nmGsPJTMm3dhx73uK2n/d+b9qq8wZY4Tv41ruvUBx+2DmN00VV/PMcWcybmwpmTpn2mN1dVWWzeTQ\n6r6ZtnTPmxnv6r6ZfuC3sh8ulwOp83I3Jra3t4uxzN7s9in3vl97X/e8aj3Z2dmRZTNrs2pLd123\nXqh57dpZrSfuvurarmymLTP7mKqzW/O7fF5V3vVhZh4q7ne4zJ6hnilztuH2XtUPrp27WrMclwNl\nZM5Gjo6OijH3vOqdwY07JfPO9xz5EX9ZGAAAAAAAAAAAAAAAAAAAABhTfCwMAAAAAAAAAAAAAAAA\nAAAAjCk+FgYAAAAAAAAAAAAAAAAAAADGFB8LAwAAAAAAAAAAAAAAAAAAAGOKj4UBAAAAAAAAAAAA\nAAAAAACAMcXHwgAAAAAAAAAAAAAAAAAAAMCY4mNhAAAAAAAAAAAAAAAAAAAAYExNvnQFftlwOCzG\npqenq8uqWETE5eVlMba+vl59X+f09LQYm5+fr76uq9Pd3V0x1uv1qq89Go1k2cFgIOOqPVz/Z9ry\n/fv3xdiv/dqvybIZrr1qZcaOc3t7K+MPDw/V186UVfP04uJClu33+8XYzMyMLHtyclKMuTGb6Sc1\ndiYn65d1NyZd/6u2PDo6qqrTU6h6ffjwQZbd3t6uvq9qL7WvOJl9ZW5urhjL1Al5TdPI+anGsds/\nFVd2b2+vGFteXq6+9tXVlSyr5s/a2posq/b8TA7k2ur8/LwYU3PP3TdC19vVS7W12xPUfXd3d2VZ\ntc9l9iInc21V1l3X7YEqvri4KMuqMe32ZjVfXJ3VM7v5r+rs2nJpaakYc2uH4u6r8hT3DpTZQ8/O\nzmTc3buWm8MbGxvFmKtT5j1GrTsTE/X/LrNak66vr6uvi+6p+eXeqdQ659ZANTfdu1pX8/bzzz+X\n8Z2dneprZ+atWptdH7m2UtfOrL1uP1H1UmdEEXp8ZN7zM3lb5h3ScXmsirtnUuPS3Xd1dbW6bOb9\nSl3bzQfVHpm54tY7VS+3NmTWDjeHVb3cWZ4qe3BwIMuq/Dkzl1xbqWtn3kVUW3S5NuBpmqaR7yGK\nmwcqx3X3VO8KW1tbumKCO99XY93toeod1FFzyM0/1Q/qncqVjYi4ubmRcWVqaqr6umrPcXVeWVmp\nikXoNen+/l6WVb/TuDqrsu6+jvp9yP22fHh4WH1f1dbuuqqs2zeOj4+LMfdbWmZPUvPUXTczzzLn\nTKqt3LUz53nqrD9C59CZPnLnLmrdyeRBb9++Lcbcb4PoViYH6vJbg3fv3hVjn332WfV9M2uN+516\nf3+/GMusvZk6LywsVN83Qj+TWxNUPDN23LrtfuOp5eZJ5plUnTP9H6F/L3XPlCk7OztbjLlnUu3h\n9rHM+0Tmd0nVHl2ulW5tUeVV/7pru7VDlVXre0TEmzdvirHMO4Ebd+qZ3HhX40O9i7pzsV/gLwsD\nAAAAAAAAAAAAAAAAAAAAY4qPhQEAAAAAAAAAAAAAAAAAAIAxxcfCAAAAAAAAAAAAAAAAAAAAwJji\nY2EAAAAAAAAAAAAAAAAAAABgTPGxMAAAAAAAAAAAAAAAAAAAADCm+FgYAAAAAAAAAAAAAAAAAAAA\nGFOTL12BXzYcDqvLTk9Pd3Jd5/LyspPr3t7ednLdiIjBYFCMHR0dVZddXl6WZV0/dNWWo9FIxlW9\nHx4eZFn1TPPz87Ksel7Xll21ldPv92VczUPXD7XXzVJ97NpZtcfkpF5eM32o7uuu6+qVodYHN5fU\n+HBrx8rKSjH285//XJb98OFDMebmoeLmv1rjXdnT09Ni7OrqqhjLzEHktW3bWT7S1XXVnI6IOD8/\nr762WtddLqLGsqtzZq3p9XrF2NTUlCzr4icnJ8XY3NycLHt2dlaMLS0tybIZXd3XlVX3dXtcZq7c\n3d3J+NbWVjGm6hyh1303phX3PqHmoatz7XXdtV0fqnno9jmVi7g8JZOLun64v78vxtyYnZ2dLcZU\nTvCUuNJVe7gcSK2ValyRA/2wqfHk1gQ1nlzZzc3N6rKZdzn1vDs7O7Ks2ovU+viUuKLaw+1Tri27\n2tddPpFZi1Q/uPZQdXbvn+o9MCO7Rq6trRVjmXOA1dVVWVadi2TeU1wOpMZlZu1w4y5DjUu3nmX2\n/IuLi+qymfu69U49s8u9VL3c/H+J3xAeHx87uSeej1onFxYWZNnM++1nn31Wfd/j4+Ni7ObmRpad\nmZkpxtw+p9YUV+eMTDtvbGzI+Lt376rvq57ZtYdqS/WO6VxfX8u4OhtbXFyUZdXe7tZ99c7tuLOg\n9fX1YszlWGpPUXPFyeTQan5H6PzLjVmXF3bF/aapuFxGzbUu21KND5dvZOZ45rdF9byZ7yFUH3EW\n9LLatpXzXo3zzLx1tre3O7mvWy/UvHVl1XqS2S/cO3dm3Xa5SOadXXHt0dVvqV3uceqZ3POods7O\nM5e7KV3NcTeXMntRZg9U70iuLWrXUXdt9zzuvU71/97eXnW9MnMpcwbp1iT1O4KTOUdU1N7w1LWO\nvywMAAAAAAAAAAAAAAAAAAAAjCk+FgYAAAAAAAAAAAAAAAAAAADGFB8LAwAAAAAAAAAAAAAAAAAA\nAGOKj4UBAAAAAAAAAAAAAAAAAACAMcXHwgAAAAAAAAAAAAAAAAAAAMCY4mNhAAAAAAAAAAAAAAAA\nAAAAYEzxsTAAAAAAAAAAAAAAAAAAAAAwpiY/9g2Hw2ExNj09XYxdXl7K605Olh9FXdfFVX0dVzbz\nvHd3d8VYr9fTFRPW1taq73t1dSXLLi0tybjrJ0X1v2sPVbbf78uyKr63tyfLrq+vF2OqnSP0+Jif\nn5dl1bVdH7n2ODk5KcZc/6p+ur29lWUV1c4REWdnZ9XXVnV2fajaYzQaybKZdlbrkruvm0vumRX1\nTCsrK7Lszc1NMfZbv/Vbsqxbt2rv68bdhw8firH379/LsmrN2tzcLMb29/flddGtpmlk36lYZg10\na8LMzEwxdn5+LsuqvcjdNzP31Drm1qlMjqSoNSzCr2PqmVxbqbbOrMtqTEZEDAaDYsyNWZVvuDpn\ncnPFPa/LkdV8cPlVV/3v6pyZh+q+rv9dWysuV1GWl5eLsaOjI1nW1Vn14f39vSybyevVtX/7t3+7\n+r6undXzbmxsyLLKn/3Zn1WXVfPs+Pi4+rrIa5pG7hlqvGVyoMy8de+Iql4uB+pq7qk2jsjlmqqs\n27czuYiTubZqy0wO5Oo0NzdXjLlzQDU+XI6r+jhzhuS4M6TMfVV8a2tLllVzPJOnuDxVXduVzYxZ\n1f9u3XF9qObDxcWFLKveN9VccVTOF6HbMtMPbv1Xufm7d+9kWXVtcqAfNzXH1Lljl9x4VOcfCwsL\nsqxaF9xallmfVVm3Vqmyu7u7sqw6c4vIre2qn9w7mXtH7aps5ndYte+7cafqPDs7K8tOTU1VX9tR\n89+dMyouD3JjXnl4eCjGMr+lOpm5osq6tsj8Zu2oPXp1dVWWVfvD27dvq+uUyYO2t7dlWfXb1MHB\nga6YoOZ/5twTeU3TyHmfyYEya5G6tvs9TJV1e34mr3PXVtQcyewH7nkyz+vWoq6unelDNSYjIhYX\nF3XFKu/r6qzqlcmtI3L1cnNNUX3405/+VJbt6v3YtaVal9zY6WrMunnk5mEmr1Nncu63VNVeLgdW\n3BxVY9blbep9wa3Dqh/UN0HffvutvO4v8JeFAQAAAAAAAAAAAAAAAAAAgDHFx8IAAAAAAAAAAAAA\nAAAAAADAmOJjYQAAAAAAAAAAAAAAAAAAAGBM8bEwAAAAAAAAAAAAAAAAAAAAMKb4WBgAAAAAAAAA\nAAAAAAAAAAAYIV6mgQAAIABJREFUU3wsDAAAAAAAAAAAAAAAAAAAAIypyY95s7ZtYzgcFuO1Mefw\n8FDG5+fnq6+t6jU9PS3Lnp6eFmOTk7prVFxdN0K3x/Lysix7dXVVjLnnvbm5kfHb29tizPXR5eWl\njNeWdfe9u7srxpaWlmTZXq9XjKm2iIjo9/syrgwGg2LMzRVnNBoVY2rsRPjxU3vfDx8+yLJqLrl2\nVvdV/Ruh23p9fb36vm7tODo6KsbW1tZkWdeHaszv7e3JsuqZHh4eZNmuZO7r1mG1d7h1R40d1f9N\n08jrolsuB1LrvpofERFzc3NV142IWFxcrL6vWufOzs6qyzqqrFun1L6dyTWdb7/9trqs6t8Ince4\ndV2VzeSibk9XY6vLflD977ixpWTmkmtLNce7zCcze6TqB7c2qGfKvHu5ueLqpfLr8/NzWVbV2/Wh\nm6fKS80HNcddP+zu7hZjs7OzxZjLy9A9tQ5m1jF1fuHOCDLnQGrO7+/vV5d1czqTe2XWCzVv3XVd\nH9beNyK3f6r2yrSV6t+I3BliZt1WsrlX5kxNcW2p9iK1X0T4/LormbZWe1nmLDd7DqRk3glcrvn4\n+FiMTUzov42i1iU3/2vf6SNyc7i2zpwD/fCpse5yiq2trWIss/8uLCzIuJrbLu9W49WtkTMzMzKu\nqLZ078XqNy1XJ/d7mOLaY3V1tRhz9bq+vq6qU4R+73JrqHo3npqakmVVXnB/fy/LqvXX3ffi4kLG\n1Rx2/ZA5V1Pjw60dSmY+uLLHx8fFmFt3urrvxsaGLOvmYea3RRXPrB1OV99hZMbdyspK9bXVPCMP\nellt21afH7s5oPZAdw6ryro1f3Nzsxj74osvZFn1O5xrJzXO1Rrnrp2Z827Pd22pyrtrq/XT7b2q\nvTK/h7n7qnHpyqrnzfRhZs93XFk1H9wzqTz2q6++qr5vV+emEblcRO2Brs7q2xy1nkXk3nPc2FLv\nE5nfHdV1I3Q/ZOaSexc5OTkpxtzzqnqpPnLnYn/xzz3pnwIAAAAAAAAAAAAAAAAAAADwo8PHwgAA\nAAAAAAAAAAAAAAAAAMCY4mNhAAAAAAAAAAAAAAAAAAAAYEzxsTAAAAAAAAAAAAAAAAAAAAAwpvhY\nGAAAAAAAAAAAAAAAAAAAABhTfCwMAAAAAAAAAAAAAAAAAAAAjCk+FgYAAAAAAAAAAAAAAAAAAADG\n1OTHvFnTNDE9PV2MD4fDYkyVc2VVzLm9vZXx9fX1Yuzy8lKWzdTLXbuWe96Hh4di7OTkRJZVbRUR\n0e/3i7FMW6nrRkSMRqPq+/Z6vWLMjdmM+fn5Ykz1UUS3banGz/Lysiyr+sGZm5srxlw/XFxcFGOq\nf52ZmRkZV/Ph8PBQlh0MBsWY6yM1PtzYcHPcrR+K6qe7uztZVs0H14eqrScn9Rap2suVVWu4K1tb\np7Ztq6+LvImJCTl3Vd+59fHq6qoYc/NSzT113Qi9FjnqmaampmTZ+/v7YszNn93d3WJsa2tLllV1\ndnmZ24tUvV0/ZMrW5uVOZv90MnVW+YKbK+6Zzs/Pq+4bofvQzf+lpaVizI2709NTGVdUvTI58NnZ\nmYxnru1yJOWbb76RcbUeujVNjT2XAymLi4syfnR0VH1tNWbdPHR9rKh2VtclB/rxcvu6Wsfc/FFz\nz+2fal13+0XmHULFXVk15zc3N2VZVWe3l3R5LpK5tuonN3bUWpR5l3O5iBp3mZzPrcsu58/kjIrr\nB3XG5NpStVdX7zgR+uzCnQOpPsyMO0edmUXkzoHU+dTj46Msq/rJrQ3X19e6Yh1R66UaG66s6v+m\naXzF0KmmaarnqDujOD4+rrqu4843MvlI7XWdm5sbGVdr1cLCgiyr1iq3Rna5PiuuPRR15vaUuDI7\nO1uMuXxCvd+63yxUH7t55H7jybS1ykfd+YWKuzqpuLtvZv6rfnD5hKpX5r6OOkeOiFhZWSnGMu92\nbj6oshsbG7Lsl19+KeOKWg/V/I6I2N/fL8bcPFP9n8lF0a2JiQnZt2otcmNib2+vGMu8n7r1U53/\nu3PYrnICd12Vq7j1QrWH2z/dfpLpp8z6mdk/M98xqfHh7qvmg5oL7r6uD93YUu/Vbj6oZ3L5teoH\ntxepZ+7yPUbNNdf/7rsvRfWDq/O7d+9kXD1Tl+91isv51Dx17azGnXt/VmPazRU1z9Q70FN/D+Mv\nCwMAAAAAAAAAAAAAAAAAAABjio+FAQAAAAAAAAAAAAAAAAAAgDHFx8IAAAAAAAAAAAAAAAAAAADA\nmOJjYQAAAAAAAAAAAAAAAAAAAGBM8bEwAAAAAAAAAAAAAAAAAAAAMKb4WBgAAAAAAAAAAAAAAAAA\nAAAYU5Mf82ZN08TkZN0tDw8PZXx5ebnquhEh6+Tqq+p1eXkpy66vrxdjX331lSx7d3dXjD08PMiy\n09PTxZhr5/n5+WJsZWVFlr29vU3FFdUeTq/XK8ZGo1F1WdeWqr2Gw6EsmzEzM1OMuT4YDAYyfnFx\nUVWniIiTk5NiTNU5Qre1K9vv93XFBDXubm5uqq/r1h0Vd/NfrZVuHtWu3xF6rkTotcVRY0etd1mq\nPdwcVuu/WzvUfdXzTkzw7we9pNFoFFdXV8W46js3N9XanNlbHfU8KhYRMTc3V4ypOR2h55ebe0tL\nS8WYq7O69vn5uSzr1s9MP6l6q+ftUqb/MzlQl/uY20+Ojo6KMZc/nZ2dFWOqrSIijo+Pq++byYEU\nl4somT5yYyezVrq51NUzu3Gn3jevr69lWTW23FxS70iuD1VbuvdnRT2Py0PRrcfHRznHVN+p9TEi\nlz9lzoHUPufOENRa5OqcmSPqvnt7e7Js5swkk+O4a6v2yLwHZtYMtxdl9jmXX9Vy7+KuPVw/KbXv\nRxERp6en1WUzMs+buW4mj8nk3q5emXxS9ZO7r8q9Mufiri3VOu3mkjqPU+M5Irdn4WU9Pj7KM2I1\nh9w5++zsbHVZ9btEZp07ODiQ8YWFhWJMvds6bi1S8961lVon3fm/+73MnSUpqp/c7yG1143Qfeje\nQVVb3t/f64oJXb7vuWtn1uBvv/22GFtdXZVl1bhVfdQll1OocenKqrnW5T7o2jJzhqnaw+UyKl9x\n65J6JpcHZfpQ5UmZ91z8cLVtK8eFGk87Ozvy2mqNzOQTbv9Uc9Odq6g6u3mb+d5APZNrq0wO5Pax\n/f19GVdUP2TOZLrcTzK/Hai2XlxclGUzz+vy68xZgBp7rq1UH2fOApwuf2tXVB+7Psh8x5J5n8jI\nrMPuvS6zDitd1ln1k+r/pmnkdX+BL4cAAAAAAAAAAAAAAAAAAACAMcXHwgAAAAAAAAAAAAAAAAAA\nAMCY4mNhAAAAAAAAAAAAAAAAAAAAYEzxsTAAAAAAAAAAAAAAAAAAAAAwpvhYGAAAAAAAAAAAAAAA\nAAAAABhTfCwMAAAAAAAAAAAAAAAAAAAAjCk+FgYAAAAAAAAAAAAAAAAAAADG1OTHvNloNIrT09Ni\nfHKyXJ3l5WV57drrRkTc3t5WxVy9hsOhLHt4eFiM9ft9WTZjaWmpGHv//n31defn52Vc9VFExMzM\nTPW1M7pqa1fnh4eHYmw0GsmybkzX3nd6err6uo6bS6q9uqyXsrCwUF3Wjaubm5vqayuurVQ/uHHX\nJTUu7+7uZFnV1m7cqbm0vr4uy3748KEYu7y8lGVVP2XWJLWvuD0JL0uN1cFgIMuqOeLWhLOzM10x\nYW5urhhTczoi4v7+vhjr9XqyrBrLbn9UOdDu7m71fVdXV2VZt46pa7v+V2NHPW9ELp/I3FftN27d\nVmPatVWXVL2urq5kWTWXXB+pZ3Zl1bh0717qmdwcVn2s2iJC19n1fybPcX2ouPZQ/eT2blU2U+eN\njQ0Z/+abbzq5r6Pm2dHRUTH2+PjYRXXwTFTfuTwms3+qHChzXzfn1Trm5ryKuzqr922396p3jOy7\nurq3u7Y6Y8qcIbm8TXH7p3ped181pjPveq7/3d6cubZrLyUzD5VMfuxk8hhVr8xccVxbqvc+d7aR\nyc3U3j4xof82SuY9JvMbguL6sMtxiW49Pj5Wjw13znBxcVGMubm7v79fVacIPV4zZ+murHrezLVd\n/5ycnFRd9ynXVutzpj1c/2fW3+vr6+qyiquTOkfMlHU5lLu26mO3D6r3bteHmd+WMu8UGZl8Vc0H\nd131u7Nrx+Pj4+p6ZbhnUv2k1ix37ZWVFVlW7R1uvVNj2uU5Kq76sG1beV10q21b+a6gYi4HUv3u\n1t5MPqGu7eqsyro5r553cXFRllXc3NvZ2SnGNjc3ZVm3fqr+V+t2RK7/a6/rru3aUvWxu29mH1Nl\ns98MZNpajVv32/L5+Xl1nVR7uXGX+V36pXIg1R6unTO5pmsPd+9aKud33Bq+t7dXjKkxGaH7wY27\n2hzoqb+H2b8s3DTNdNM0/0fTNP9X0zT/b9M0/9t3//e/0jTNv2ua5j80TfPPmqaZetIdAQAAfgTI\ngQAAwGtFHgQAAF4jciAAAPAakQMBAPB62I+FI+IuIn6vbdv/LCL+ekT8l03T/M2I+IOI+Idt2/7V\niDiJiN/vrpoAAAAfHTkQAAB4rciDAADAa0QOBAAAXiNyIAAAXgn7sXD7537x31Pvf/f/2oj4vYj4\nF9/93/8wIv52JzUEAAB4AeRAAADgtSIPAgAArxE5EAAAeI3IgQAAeD2e8peFo2maXtM0/z4i9iPi\nX0fElxFx2rbt8Lt/5H1EbBfK/r2maf6kaZo/OTk5eY46AwAAfBTPlQMdHx9/nAoDAAA8k9o8iBwI\nAAD8mHEWBAAAXqPnyoGOjo4+ToUBAECVJ30s3LbtqG3bvx4Rn0bE70bEX/u+f6xQ9h+3bfs7bdv+\nzsrKSn1NAQAAPrLnyoFWV1e7rCYAAMCzq82DyIEAAMCPGWdBAADgNXquHGhtba3LagIAgKQnfSz8\nC23bnkbEH0fE34yI5aZpJr8LfRoRu89bNQAAgB8GciAAAPBakQcBAIDXiBwIAAC8RuRAAACMt0n3\nDzRN8yYiHtq2PW2aZiYi/ouI+IOI+DcR8Xci4o8i4u9GxL9015qYmIj5+flifDgcFmMfPnyQ156e\nni7Gbm9vZdnDw8NibH19vbpsly4vL6vLqjqrdnT3de28sLCgKyY8PDzIuBo7rl4q3u/3Zdm5ubli\nzPWRis/MzMiyah65tsrMFdXO7tpubKl7u3oprj26cnNzI+Onp6fFmGsrF1fOzs6qr+vmQ6ZeXfW/\nmqOOaquIiMnJ8haq5mhEbg1X91V9cHV1VX3P1+o5c6CmaWTfufVVGQwGxZj7T16q+eXmtPpPSY1G\nI1lWyZR14zzTB5l5666t+kH1b4TuJ3df1V5u/VT3vbu7k2W//vrrYmxjY0OWVX3o1K6fEX4vyuyB\nql6uLdX4OD8/r66Tm0sqv3K51/X1dTHm2nlpaakYc+N9f3+/GFtcXJRlnV6vV4y5/On+/r76vqqf\nVFtF6LZWbRWhx51qiwi9dziZnBC/uufKg1wOpGJu783sRWrNcOtJV3mbe17VVu5sanl5uRhzz6Ny\ns0zeFuHXSEXlKpncy1F9qN7zI/T66v7yknqmTB7i3l3de7Fa9zPjI3M+5fpf1TkzHzJrhytbe54e\nocdl5qw2IjeHVT+ofDFCr5duLqn54vKUzLtIZu/o6iwB3+85z4ImJydD/dc2Ly4uqmIR+veDzBrq\nxrmaf+48vLYtnJcq69Yqtw9m1mBV1t03s26ocefGzpdfflmMub/CrfYM91ua4s6g3JhWz5z5XcqN\nS9X/JycnsmxXv5e5tlJx14cq38icBblx5+ZoJg9y7aWoPnb/hWe1P7ixo57X9eHubvnbUldW9bEq\n2zSNvC7+suf+Pax2jrj5ofp9Z2en6p5PkXmfU23hnleV/eqrr2RZtc5l6ux+d3R7TWZdV/mXW08y\neZ+6thvraly6Oqu9KJNLuvtm9qmXyglcP2SeSXFjtqv7unmmfh90v4e5flA5cGZsuT5Ua0/mnSCz\nDjtq3cnk5aoPnpoDPeWE69ci4g+bpunFn/8l4n/etu2/aprm/4uIP2qa5h9ExJ9GxD990h0BAAB+\nHMiBAADAa0UeBAAAXiNyIAAA8BqRAwEA8ErYj4Xbtv2/I+JvfM///T9GxO92USkAAICXRg4EAABe\nK/IgAADwGpEDAQCA14gcCACA12PipSsAAAAAAAAAAAAAAAAAAAAAoBt8LAwAAAAAAAAAAAAAAAAA\nAACMKT4WBgAAAAAAAAAAAAAAAAAAAMYUHwsDAAAAAAAAAAAAAAAAAAAAY4qPhQEAAAAAAAAAAAAA\nAAAAAIAxNfkxbzYcDuPDhw/F+Pr6ejG2vLwsrz05WX6U6elpWfby8lLGaw2HQxk/PT0txq6urmTZ\nh4eH6vsqrq0y1+71etVlM/ft9/syrtpSxSIiRqNRMebacn5+vhjLjEn3vLe3t8WYq7Mq6+7tyiqu\nPVRbunGnru3a4+bmphibmZmRZdWapdaGCN2Wrq3UM83Nzcmyri1VvdRcidBzPDMuVTs7qn+75NpZ\nPZNqx7Ztq+uEvMfHR7m3DwaDYsztgWp+uXmtqDpF6LF4dnYmyx4dHVXVKSLi7u6uGHNtperlyqo9\nzq29a2trMq7WOddW6tpuDVRjR7Vz1tu3b4sxlwMr7nkz+aS7du3aHKGfOZMDuflfuyZF5PLJpaWl\nYsztgaot3ZhV7ZFZKyN0W7ocSMmM2Qw33jN5vbq2u68a75l9Bd0ajUZyLcuc5ai559YTdW23Bipu\n3Vbz2s15Fc+cA7h1SrWle17XlqofXE6gymbeA11bqn5w91U5kOv/TK6hZNb0CN9etfd29XLrQ+19\n3XXVfpOpk9rjInQfu7ZaWFgoxlRe9hTq3u5MVY2diQn9900WFxer75vpf9UPbs3K5ED48bq/v4+v\nv/66GH/z5k0xpuaus7q6KuP7+/vFmJsHKm9w+9HFxYWMK5n9Sp3xuvmn8iCXQ7nfB+7v76uvnbmv\n6n837jLn5apemfHe5VmQo363dr/xuH5S1DOtrKzIsicnJ8WYa0vV/xsbG9VlXR9lfg9XY8s9r3um\n3d3d6mtnqLHj2kP1Q+Z3OLc2ZMa7qpfqg8fHx+p7Im80GsX5+Xkxrt4FMmvR5uamLLu3t1eMufxJ\n5TGZ/dE9r7qvei+K0HPP1VnVKzvn1Vrl3ufcb22KGpOZ339ce2xvb1eXzZxBZbhrqzms2jlCzzVX\nNnMGpZ7JjTvVTy6Pzezbiiur1gfXv+6Z1Lrk3vkyz6zaMrOWut8R1PjIrP9uPE9NTVXd96k5EH9Z\nGAAAAAAAAAAAAAAAAAAAABhTfCwMAAAAAAAAAAAAAAAAAAAAjCk+FgYAAAAAAAAAAAAAAAAAAADG\nFB8LAwAAAAAAAAAAAAAAAAAAAGOKj4UBAAAAAAAAAAAAAAAAAACAMcXHwgAAAAAAAAAAAAAAAAAA\nAMCY4mNhAAAAAAAAAAAAAAAAAAAAYExNfsyb9Xq9WF5eLsYvLy+LseFwKK89Pz9fjB0eHlaXdfed\nnp4uxk5PT2XZycly8/d6PVn25uamGFPtGOGfSVF1vr29lWVnZmZkfDAYFGOuzqpeqq0iIkajUTHm\n+uHi4qIY6/f7sqyyvr4u4+qZXDureqm2iNDj3V3b9YMaP+6ZupqHbkxnxp2rl6LaQ9XJxV1ZNz5U\n3I0dNdfOzs5k2aWlpWLMrYcqvrKyIss+PDwUY65/1b7jnrf2umq9QvcmJiZibm6uGHfrTa2rq6vq\nsvv7+zKunmdqaqq6rFuLXL2Uo6OjYsytUypPcf13d3dXfW1H9bF7JsXlQOq+medRa3qEzgndfVXc\n9ZEbl5n8Wu2fLp9U9XJ1UvPQlVXjw/WD2udcrpGh6uXum8mRVDtH6Lnk7ru2tlaMuXVJ1dn1oRof\nbi6pXCXzHqvWu4kJ/h3pl9Tv92Nzc7MY39vbK8bcWpRZ19Xc3N3drS7r5q1aA1U7Rei2cjLra+YM\nwcmcxyjunUpdO5MDqXU5oruc31FrpKuTyydVefXO7LixpfrJlVVz2L0/qbKurdS7SIZbdxTX/64P\nVTwzdlwOrPr/+vpals28I2WoOrs9S1HP0zRN9XXxPAaDQXz++efFuMo53DxYWFgoxg4ODmRZdebp\n8qDV1dVizM0vdV6unidCrzduLXPrgqLOt9xZq4urZ3L9r8q6+6oc2+Xf6r7uNxzVx+63FMXdN3Nu\n4q59f39fXTbTD0rmNwB3XzUuM+dmmf7P/B7mZMZHJg9y7ZEZ06ps5jdt186qrc7Pz2VZ1VZqPzs+\nPpbXRbcmJiZkv6tx7sbE4uJidVlVp52dner7ujVfPa+6rivrqD3BvevV9t9T4pmyqo8z69js7KyM\nq/U10/9dUu2RzYFU+UxOkMlFXM6f+Y5JtYc7C1TnuZlzs5dqZ8fVK/NOkMnrVB+qnD6iu7Hjyta+\nazz19zB+NQMAAAAAAAAAAAAAAAAAAADGFB8LAwAAAAAAAAAAAAAAAAAAAGOKj4UBAAAAAAAAAAAA\nAAAAAACAMcXHwgAAAAAAAAAAAAAAAAAAAMCY4mNhAAAAAAAAAAAAAAAAAAAAYEzxsTAAAAAAAAAA\nAAAAAAAAAAAwpiY/5s2aponJyfItP3z4UIx98skn8tq3t7fF2PT0tCx7eXkp44p6Hnfdfr9fjJ2d\nncmyg8GgGFNt4bi2Gg6H1dd2Mv2grKysVN93fn6+uuz6+np12ZubG1lWjbteryfLKqPRqPq+zszM\nTHXZDDem1TM/PDzIsmoO393dybLLy8vFmJsLanwsLCzIsicnJ8VYZu2I8G2tHB4eFmNuLqk+dHVS\nz3xxcSHLqvXQzZXMeld736Zpqu+JvLZtZd998803xdiv//qvV9/X7QlqrXJlr66uquoUoeetu66q\nV6as43Izxc15dW23ju3v7xdjb9++lWXVmFS5prO0tCTjatxlcs1MnpJ53iz1zIuLi7Js5pnVHuiu\nq3JklwNl5qHi+vD8/Lz62u6ZpqamijGXT6r+n52dlWUzOZBaL908zKwdar1z983mqngZj4+Pcrzt\n7e0VY9vb2/LamT1DjcVMDjQ3NyfLqrmp2iJCr80uT1H3detUV/t2hK6X64ednZ1izOVAmfXTnZso\nmXfmzPtnpmyGOjN5Sry2rOujzHuMai93XdX/7n1B3df1oaqX27ddH6m92ZVV9crMFfdMR0dHxZhr\ny8x5bGbtUO2s5nfbttX3xPMYjUbyfPHrr78uxn7zN39TXtudWyrqfNidLb97964Y29rakmXVHDo+\nPpZlu+Lmpjpzcbr8PeTg4KAYe/PmTfV13Rqa+V0ikzdmxvtLcW2pcmz3m6ZqS9cPqi1dH6k5nJkr\njqpX5vcf9x7kZMqr9trY2JBl3e/HimrLLn//V/tOph3Jg368VD7x2WefdXZfNX/cvq3OINy8Vdd2\nZ9bqvSpT1s35zN6b2U9cP6g+dL+lKO6+qq3d3qu4NbCrM5uXPAtSXD+ouLvv9fV1VZ0idHu589yM\nzJ6vZL6BitDt4cqq9z43lzLtoeawq7Mad+4sSJ33Zc5znwN/WRgAAAAAAAAAAAAAAAAAAAAYU3ws\nDAAAAAAAAAAAAAAAAAAAAIwpPhYGAAAAAAAAAAAAAAAAAAAAxhQfCwMAAAAAAAAAAAAAAAAAAABj\nio+FAQAAAAAAAAAAAAAAAAAAgDHFx8IAAAAAAAAAAAAAAAAAAADAmOJjYQAAAAAAAAAAAAAAAAAA\nAGBMTX7Mmz0+Psbt7W0xPj8/X33t4XBYjE1O6sdU9728vJRl1fMsLy/Lsqenp1V1cvVyz5tpq5mZ\nmWLs5ORElr24uJDxu7u7Yuw3fuM3ZNmzs7Ni7OHhQZbd3Nwsxvb29mTZXq9XVSdXr+npaVm23+8X\nY6PRSJZVdVaxCD12IvR8cGNatUdmHrq2dOOjtqxrSyUzh908U+2h2jHCP5Mrr6jxoca7u69rS8Xd\nV3FzJUPNB9UWj4+PXVQHvwI1LpaWljq5rpsDas9w6+fV1VUx5taLubm5YszNH5UvuHVIXdvtnxsb\nG8XY7u6uLKvq7Lh+ePv2bTGm+igiYmtrqxhzz6T60N1Xcc87GAyKscya78ZdJgdy81uV7bItVXtl\nxqybh6perp1Vnd0cVlyu6fLYTA6UGbddyeSibuxkclH1fvVDbEc8TeYcSMm8j7k6qbGYeafKzD1H\n1cvVWZ1tHR0dybKuzpm1W+Vm7plUH+/s7Miyqj0y+4Gj1k83ZlU/uD3QyZwDZfINdabqcqDMXqTa\nMjNHu5z/iusDlXtH6DModz6ldHn+pLixkzl/UnE1niMirq+vZRw/XO73MLWXOWpMZc67HfX7kFur\n1Fnr6uqqLHt8fKwrJqi1zrWV6iO3zmXWwYODAxl/8+ZN9X3Vb20/+9nPZNlMe2TOLxcWFmS81s3N\njYy7/UiNLbeHqrjbFxQ1RyO6yyky5zmOmqdubGTmoft9SMXd2qL6yT1TZo0/Pz8vxmZnZ6vv68ad\nyrHc83S5p6E7bdvKdUHt+13+NqvGqtsTMjmQ4spm5p7Kn1w7q2urteQp11bl3bq9vb1djLl88Sc/\n+Ukx9sUXX8iyqj1cnVUfZvrBjVl132w+oK7t8npXb0WNHdeWXc3TzBzOcPlC5v3J1Vm1tetflRO6\nPlRzLXOu5u6rvhl067Bqy0xbPQf+sjAAAAAAAAAAAAAAAAAAAAAwpvhYGAAAAAAAAAAAAAAAAAAA\nABhTfCwMAAAAAAAAAAAAAAAAAAAAjCk+FgYAAAAAAAAAAAAAAAAAAADGFB8LAwAAAAAAAAAAAAAA\nAAAAAGOKj4UBAAAAAAAAAAAAAAAAAACAMTX5MW82MTER09PTxfj6+noxdnp6Kq89Pz9fjN3e3sqy\nw+GwGFP1dfVy9+33+8XYw8ODLDsYDIqxm5sbWXZystztl5eX1WWvrq5k2aWlJRnv9XrF2IcPH2RZ\n1dZqbGRPMFP8AAAgAElEQVTNzMwUY64fMvU6PDwsxtQ8itB9eHd3J8uqcRfh54ui5oO77mg0qopF\n6PZS7Ryh2yszD9WaFKHH3cXFhSyr5oprK9f/GW7sKa69FDXuMlQfRUScnZ1VX1vNBzVmHx8fq++J\n56Hm/eLiYjGWWZvd3qzWqsz8OD8/l3G157v5odpRXdfFXZ2/+eabYszlfHNzczKu1l93bRdXVFu7\ntlTjzq3LmT48OjoqxjY3N2XZzLhbXV2VcbeHKionVG0Voee464fMvp7Zt1Vbq7UwQrezW++6zGPU\n3uzmqBqXblypeCY/cvNQPa+7r5rDbryrOb6zs1OMkQO9rLZt5bhQ/ermT+bdNjOOM/NWcc+bmfOZ\nttrb26u+r6PKuzXB5W5dUXmd24vU2MmcP7qymXZ2Pvnkk2Isk6e6c1Elm5srmTmuZPoh086urMuf\nFhYWqq+dkXlXzZzHqbnmztTV+uDOiDc2Noqxr7/+WpbFy2qaRo65N2/eFGOZvd3tC2q8ZuaXq7M6\ng3Jn2rXXdVydd3d3izG3dmf2Mlf25OSkGHPn0mrdcGUV1w+Za+/v7xdjW1tbsmxm3V9ZWZHxrvY6\ntb9G6Pni+kFd25XN5P6qrdzzqj7scv679VCttZnfB9V4j9BzyY1Jdb6ZKXt8fCzLqjUt0w9qLrRt\nK6+Ll6XyW5cTqDXDneGr8w0357vcIxVVL3cukimr4q4t3LXVeuPWBLfeKKr/3V60trZWjLnnVe3l\n2lI9ryur4uosPSJie3tbxrPngSVurqgxnXmPcbl3JudTbeX6UD1Tpq0ya1KErrdrSxXPzKXMt3nu\nrE+9E7izoEydu8ZfFgYAAAAAAAAAAAAAAAAAAADGFB8LAwAAAAAAAAAAAAAAAAAAAGOKj4UBAAAA\nAAAAAAAAAAAAAACAMcXHwgAAAAAAAAAAAAAAAAAAAMCY4mNhAAAAAAAAAAAAAAAAAAAAYEzxsTAA\nAAAAAAAAAAAAAAAAAAAwpvhYGAAAAAAAAAAAAAAAAAAAABhTkx/zZo+Pj3F7e1uMT09PF2OffPKJ\nvLa67nA4lGUnJ7tphvn5eRlX9bq7u3vu6vyFq6ur6rKqj9bW1mRZ1UcREaPRqBibmZmRZVVbqjpH\nRPT7/WLsw4cPsqzqYzeuVL1cH83NzVXFnC7nihvTDw8P1fdVz6zGVUTE2dmZjCu9Xq8Yc2N2MBgU\nY6enp7KsaqsMN3ZcvQ4PD4uxlZUVWXZpaakYc32k+sGNHXVfNx/UPD04OJBl1Xp5dHQky6q1I7Ou\noHtqTKmx6vYxNWZcWbdGKmruqTUuor4tXFlHPa/bp9S8VWtJRG4dU7GIiOvr62JsY2NDllVt6caG\nai/XRyp/cvd1ba1kxqyj9lC3/mbGtJovbuxk3jdULuLuu7q6Woxl8rIMV+dMTuCureL39/ey7OXl\nZTGm2jlCjx33vOq9zo13NYf39/dl2ampKRnHD1PbttXrnMsJ1JqRWQPdfVXc7WNq3mbyMrePnZ+f\nV9UpImJ5ebkYc/uye7dRYyOTX7mymT1QvRe7PlT5pDszW19fL8bcmFXjw63b7n1CjR/3TBmqXm7+\nq35yfajGrHtelQNn8kE3nlV7ZPo3IpcDqXu78yd1luvuOzs7K+OKys1cP6h1aWdnR5bNvD9jfF1c\nXBRjao5E5M6g1BmvWwfVWM78LrGwsCDL7u7uFmPuHWRra6sYc+18c3Mj46q93PucqrdrD/W7hRpX\nEf6Zau/r+kGdb6lzsQi97ru2ynDv8+r9NjMfsuNSUWdB7vcwtba4cZfJKV17KMfHxzKu6uV+D1N9\n6H5bUn3o1vDMWqqe1/1mubi4WHXdCP1Mqn+bppHXRbeapqnOU90eqOamu6caM5m82s2BTJ0Vt/aq\n9dWtcdvb28WYW1sz67ZbP1XO4H4Py3xfoc7VMmvg3t6eLOu+v1JUPqHqFJHLF7qU6cOuvq9xc1jt\nry7nU3XOlHV7vsvN1Lh1Y0utW25dyuSiGSqvd+NK1StzFqjm6OPj45OuwV8WBgAAAAAAAAAAAAAA\nAAAAAMYUHwsDAAAAAAAAAAAAAAAAAAAAY4qPhQEAAAAAAAAAAAAAAAAAAIAxxcfCAAAAAAAAAAAA\nAAAAAAAAwJjiY2EAAAAAAAAAAAAAAAAAAABgTPGxMAAAAAAAAAAAAAAAAAAAADCmJj/mzSYmJmJ6\neroYn5wsV+f09FRe+/Lysuq6ERHD4bC6rIqr60ZEHB4eFmPz8/OyrHre5eVlWfb9+/fF2MzMjCyr\n6ry9vS3LjkYjGV9fXy/GBoOBLHtzc1OMZdryJz/5iSybcXV1VV1WjTvVvxG6ndX8fApVL3ft29vb\nYuzu7k6WVX2orhsR0e/3ZVxx60MtNw8V1RaOaytHjS1XL9WWbg4ras2KiOj1esWYW7NU2cy649a7\n7DzFD1Mmnzg7O6u67lOuXcvdV9VZza0IPUfcfnF+fl6MPTw8yLJ7e3vF2NLSkiyb4fap2dnZ6mur\nfnJrUYbq/7m5OVlWxY+OjmTZjY2NYmxtbU2WdeNDjT3Xliru8kV1X1c2k8eofS6Ti7i2Us/b1XoW\nkcuR3Lqkntk9k1p7jo+PZVnV/67OGWqeuvmvnknVuW1bXzF0pm1bOYfU+YWbeyruxpOSWcdcHqPm\ntVsD1dq7s7Mjy6pru3cXVWfXR24dy6xFL5XHqNzM5W1qb3ZjJ3NmquaZy4FcH6t6qTOCp1y7tqx7\nl1dcPyjuXV1dOzOXXJ6q4q7O7trqmTLzIXNWp975IvSYdf2g3r3c2KndC11ZFSMH+uFTY93toQcH\nB8XYmzdvqut0cXEh45nf0jLr0cLCQjGm2sKVdefwqs4vdeYWEbGyslJ9X/VbWubc2d13f3+/GFN9\nFBFxf39fjE1NTcmyqo/d856cnMi44uqlZOaS6l93bbf/ZsZ0Ju9Tde5yzcrkI13WS41pdxZUW6eI\n3NhR+Zkrq55XjXfyoJfVtq0cU6pfXT6v4qurq75yBZk1LjOOHfVM7ixIcd/1qHZeXFysvq/j9gt1\n78y63WVZtVa5tszsgaqPMzlwRO6MIkO1dea+XeZAtftYRC7nU2Xd87i2zKzhGeq+bj649lIyZ1/q\nvuo36wjdD+q+TdPI6/4Cf1kYAAAAAAAAAAAAAAAAAAAAGFN8LAwAAAAAAAAAAAAAAAAAAACMKT4W\nBgAAAAAAAAAAAAAAAAAAAMYUHwsDAAAAAAAAAAAAAAAAAAAAY4qPhQEAAAAAAAAAAAAAAAAAAIAx\nxcfCAAAAAAAAAAAAAAAAAAAAwJjiY2EAAAAAAAAAAAAAAAAAAABgTE2+dAV+2XA4rIpFRExPTz93\ndSIi4vb2VsYzdZ6ZmSnGTk9PZdmbm5ti7OHhQZb95JNPijFX516vV4zd3d1Vl43Qz3RxcSHLqv4/\nPDyUZdfX14uxs7MzWXZpaakYc8+rjEYjGe/3+8XYyspK9bXdeFdtFaHHj7u24tpS9b+7rxp38/Pz\nsqzrJ0W1lZvDquzkpF7WVfzq6kqWzVDrXdbJyUkxtrm5KcuqOe72FdVemb3DjSt1bTVm3b6C7rn5\nWeLm5tbWVjF2fn5eXSeXEyiu7NzcXDHmnvf4+LgYc3NPPa9bL3Z3d6uu+xRqn7u8vKy+rsvN3r59\nW4ypdo7QfahiEbqfMuNO5WURuRzI5SLq3i6fzDyzqpfrh6Ojo2LM5UBqzK+trVXf141Z1Ydu356d\nnS3G3FrpTE1NVcUiIq6vr4uxwWAgy6r10s0HNy4VNe7cXFleXi7GXK6i5ooa7249Q/fUmpHZE9Q4\nd+uJqpPb1zO5vJoD7uxiZ2enqk4Rer1wa41aL9x93f6q9huXA6l+UnlbhN6rutqXI3Q/uH1bPa/b\nt2vrFOHHh6q32vOdTH6dmQ/uvq6flMzeq57J9X9mnjkq/3LrYebcVK0tLhdV/ZAZd5nndWO2tv8z\ncxDPY2JiQp6JqjNgdd4ZEfH5558XYy7/Vb8tZM6l3W84CwsLxZh7XpUHubxPPZNqiwj9TJnz3wi9\nhrp+UPVyz6R+P3J9qMazynMjdK7r7ruxsVGMubVbtaW775s3b2RclXf9r84CMnuk+x0m836s5nBm\nL3NjVnHP29VvhxG5c3V3bUX14erqavV9XZ27Wktd2f39/WJMjUlVDt1rmkbOETWe3PxQv+Nkfg/L\nzHm3tqqx6u6rcqDMWpLh2tnlMZlvBtRa5ea9yidcH6r+z6y9jloj1biK0P3g1nx37cwzZeqlxo7b\nTzK/Lat8Qv3u5O6b4eZRZt3JnI07mW+kVD7hxuz9/X0x5n7Dy8jMFbXWqj5qmuZJ1+cvCwMAAAAA\nAAAAAAAAAAAAAABjio+FAQAAAAAAAAAAAAAAAAAAgDHFx8IAAAAAAAAAAAAAAAAAAADAmOJjYQAA\nAAAAAAAAAAAAAAAAAGBM8bEwAAAAAAAAAAAAAAAAAAAAMKb4WBgAAAAAAAAAAAAAAAAAAAAYU5Mf\n82aPj49xe3tbVXZyUldVxS8vL2VZVSd33+np6er7KgsLCzKurn12dibLquddW1uTZa+urmRceXh4\nkHHV1q4f5ufnq+oUodtyMBjIsqotXR/2er1izD2vast+vy/LLi0tFWNufrr48vJyMabmSkTE6elp\nMTYcDmXZmZmZYsyNWddeihvTtdzzKq6d1fowNzcny7r+V/dW/euufXd3J8uqPnR1Vn2o5qiT2Ttc\n/3/77bfFmHrex8dHeV28LNXvbkyovcqVVWPVrXFuvVFUvdxapNYxt17U1ikiYjQaVZd1e5HKv9w6\ntrW1JePK3t5eMZbZt13+pMZdZly5tVflQC5vd/mCemZ13wg9PtyYVu3l3gkyOZCqcyY/UvPMcfv2\n8fFxMebWncz64Npjdna26rqOK6virj0Ut96ptlTvEhER5+fnxZiaC03TyOuiW03TVK/7bm3O5E+Z\ndy7FrUVqjrh9rKv3E7fXqDq7Pc49U+19I3T+lNl7M+/FLo9R9cq0lcu9Mu+fmXptbm7K+NHRUTGW\nmaOZM2Q3z1R+lbmvy/kz/aDa2c2VzPtVhstjJybq//6JGluLi4uy7PX1dTHm2jJz/owfr7Ztq8/x\n3W8Lat3IvEe49VftKe7dR13brXNTU1PFmHtede2bmxtZVtU5807tHBwcyHjmLMg9c1dlV1ZWijG3\n/97f3xdjLv9yc0lx+7MaA+o3K+fi4kLG1e+hbj6o9nDPq+qVmQ/ueTN5kJpLakxG+D5U8yHze3jt\ndxRPuW8mx1btkfnt0I2d2t//2raV18XLUmMxs352ddbjZOqc2Vvd+phpj8z3UxluT1D7mNvz1bXd\n+qnirg/V+Ogyn1T3dWPD9UNmzKu2zIwtV6fV1dViTP3uEKHbS/3uFJF7F1FxN3ZUWXf24doj811f\nJn9S7wQZrs6Zeaqu7frhw4cPVXV66u9hTz5Za5qm1zTNnzZN86+++99/pWmaf9c0zX9omuafNU1T\nflsHAAD4kSIHAgAArxE5EAAAeI3IgQAAwGtFHgQAwPj7Vf41/L8fEV/80v/+g4j4h23b/tWIOImI\n33/OigEAAPxAkAMBAIDXiBwIAAC8RuRAAADgtSIPAgBgzD3pY+GmaT6NiP86Iv7Jd/+7iYjfi4h/\n8d0/8ocR8be7qCAAAMBLIQcCAACvETkQAAB4jciBAADAa0UeBADA6/DUvyz8jyLif4qIx+/+91pE\nnLZtO/zuf7+PiO3vK9g0zd9rmuZPmqb5k5OTk1RlAQAAPrJnyYGOj4+7rykAAMDzIQcCAACvUXUO\nFPGf5kFHR0fd1hQAAOB5PctZEDkQAAA/bPZj4aZp/puI2G/b9v/85f/z9/yj7feVb9v2H7dt+ztt\n2/7OyspKZTUBAAA+rufMgVZXVzupIwAAwHMjBwIAAK9RNgeK+E/zoLW1tWevIwAAQBee8yyIHAgA\ngB+2ySf8M38rIv7bpmn+q4iYjojF+PN/q2i5aZrJ7/5Nok8jYre7agIAAHx05EAAAOA1IgcCAACv\nETkQAAB4rciDAAB4JexfFm7b9n9p2/bTtm1/EhH/fUT8723b/g8R8W8i4u9894/93Yj4l53VEgAA\n4CMjBwIAAK8RORAAAHiNyIEAAMBrRR4EAMDr8ZS/LFzyP0fEHzVN8w8i4k8j4p+mKzNZrs709LQs\ne3l5WXVdd211XXft4XAoy97c3BRjDw8Psqyq89nZWXXZ09PT6rInJyey7Keffirj6t7z8/Oy7N3d\nnYwrqq3duKu9rrO3tyfj29vbxViv15Nl1bh0z+vmkhvztUajkYyrZ56bm5Nl3ZhXMs97eHhYjK2s\nrMiyFxcX1fdVbXV7e1t93YiIq6ur6mursdfv92VZtU67Ma3WSzc2MnNJlVXt6MqqOdo03/dfC0Kl\nXzkHattW7lVqrXLjSV3XrdsZap9z6/bx8XExNhgMquvkyrr5pSwuLlZfd2lpScaPjo6KMfef7HJ5\nn6Lay42d5eXlYsztj+ra7nnUXHF7vuLyJ5fXqWdyZdW9M23p2qOr/Nk5Pz8vxjJ1duuOiru2cPMh\ns7aoNf7+/l6WVfWampqSZVVbZ54ns++4+a/mQ1fvIfhLPuo5UKZf3VhUcfceoMq6+6r3IldWrWPu\nfav2HSJCrxeZ97wIf+bWVVkluycoqj3cOZAq63LgzPqZmUtufKizvnfv3smymfM69cwuJ1Rt7fZP\nNWZVbh2RO69RdXbXdbmZeo/J9JEb05lcVOVIKk+NyPVD5ixf4Rzoo6nKgR4fH+W4UWeebv1V49Wd\nLe/v78u4oursznAzZ9rq3cit3Wpvd/vgwsJCMeaeJ9MPmTVU/e7ouHGn+ti1peon9VtJRMTbt2+r\n71s7B59CjQ83z1Rbu7Gl7ptpD3XdCH2e68adirv7qvZwfahyBldnd+3MM2VyCnXfmZkZWVb9ju/G\nzvX1dTGWmUtuvHf5+wae7KOeBblcX8W7/I5BjXO3nqj1U/3uFOHPCrqSyYEy63pGdl1X1DO5Mav6\n2PXvT3/6U10xIXMWuLq6KuOqrV07q3qpuRKh9znX/+59vytqvLu5kulDNS7duHP5RKZembU0Y3Z2\nthjrak2KyI13RfVv27ZPusavlGG1bfvHEfHH3/3//2NE/O6vUh4AAODHiBwIAAC8RuRAAADgNSIH\nAgAArxV5EAAA423ipSsAAAAAAAAAAAAAAAAAAAAAoBt8LAwAAAAAAAAAAAAAAAAAAACMKT4WBgAA\nAAAAAAAAAAAAAAAAAMYUHwsDAAAAAAAAAAAAAAAAAAAAY4qPhQEAAAAAAAAAAAAAAAAAAIAxNfnS\nFfhll5eXxdj09LQse3t7W11WcWVPT0+ry6o6q1hExGg0kvFaZ2dnMr65uVmMzczMyLJXV1cy/umn\nnxZj79+/l2UV1w/9fr/62uqZXR+p8T4YDGTZ5eVlXbGOuPuqtlZzxXHtobi5pOrs+nBysn4JXVlZ\nKcZubm6q7+vm4XA41BVLmJ+fL8YeHh46u2+v1yvG9vb2ZNm5ubliLLMOX1xcyLLq2qpOEbqdXZ3x\nctq2lfNvd3e3GFtdXZXXVvurmh8RuXwiU1Y9k8tF1BqYWeNcnqLaMrPnR0RsbGxUX/vu7q4Yc+tJ\nhmovd19V1uVtqh/cvqz2V5cPqj6K0GPPzcPj4+NizD2Tirs9QdVLjasI3V5uz1fjw9030/9dWlpa\nKsbc/Ff95MaOGnfX19eybEYmN1d9rHKcCD1X8ON1eHhYjK2vr8uy6h3T7Z+ZnCFTVr1Tu3fmzNmW\n2nvd86h1yuVPjpr3LifMrCdqz3BrXGbdVuPd5U+qzm5sqD52beXqpfrJ7c1qnqq9NSI3D9W1Xf6k\nninTh+55VFnXzioXcXmKOwdUz5x5v3JtmTlTV6ampqrLuj5U7eHWndqx07atvC5enspvFxYWqq97\nfn4u4+p9zs2h/f39YsydS6uzIHUuFhGxuLhYjLk6u/dMRZ0juDN8ty6oPnZl1dlzJmd0ZyPqfd+V\n/fbbb4sxN3YUd16j+sntoe5MVl3b1UvNpcwzOeqZ3XXVmM3sv13OFdWWmXaM0OPW/T7U1X1PTk6q\nyzqZ3yXV73Ruvzs4ONAVww+S+z1M7SduPKn5pfKFCJ0juTNtl18p6vsa9zt2Zt6qXNM9j1rXM20R\nofdXtT9G6Gdy/a+4PCZzBuH6uJYbG5kzCHftzLhUMn3odNUe7jeLTB6j7uvymEwfufmg1uHM73Tu\nvuo8z+Ve9/f3xZhrq0xZtbdk1hXVVk3TPOka/GVhAAAAAAAAAAAAAAAA4P9n725+88ry/LCfhy8i\nKZLii1RUtVRdXV3dg8nMtOFJ0MgmQIDAQLb2xt4OEgOzy97/RlYBDAfBLBLARgDDXhkJZmUYcJAJ\nEsPBZAYeD7q6W1USJfGdIimSerJwTeAYfb8/9j31lKoffT6bnulf/e4997zf89xmAQDMKR8LAwAA\nAAAAAAAAAMCc8rEwAAAAAAAAAAAAAMwpHwsDAAAAAAAAAAAAwJzysTAAAAAAAAAAAAAAzCkfCwMA\nAAAAAAAAAADAnPKxMAAAAAAAAAAAAADMqaX3XYBvyurq6mDs8vJy9HWXlsZX0c3NTYxvbGwMxl69\nehVzr6+vB2OLi4sxN9XHo0ePYu7Ozs5g7MWLFzG3KtfR0dFgbHt7O+amdkr13FrdTkl6pqrfpTJv\nbm7G3PPz88HYw4cPY26SxtFdpLqsxlLqe9V4SHWd+mx17TTOWmvt6upqMFb196Tq72mszFJ63tZy\n/6n6VmrDqi7X1tYGY7e3tzH3+fPng7GqHVK5qv6e4ml83+XafDdNJpPYdisrK4Oxqh+vr68Pxqo1\nrqc/XVxcDMZ65sCDg4MYT3NRqscqt2f/VNVzde007nvmkx7VnP/gwYPR197a2hp935OTk8FYtdak\nPWE1zqo2TvlVudIYrvpOGi/Vfav1Jkl7pGrvfXZ2Nhjb3d2NudX8kKS6rPpdKnNrec/Q8y5StdH9\n+/cHY2/evIm5x8fHg7HUJ1vL9dXz7l2Ns3v37o2+Nu9PtQdK82c19pJqT1D186Snn6fn7d1PJKkN\nes4uqjJVbdizj0ltPKtzntbq98Sx1672Iqkuq3OgWZ5tprHUc98qt1q7k57+0WOW7TD2vtU+pWd+\nqObZ1Oerc6+e+bDaIyXpeXvm8J59eWqj6XQ6+rp8MxYWFuJ7WeqPp6en8drLy8ujYpXqPLzn94NU\nruq6s5q7q+umdqjqeZZnuD3zUcqt2r96Z0/Sb17pjLG13A7V7z/pedPvCq3V9dHTDqnc1fh/+/Zt\njCc97wVJ72+LSdU/xuqdV6rfcZPUDlX794ylZ8+eDcYeP34cc5Pq+4A0X1b9Lj3v/v7+YMw+6P2q\nzoJSX63GZjV3j82t7jvL/VWSytVz3ep50lzUu35W81yS7l3dNz1zzx44ndG31tdnU1317MsqVZlT\nv+xZt6v9c6qP93XmUu0HesZw6rOz+n3/Lvb29gZj1fju2QOl3Ko+0m+L1dlYOr9Kv1m3ln9Lr/ZP\n6bf0w8PDwdhd90D+sjAAAAAAAAAAAAAAzCkfCwMAAAAAAAAAAADAnPKxMAAAAAAAAAAAAADMKR8L\nAwAAAAAAAAAAAMCc8rEwAAAAAAAAAAAAAMwpHwsDAAAAAAAAAAAAwJxa+jZvtrCw0FZXVwfjR0dH\ng7GU11prNzc3o8u1tDRcDc+fPx+dW5UpXXt5eTnmpvjt7W3M3djYGIxVZX716tVgbGtra/R9W8t1\neX5+HnMXFxcHY9UzVeVKdnZ2BmNnZ2ejr1vVZU+/63nenmunMreW66vKTfGqHVLfub6+jrmpnar7\nprp6+fJlzE3Pm8Zoa62tr68Pxqr2TXXVWmvHx8eDsaouk6ouU32srKzE3O3t7cFYNQ9fXl7GeJLW\ntOp5q/WQ76bJZBLH0NXV1WCsavNqjRzr4uJidG61F/nyyy8HY9Vck6R6rOJpfmwtP1M11/Ss6/v7\n+zF3b29vMFbVR6rrqt+lMqf1oLXWHj58OBir2qFnDkzXrvpsVZdVuZPUf3rWmio3tWFVH7Nax6q+\nk/psVeYe1Z4gzcPVHiiVu+eZ7t+/H+Opf1R776TaT6ZrV302tUPqV5PJJF6X2ZpOp7FfjI21lufm\nnjlwlmMgnXtV903Xrp435Vb3TXNcNT9We6R072r+TGtCVR/pfbyae1O5qvpIqr1E2k9Wz1vtRZOe\nfllJudXZVXrmnv1Tz/lTz967p8xpXmmtb//cc5bXc6ZSjf/0TJubmzE3tVPVn3vO1Hqk501jZWHB\n34l536p9UFpzdnd347XT+fHp6WnMTX2qmjPSuK/Gwc9//vPBWM/+6+3btzGe5pTqece+g7RWz+1r\na2uDser3gfS7VNUOqX+kM6bW8t6u+l0itXGqiyq3qudUV9V9q/Uo9Y+edqjW0GrPkaRyVWtoUs07\nqQ2r3GRW3wa0VrdDz9l56rfVM6X4kydPYm5PmVN99XzTcHh4OJMy8X5Ve6A0v/bMzT3zSSWNn6ov\nfvHFF4Ox6nmTnt/eK2nczvKc4OTkJMbTHrm6b3qmah5LZ+0HBwcx9+nTp4Oxnnms6juprqr1oIqn\n+qrKla7dk1u1f/V7SZLGWrUX7TmTSfft+R2mqudqPKQ+39OG1e+D6Xyzum86O6nOTdPz9pwFz/Ic\n6S6cGAEAAAAAAAAAAADAnPKxMAAAAAAAAAAAAADMKR8LAwAAAAAAAAAAAMCc8rEwAAAAAAAAAAAA\nAMwpHwsDAAAAAAAAAAAAwJzysTAAAAAAAAAAAAAAzCkfCwMAAAAAAAAAAADAnFr6Nm/27t27dnl5\nORjf3t4ejL169Spe++OPPx6MnZ2dxdwU39jYiLlJetbWWtvZ2RmMVc+7uLg4GHv06FHMvb6+HoxV\ndZXue3p6GnNvb29j/OHDh4Ox9fX1mJusra3F+NLS8DC4ubmJuV999dVgbHNzM+amdqj6Tk+/TLnV\nfbihK98AACAASURBVFdXV2M81WUlXbt63p4xnPpl1WfTfat+d3FxMTo3jcOqDVOfrtq3Z35I/b21\nPOdVbbi8vDwYq8qcrl3VR3reFKv0rFl8d00mk9indnd3B2NXV1fx2nt7e4OxL7/8MuZWY3Os8/Pz\nGH/y5MlgbJZl7qnn9EzVmD84OBhdrtS+d7n3WNV6ktbI1L6t5Xms2kuk561y0xisnrdnL1rtJ7e2\ntgZj1V7k5ORkMNazL6ty0zOldbm1PJZ67lvV88rKymCsGkcpt7XWXr9+HeNJqo9qL5K8ffs2xtMe\nqKqPNF6qsdLzvNXawnfT4uJi7G/V2E165uZq3k/SXFXtJ9K519HRUcxNY6DnvOX4+Djmpjmhum9V\nH6kuqzkh1WWlp9+ltffTTz+NuanfVWVK7VS9M8/qrKa1vjOG1LeqftmzRiZVn01tWK2fabxUbZTu\nW+WmeM/eq7V635ekc+SqfdM7Yc/7YvXuld7revpk1e9SO9y/f38wtrDg78S8b4uLi+VvBEOqvcpH\nH300GKvOVXrOtNO8kc67W8tjrDo36Vm7032r37TSnNIzV7WW6yv9dthaboeeNqzK/Itf/GIw9qMf\n/SjmpnfU6v01xXvqqnreap1L+T3tUPXLNK9U4zCp6qNnHKbcnn1Qj2p+7vmtrWqHnr1dUs3/qV9W\nfTbN09UeKtVV1d/39/dHXZf3azKZdK1VSfotpWc/0fNeVfXjp0+fDsZ6ylyNgQcPHgzGqjUuPVN1\n32oOTO3/+PHjmJvWqmouSvNJ1f7pLCi1b6VnHqv6ztj3kNZy36lU7Z/KVT1TzxlEasP0Xt1a7nc9\n+8Uqt2dfl1Tt23OuUpU5zS1VO/Tc9969e4OxavynOSutSa3l881qjFZrSy8nRgAAAAAAAAAAAAAw\np3wsDAAAAAAAAAAAAABzysfCAAAAAAAAAAAAADCnfCwMAAAAAAAAAAAAAHPKx8IAAAAAAAAAAAAA\nMKd8LAwAAAAAAAAAAAAAc2rp27zZdDptNzc3g/EU297ejtc+OzsbjB0dHcXcy8vL0fdN13727FnM\nXV9fj/FkaWl8011fX7+X+1bPe35+PhibZZkXFxcHY7e3tzF3Y2NjVJlaa211dXV0burvPdet6qrn\n2ml8V9LztpbLlfpVpWeMXlxcjM5NfXKWqnquxkOyvLwc42ksVbljr9taa4eHh4Oxq6urmNvTTqmu\nX716FXNTf0+xyWRSF4yZubm5aa9fv57JtdN1q/Xz7du3g7Gtra2Ym/rxwcFBzO2ZX9Pe7MWLF6Ov\nW9nb2xuM7e/vx9x79+7FeKrLau5Nc1G1j71///7o+z59+nQwlvbWlWpuTXNzNeenvUi1B6r2MT17\n5PRM1X3TWKpyq3VurKr9UxtXbVjtVZK0J6z2uFVdpvxqLKW+U42Hnr1Zz948tVPV/qkdjo+PR5dp\nZWVlMLaw4H8j/T7d3NzE/W3qx1U/Te1e5aa+WO2Bkmovn8ZPz/t4NfZSvLpvilf3rebX6rwuSXvg\n1Dday3ukqu88fPhwMFbVR+rvVV31nD+lZ+o552kt94+eM4aqHVK8Wh/TGUO1P+rZ86V5p3o/SmOl\nasOe8V+N0Z55KeX2nANV78Bpjq/OENPcUuWmPtszF/Lddn19Hc8Let6dT09PB2M9c2iPly9fxvjm\n5uboa6f5qjqHf/Pmzej77u7uDsZSG7TW2tra2uj7VtdO9VHdN7VD1Td+8IMfDMaqdkjX3tnZibk9\nz5tU6031Tj6r/XnV/j162r9nT5l+h+l5D0rXbS33j6qNen7j65nvevS0YfW8qS6rdkj7s+p3hHTf\nVM/VeT2zdXt7G+ey1CeqftzzG1C6bzVu0xg5OTkZfd9KmiOr+TOVuSpTmnt716k07qtnSmt3z3xS\nSXvCnr3oJ598MrpM6TfL1vreq3t/L0tmVa6qT6f7Vrkpnn5nba1vvKTn7Vnzq9xqLKX5shoPPfv6\nVO6eftXTRtVvWmmPVNXzrPnVDAAAAAAAAAAAAADmlI+FAQAAAAAAAAAAAGBO+VgYAAAAAAAAAAAA\nAOaUj4UBAAAAAAAAAAAAYE75WBgAAAAAAAAAAAAA5pSPhQEAAAAAAAAAAABgTvlYGAAAAAAAAAAA\nAADm1NK3ebPpdNpubm6+zVu21lpbWsqPub29PRg7OzuLuaurq4Ox9fX1mLu2tjYYe/XqVcx99OjR\nYOzFixcxd3l5eTC2s7MTc7/66qvB2ObmZsyt2iFZXFwcfe3Ly8uYm9rw4uIi5q6srMR4srGxMRir\nnjeVuarnlFuNzyqenqmSrp36e2utHR0dDcbOz89Hl6mqy9Q/0vjudXh4OBhL81lreW7pGaOttXZ7\nezsYu76+jrmp/au5JY3x6r6pbx0fH8fcVF+pLio//elPY/xf/st/ORhLY6GnTMxe6k89Y7Pax9y7\nd28wVq2Baa3a2tqKuWn9rNbtpKqrNNdUa+/V1dVgrNrzVddO4zPt21pr7cGDB4Ox169fx9z79+8P\nxqr189mzZ4Oxhw8fxty0F3n8+HHMrfr0WKlMrfXtzar1JKn2munaPe9d1fOm9aYa/2ksVd68eTMY\n63lfqOq5Gg/Vvceq+mVS9bvUTj17oEq6797eXsz90z/908FYaiN7oPdrOp3GNkhzVbW+pr5YzTXp\n2tVeJJW5eh9LZa7OgXrmmtQGPWtcVc/VGUHPO2Qqd89aU+01Ut+pnjetN9X+KfW7njW/mtOreHqm\nnjOmnnJV9039rtoTnJycjM5NY6la89PzVv0u7duqeq7eRb6La2w1h6dnquadpOdd9Ec/+lHM/fM/\n//PBWOqT38X24e6q32l65v7U109PT2Nu6utVmdN5+cHBQcx9X1Jd9a6hqa572r86z0vtUOX+23/7\nbwdjT548iblpnazO/1O5ZvmbVSWVu+d3yao+ZvU7e3Xfan5IUr+r9kEvX74cdd3W8hiu9jk9+6Rq\nTkv9o+e39v39/Zibrl21b08bpjL//u//fsz95//8nw/G0m+l9kHv13Q6Hb23rsZ1ilffyKTcnjku\n/UbTWt97xqz2fNUcmO5b5fZ8I9Fz7lyVK6naqNojJal/9Kz5PX2jp/1by+f41VqU6rrqOz3t0NMv\n0/yQfmet9HyL1HMGUeVW/bJnrFW/W49VzeHVM70Pv/M7vxPj/+Jf/IvBWOob7969u9P9/WVhAAAA\nAAAAAAAAAJhTPhYGAAAAAAAAAAAAgDnlY2EAAAAAAAAAAAAAmFM+FgYAAAAAAAAAAACAOeVjYQAA\nAAAAAAAAAACYUz4WBgAAAAAAAAAAAIA5tfRt3mxhYaGtrq4Oxs/OzgZjNzc3o++b7llde2NjI+am\nMm9vb+eCBR9//HGMpzIvLi7G3LW1tdG5p6eng7Glpdydjo6OYrxqpyTVx87OTsy9vb0dfd/U/tXz\nXF9fj75vuvbl5WXMTfGqv/eMpWo8pLpMseq+lZWVlcHY8fFxzE1jqXJxcTE6d3l5eTBWtX/POKuk\nsVS1f88zjb1ua7mNU99orbWrq6tRZWotz5fVXJnq8vnz56PLxGxVe6Dz8/PR1059sZqn0vzZs05V\n0n1nOU+lcV3NF6mNeueLNM9V9fH69evRuW/evBmMzXIOTNLztNbaw4cPB2PV2prm3s3NzZj77t27\nGE99uqrLHj3tkMrVs7eq3gnSvq5a89O7SnXfFK/2fOvr6zHeI8211Tyc9l5VfaSx9uDBg5ib5o6e\nPlm1w9izhGr8MluTyWT0PFjNRam/VWNgVvNcNQZmtQeq5qmTk5PBWE9dVfet3uWTam/Wc7aRcqu+\nkebP6r5JzzpWrZ8/+MEPBmP379+PudWZWerzPWdI1ZrQM4ars8+x963mup7nHbsGttbXd3rqqqqP\n1Leq+6bcqt/1vHul+1b7ttQOs2rD6XQar8vsLSwsxHOZ1H5p7W6tXifH5lbX7TnTTrnV+VUaB+n9\npFLNN6k+qnOE9Ftaa/mZq3Xu8PBwMFb9Htaj2q8k9+7dG527t7c3GEt10Vpup9666vldMo3xqu+k\n+1Z9J9XH27dvY26PnnPmnn6Xcqv5rGeereaHHj1zaVp3qjL39Lvk2bNnMZ7K9a/+1b8afV9ma3Fx\nMZ4vpn58cHAQr536ec+47dEzT/WUeZZza6rnnv1gde+qPnrO0tO1q3ks9cvHjx/H3J7vSdIeqJo/\nnz59Ohib5TrV87xVO/SsN7Pq0z3vIj1ngT1muQeqpHeC3d3dmJv2yFVuOnerzkZTv6vOTVO/e/Hi\nRcxNz/QXf/EXg7G7jhN/WRgAAAAAAAAAAAAA5pSPhQEAAAAAAAAAAABgTvlYGAAAAAAAAAAAAADm\nlI+FAQAAAAAAAAAAAGBO+VgYAAAAAAAAAAAAAOaUj4UBAAAAAAAAAAAAYE75WBgAAAAAAAAAAAAA\n5tTSt3mzyWTSlpbG3XJ1dTXGz87OZpJ7c3MTcy8vLwdjV1dXMXdlZWUwVpU53beq44uLi8HY7e1t\nzE3PVD1vj6pcPa6vrwdjVftvbGyMvu/x8fFg7OnTpzE39dnt7e2Y++jRo8FY1XeqeKqvqi7Hzg1V\nbhpnreXxsLW1FXNTG6Y2aq2v73z11VeDsc3NzdHXXV5ejvHDw8MYX19fH4ylOauKpzHaWmuvXr0a\njFXjYXFxcTBWtWEqc9Xfx16399q8P5PJJM5H5+fng7FqHkvrb8/cW/XFNH56+mk1975+/XpUmSrV\nmE97kWoNq8qV4tUcmFR1mfpd1f6PHz8eVabW+vYLKV6tY6k+qtyqHVIf6BmHaa/RWl57q/uma6fr\ntpbfVar7prqu9kf7+/uDsWocprmyGiuzfM9J9ZHGaGu5DXd3d2NuasM3b97E3KSa79Iz9bzzpedZ\nWPC/kf5NVZ2LpDFQrWM981jPOpbuW82BR0dHg7Fq/Dx8+HAwlvZWlaqe35dqTUjlTvXcWmufffbZ\nmCK11nL/qNohrVVV30lzc7UH6lkDqzVhVudA1ThMa1E176R3s2o/0fOOlHJ7zup62qC1XF8975NV\nvzw4OBiM3b9/f/R9T05OYu6DBw8GY9W+bVZnSKlMae/Mt+P29jb2q9QfU9u2lvtr9f6a3hWqfpPm\njZ71t8o9PT0djN27dy/mvn37dnTu2DK11tra2lqMp98levaylVTuVKbWWtvb2xt939QO1X1T/6jq\n4qOPPhqMVetNpee3xbReVeVKz5zqubU8xnt+W6r6e89a17NfSfetnrca4ym/Z39WjYdUruosKF27\nZ06r2rdn3knzYRrf1e+ZvF9p/qzWmp49burHPb/DVGOg59opt5p7k6rM1dhMqjkwrXM9dVWtn2n/\nXD1v+nanZ85P77at5Weq5vzUP3r2qb161vW09lbr2IsXLwZj1btXUvW7nnPknr1qunb1vFW/rPpe\nkub46r49+7p0Zvfs2bOY2/O8qcxVn02+ifXMr2YAAAAAAAAAAAAAMKd8LAwAAAAAAAAAAAAAc8rH\nwgAAAAAAAAAAAAAwp3wsDAAAAAAAAAAAAABzysfCAAAAAAAAAAAAADCnfCwMAAAAAAAAAAAAAHPK\nx8IAAAAAAAAAAAAAMKeW7vIPTSaTn7XWTltrt621m+l0+tPJZLLbWvuHrbXPWms/a639nel0epiu\n8+7du3Z5eTkYX11dHYzd3Nzcpai/Urpndd8Ua621X/7yl4Ox6+vrmLu+vj4Ye/36dcxNqjKn+tjY\n2Ii5h4fDTbyzsxNzU1211trS0nB3vL29jbkrKyuDsaodzs7OBmMff/xxzE2qukyq/p7KlZ5n1lLf\nq8ZhUtVlzzOvra0NxlJ/by33rYuLi5ibnqnK3d7eHoxV9Zzqqpo70hhtrbXz8/PBWDUOFxcXB2NV\nO3zyySeDsWruSPVRzWmpnZ4/fx5zNzc3B2M9Y+XRo0eDsePj49HX/ZB9U3ug29vb2AYPHjwYXcbU\nF9Mc11peb6q5Nz1PNV+k5z04OIi5ycOHD2M8zVPLy8sx9+rqajCW9nSt1c+Uyn1ychJz0/xajfvU\nTltbWzF3Vqp5O5W5Wj9Tf19YyP8byqp/pHa4f/9+zH3z5s1grGqH1C9Tf28t99t03dZyO1T7sjS3\nVLmpLqu+0/M+2WOW993d3Z3Jdas5PO3bqvfY1O+qPpvaOO1jJ5NJvC6/2jd5DpTaNs1z1b6452yj\n571o7LlWa3n8VHNvUt03qfah6XnT+0drfWvCixcvYm5am6u5N9V1er9sLZ8/pfZtra+d0txcPW/P\n+VR63tb6+m2P9ExVfaR2qM4uki+//DLGe/ZePblpHFZrfhXvOWdIY7ga/9X+eqzqfbLH+zgzreYk\nhn1T+6DW8h42nT1Wc1nqN+ncsbV8RlGN++rdOEnl6nmfn9X6Wl27OjuuzrTT+9z+/v7oclV9Jz3z\nkydPYm46Z6zmnJ65O9XV6elpzE19tlr3q/7esx9N5e55pkr6bamS+k5V5p69bM/zpmtX963O1asx\nnvSUK42Hqk+na1fPm1T37dl/V+sD36xv8vew9LtGz+9hqc9U10251VzT0xdn9XtYz+9/1fOmebuq\n52ofU+2/xup5n3/8+HGMp2eu6jLtgXtyqz6Zcqt1u+pbqa6r3J6x1PNbS89etGcd69kD9eQmVZmr\n352q362Tnr1IzzOnfre3tzf6uj3zTiX9dpzqYjqd3un6v85fFv4vptPp70+n059+/f//vdbaH0+n\n099qrf3x1/8/AMC8sQcCAD5E9kAAwIfKPggA+BDZAwHAnPt1Phb+D/3N1tofff1//1Fr7W/1FwcA\n4DvPHggA+BDZAwEAHyr7IADgQ2QPBABz5q4fC09ba//LZDL5PyaTyR9+/d89nk6nX7XW2tf/+Sv/\nNvNkMvnDyWTyJ5PJ5E96/pUcAADvwTeyBzo6OvqWigsA8I1wDgQAfKjsgwCAD5E9EAB8AJbu+M/9\nZ9Pp9MvJZLLXWvtfJ5PJn931BtPp9O+31v5+a6393u/93nREGQEA3pdvZA/0u7/7u/ZAAMBvEnsg\nAOBD5fcwAOBD9I3sgX7yk5/YAwHAd9id/rLwdDr98uv/3G+t/ePW2n/aWnsxmUy+11prX//n/qwK\nCQDwPtgDAQAfInsgAOBDZR8EAHyI7IEA4MNQfiw8mUzWJ5PJ5l/93621/7K19n+31v5pa+0Pvv7H\n/qC19k9mVUgAgG+bPRAA8CGyBwIAPlT2QQDAh8geCAA+HEt3+Gcet9b+8WQy+at//n+aTqf/bDKZ\n/O+ttX80mUz+bmvt5621v11daDqdtsvLy+HCLA0XJ8V6pWun8rbW2urq6uj73tzcDMa2trZibirX\n9fV1zF1eXh6MvXr1KuY+evRoMHZ2dhZzLy4uYjzV5eLiYsy9uroajKV6bq21jz/+eDDWUx9V7vb2\n9qhYa3W/TI6OjgZjqS5aa21tbS3GU9+rxnAqV9W30rVT32it7ltJGktVG758+XIwtrGxMbpM1ZyU\n6rJqo554at9KGme9VlZWBmOHh4cxN7VTz9pQ5aZx+Pz588HYdOrf+jPCN7YHaq2129vbUbFqzn/7\n9u2o67bW2uPHjwdj5+fnMTfNnz3r9vr6esw9Pj4ejFXPm8bX69evY26a46q9RnXtVF9VO9y7d28w\nVvWdnvZPe9WqHZK9vb0YT+3fk/vpp5/G3KpPp/5R7YFTXVdrb88euGf/lOJpf9RabodqDazec5Ke\n98nq3Sw9U49qPkz7mKpMqU9X7Z/mnZ79YpWb6iPFFhbu9C9U4v/vG90DJWmumuUcmOabKjep1ou0\nRvbM25VUVz37p2qv0fMeOEtpXu9p//39/AeW0l7l6dOnMTfV9cOHD2NuWhOqNa5a13ukfln19552\nSnr2sZWeMveUq2ftreKp71XvQMnu7m6Mp35ZzUtpP9nzO0DaH7WW39sr6fypOjNllG9tH5TeFXve\nfdK5c2utbW5ujr726enpYKxaM9LzVvNNOg+t3rnTtd+8eTP6vtXZcc/vYT3rYLXefPTRR4Ox1L6t\n5d88qrUqlevJkycx98svvxyMpedpLbfDzs5OzO3Zn/e0YTVGUzv1rEc9a8osf8NPqrmy532zktqp\nGkvp3lW/7Pl9eFbvwdX837OmpXk43ffrdZxfzzf6TVDqU9UaOdaLFy9ifGx/6pXGQPWbxrNnzwZj\ns3ovbi2/61X13PueOVZ13Z55e1bz2GeffRZzU/tX50jpjKrqdz1jtGdfX+2Ber5FSvc9OTmJubMy\ny73mLPdmDx48GIwdHBzM7L6z2nv1tEPPt1nVt3fpXK3n/fivlD1kOp3+ZWvtr/+K//51a+1v3Oku\nAAC/YeyBAIAPkT0QAPChsg8CAD5E9kAA8OHwJ3YAAAAAAAAAAAAAYE75WBgAAAAAAAAAAAAA5pSP\nhQEAAAAAAAAAAABgTvlYGAAAAAAAAAAAAADmlI+FAQAAAAAAAAAAAGBO+VgYAAAAAAAAAAAAAObU\n0rd+w6XhW66urg7GXr16Fa+bciuXl5ejc29ubkbnpjIfHx+Pzq3KlOqyqsdUV2trazH3k08+ifHX\nr18PxtbX12PuysrKYKynPqrczc3Nwdji4mLMvb29HYxVfXJ7e3swVrVhyu21vLw8GKvqMsWrMqc2\nTH2jtdaurq4GY1Vdpja8uLiIuT3tkMpc2djYGIxV/a5nfuipy0q6bzUOk1RXreX+nmKt5f5RjZXT\n09MY57tpMpmMXquq8bG1tTUYq/YTVTxJ5ZrlviztJStp/qzmi/v374+6bmutPXjwIMbTM3/ve9+L\nuakd0t6qtdYODg5iPEn9uWqj1N+vr69j7u7ubi5Y8PDhw1Fluks8PfO7d+9ywTqk+1Z7oPPz89G5\nY6/bWt/anFTjsOd9oed9cVZzVmutnZ2dDcZ63kWq9k+51XtbeqYqt2fN4v1ZWFiIfSr1iWrsjb1u\npZoTeq6d9Iyfqsxpvuip50r1TpXKXdVH2m8+f/589H2rvfeTJ08GY9V7flojqzZMdVnVc1oTes8B\n0rtv6net5Weu1s/UTtUYTfVRPW/KrfpOKlfP/rnqO48fPx6MvXjxIub2nAP1qM5Ues6fklk+b2qn\n6r5VO/HdtbCwEH8/6JHm2Oq9OsWrOSWdaVa/D/X8lpb0zKHVmtHz+0/1m+bh4eFgLO03qtzq7Djl\nVnWZzsZ6VL+lfP/73x997dSGVV1VfTqtC9U43N/fH4xV60Iqd1WXqd9W62+PnvHfM2f1tH/VDj17\n2aQqV2qnqs/O6relnnWn6ncpN8Wm02m8LrO1tLQ0+hy/Z+9bzQmpv1X9OM2vVW6aE6pxmX5bqub8\nnvkzlTm9Y7ZWt2FaP58+fTr62lVdpmeq5qL0zD170ZOTk5hb1UeSxmDPWGltdu8T1W+p6TfNnr1G\nz/NWevYLqa6qMqd4NXf0fOfSsweq6vl9fSPTc478Purqrnsgf1kYAAAAAAAAAAAAAOaUj4UBAAAA\nAAAAAAAAYE75WBgAAAAAAAAAAAAA5pSPhQEAAAAAAAAAAABgTvlYGAAAAAAAAAAAAADmlI+FAQAA\nAAAAAAAAAGBOLb3vAvz7bm5uBmNLS7moq6urg7HLy8vRZaru+8knnwzGnj9/Pvq+KysrMd5TVxsb\nG4Ox29vbXLCR122ttVevXsX44uLi6HunZ0511VruO5XDw8PBWFUfy8vLg7Gqz56dnQ3GHj16FHPT\n81Z95/r6OsZ7pHJXfSeNteqZrq6uRuem+M7OTsxNbXxxcRFze8ZKUo2Vql+en58Pxqpxlq7d87xp\nrLSW26nq7z3j8Be/+MVgrGe947trOp3GMZbatRqbae2+d+9ezE1jr+pr6do9+4n19fUYT9eu5qkU\nT+tya3lO2NrairknJyejy/X69euYm9qpmj+ruk729/cHY1V9pD795s2bmPv27dvB2A9/+MPR961U\ndZWuXd03XbvK/fnPfz4Ye/DgQcxNe6DqvqlvVe8xPfvJ1N+rOev4+Hj0fdMep7U8HnZ3d2Nuz3zZ\nI9VX1UZpHD58+DDmpj5bSXukNI9Op9PR96TfZDKJ/a3nHaJHzx4ozZHVmJ7Vu1z13pNUz9sz5x8d\nHY0qU2v13JtU+7qevnVwcDD6vqnfVX0nrWOPHz+Ouamd0n6gV8+7bc+5aJWbVGVOqjUwjdOqHdJ+\nsRorszqraS2P8d5rJ2leqvYxPXugsddtLb/nVHvv1Ld65n9mbzKZxHmlOitI1tbWRuf2/La0ubk5\nGDs9PY256Ry2GgdpfFb3Tc+UnqdSnbn17DfSO2Zrec/R04ZVO6S5rKqP9ExVO3z11VeDsd/+7d+O\nuemZqnHUsy+o1pS9vb3BWNpvtpZ/P6p+l0rzTrWHSvVR/abVM+8kPee5lZ69Ss+1q36Z6rpqh561\nI8211dnXl19+Ofq+SSrT+zpv49+5vb2N803P3NyTm8ZINRela1dzTZqrqn1Mum+1l0y5PfNFtV/o\nmdefPXsW42l/1bOuV2VOa3O1j+nJTXX94x//OOYmPW1UqdbmWe0nesZ/T9+pxsOs3vl63p8qVTvM\nSk8bVvWR+mV13/Q7fc9Zf1XmFE918e7duzvd318WBgAAAAAAAAAAAIA55WNhAAAAAAAAAAAAAJhT\nPhYGAAAAAAAAAAAAgDnlY2EAAAAAAAAAAAAAmFM+FgYAAAAAAAAAAACAOeVjYQAAAAAAAAAAAACY\nUz4WBgAAAAAAAAAAAIA5tfS+C/Dv297eHow9f/485m5sbAzGbm5uRpdpaSlX0dnZ2ejcy8vLwdjq\n6mrMTc+Urttaa8vLyzGepGc6Pj6OuY8ePYrxi4uLwdj5+XnMTfWxs7MTc09PTwdjm5ubMTfVx+3t\nbcxN9VH1nTRWqvZPdVXdd21tLcZTG1Z6xmkaL2mMttba4uLiYOz6+jrmpjKnflWp2iG5urqK8fS8\n1ThbX1+P8dQOVb9MfSfN763ldurJrebK1LeqvpPauOqzqd+l3GpOYrYmk8nosV2N662trcFYLcsX\nigAAIABJREFUNS9X+40k9amqv6W5qHrelZWVwVi1lqQ2qO6b5rHe+TM9U9Vv0jNX7Zueqbrv/fv3\nR+c+efJkMJbqorXc36v2v3fv3mCsZ39cSfdtLfe9qi5T/M2bN7lgI69bOTk5ifG0NlfjMM0t1fqZ\n+lZVV9UYTn26mh/SnqEaw+naVZmrvUrS8y6aVLlVG/PdNJ1OY59J83p1xpD0nKn0SHucSrV/SnNz\nNW+nubeq51Suatw+fvw4xtO9e/Z11X6i51wkqebW1N8raV6v6iqtc3t7ezG3Wpt79MzrPfvYpGfd\nrvZAVb9MUjtU7Z/6zuvXr0fntja7fV2lZ67t0XOWn96f3r59G3NTOzkH+m579+5dPJd58ODBYKw6\nz0l9qtJzhp9Uv6Wk+aoqU88+aFaq8//d3d0YPzg4GIz17GWr33B66ivNK9Vc9nu/93uDsepMJtVH\n9Txp7a76bNUve94pet7Jk2p/ns6oqj6d6rq6b89vR6muqjZI46H3nTD1j55n6hmjPfNw1SfTnFXd\nN+n5HTbFptPp6DLRb2FhIY6/NO+/ePEiXjutrz37iUrqq2lPV+X2jJ/qvkm19qa6rN59Hz58GOOp\njav9U9JzjlRJudV9f/zjHw/Gqn1baqeqv6e+VbV/z3cOs3rXqPSUuZp30lirxkNq42rvlfZPPe1/\neHgYcyvVMyc9Z7I970CpvqoxnN6Bes6nqjac9Vjyl4UBAAAAAAAAAAAAYE75WBgAAAAAAAAAAAAA\n5pSPhQEAAAAAAAAAAABgTvlYGAAAAAAAAAAAAADmlI+FAQAAAAAAAAAAAGBO+VgYAAAAAAAAAAAA\nAObU0rd6s6Wl9ujRo8H40dHRYCzltdba5eXlYOzm5ibmrq6ujs4dW6bqvlVuiqfrVhYXF2N8eXl5\nMNZTV1X+yspKzN3c3ByMXV9fx9ye+kr1cXFxEXNvb29H3zeNlY8//nj0das2rPpHj6Wl4ekoxVrr\na8Nnz54NxqpxmPpWVabU/lXf6ZHuu7W1FXPPzs5iPPWPx48fx9wXL14Mxqqx0jP+0xiu7pviV1dX\no+9b9fck9buFBf/7oPdpYWGhra+vD8bT/FuNzfPz88FY1Y/X1tYGY9VclMZ8dd+NjY0YT9Lz9oyf\nag1M7dd77fRM1R4otUOVe3JyMhjrqcvXr1/HeNWnkzS/VmV++/bt6Pv21Me7d+9G51ZSuap+V+1z\nkjTGq7GSylXtNdNY6XH//v0Y79m3P3jwIMb39/dH33d3d3cwVrV/mh+qfUzqO9VYSf3j+Pg45m5v\nbw/Gvvrqq8HYdDqN12W2FhYWRr+vVetYmhN6zyeS9DzVPJWeqRo/aX2t9lZp3FbrcpoTqnqu9gQ9\n5wA9a2BS9dd036r9e/bA6ZmqNT29j1dzb0//qMzq3bdS9cuxqjU/1VXPfFfVY9pPPHz4MOb2vMdU\ne8K076v6dBpLPWdXlZ4zpPROUJU5tVPKneU5LneztLQU9+zp3CX1t9bymefp6WnM7ZlDk2rOSGe4\nlVRXOzs7Mffw8HAwVtVFz29WX3zxRYynM7lqbk91mZ63tdy3UpmqeHXmkub2qr+ndjg4OIi5P/jB\nDwZj1Vip+mzP7zhpDFf1kVR9uuofSc84TLlVf0911dNG1X2rOS3Fq/qo+l7SM/5Tuarf0pKe96+q\nzKmdUl2kc29mb3FxMb4fpf5W/Z7c85tGz3cdPb+lpdxq/PTMgWmNrNaaVJdVbjX+Uhv3rK09c1HV\nd1J/rsrc80yprnvm7Sq36ltpXzfL+ug5R0zjofp9KKnOgt68eTMYq/ZtqZ6rfXtPbtU/Un5VH6n9\nq319um9V5p5z5FmdrVRjYex7213nBl8OAQAAAAAAAAAAAMCc8rEwAAAAAAAAAAAAAMwpHwsDAAAA\nAAAAAAAAwJzysTAAAAAAAAAAAAAAzCkfCwMAAAAAAAAAAADAnPKxMAAAAAAAAAAAAADMKR8LAwAA\nAAAAAAAAAMCcWvo2b/bu3bt2dnY2GE+xpaXxRd3e3o7xnvseHR2Nvm9yc3MT4+nar1+/jrnLy8uj\n73t5eTkYu729jbmHh4cx/v3vf38wdnFxEXPX1tZG56ZnWllZibmp7zx9+jTmpmtvbGzE3NROVRum\n+M7OTsxNz9tbrrHXreLHx8cxd2trazB2fX0dcxcXFwdjq6urMTfNHZXUZ6v7JqenpzHeMw9XbZj6\nVjUeUn1U4z89UzWHV9dO0nzYM1bS80wmk9HX5ZuR2vbk5GQwluaa6rrV+EnSfqG1PEf2lLnKTfNc\nzz6mkvY51R6o2k989NFHg7Grq6uY+/Dhw8FYtSd88ODBYKwqc+qzP/zhD2NuaodUptbqtTlJdZn2\nA63VfaunXD39Mo2Xqsxpfqj2Tz1SuaqxlFR9NqnG2XfVwcHBYKwaS+mZ19fXY241XpKefU7aP6f3\nmJ49LLM3q/mmei9K7x9Vbs8ZQrp2tR6kNb+ax9J9q3fTtNb07DVba+38/Hz0tau5aqyqDV+8eDEY\n+/zzz0fft9oDp/qo2j/19+p5q2unfpvat1LtCdLc3tOnq3GY6qPqsz3vfEm1zvU8b7Vup3FYlSv1\nj6o+qvPJJPWtajyk5636bOo7PetOynUO9P69e/cunh/2nC2mfpHOGKrc6t02zQvVuE/XrsZBunZV\nj+ldoToP39zcHFWm1lq7d+9ejFf3Hqv6jafH/v7+YOzTTz+NuamNq/ZPZ5T379+PubPqs5We8d2T\nW63dqU+/efNmZvftOfvquW/qO1U9V9dOv0tXfv7znw/Gnjx5EnPT3FGVKbVDldvzvGksVddNz+v3\nsO+u29vb+P6TzjSrPpHGbpWb5oSeOb9nfMxyLUrnw6l9Wst11TtfpPbvOcfd29uL8eo3zySdBT1+\n/Djmpnao6irVR3X+n1R1Ua3bqV9W6+fu7u5gLO01W+tbA3u+J+uZd3q+60n1XI2VdO3qu72ecVjV\nZRpLqW+0Nn5PUOlZd96+fTs6t7pvasM0hu+6B/KXhQEAAAAAAAAAAABgTvlYGAAAAAAAAAAAAADm\nlI+FAQAAAAAAAAAAAGBO+VgYAAAAAAAAAAAAAOaUj4UBAAAAAAAAAAAAYE75WBgAAAAAAAAAAAAA\n5tTSt3mzhYWFtrq6OhhPscrNzc1g7OjoKOam+15eXo7OrZ4nlSs9T2V9fX107uHhYYw/ffp0MHZ2\ndhZzV1ZWRpXpLi4uLgZja2trMffVq1eDsY2NjZi7tbU1GKvacGlpePhVualvffzxxzE3tVPVZ6u6\nTOMlPW9VrkrKXVxcHH3d5eXlGO953uT29jbGU7mq+6a+VeVW82GPNJaqdkhtXPWrnZ2dwVj1vGk8\npDmptdYePXo0GEtzUmu5nba3twdjX331Vbwus5faLq2RPevn8fFxjKexV/XjsddtrbWrq6vBWDUH\npjFfjdtUri+//HJ0brXWnJ+fx3hqp2puTrlV33n58uVg7KOPPoq5n3766WAstW9rfXv+tCbs7e3F\n3NQOVRtVdfn27dvBWM++/uDgYHRu9UypXFWfTuO0yu3ZI6W+c319Pfq6lWpeSv2jav+ed7fU5/f3\n92Pu7u7uYKznXaSah9P7VVXPaZ/z+vXrwdhkMonXZbYmk8nocV/NvamvzrIf98xjPblJz/lDJeX2\nzI+t5bWqyq32G8nz588HY5988knM/fzzz0fft6cuU798/PhxzE313NsnUzv07IGqd/lZnk8kqV9W\ne5G05s9y3kl9q6eNWpvd2WZVl+lM5Ze//GXMTe8TVZnTPqZ69059Z2Eh/z2XNIZ725DZWlhYGH1+\nWJ3Dp3FS/caT+nI1/tK47ylzz+9wlTTuq/umeFWmau7umUN7pPOv73//+zE3nQVVZU51ubm5GXPT\nWKnOr9K7cXovbq3u09VYSx48eDAYq/ZBPb9LpDL37Dd65o7fVD1juOq3yZMnTwZj1fl2KnPVRum3\ntHTG3FrfHJ76e4pVezNma3FxMc5zPdL4Oj09jbnV7wdJ9Vv1WNW8ne5bzb1pjFS/O/R8i1A9U1r3\ne/ZAVX08e/ZsMJa+gWotn7t8F88JW8ttXO2BqnZIdd3zO02Vm8pd5b548WIwVu35qnjS08Y9903z\nYc97W5XfM6dVc3hq/9S+reUyV8+b5qzqd7ie+6ZxmK5bnTH9f//cnf4pAAAAAAAAAAAAAOA3jo+F\nAQAAAAAAAAAAAGBO+VgYAAAAAAAAAAAAAOaUj4UBAAAAAAAAAAAAYE75WBgAAAAAAAAAAAAA5pSP\nhQEAAAAAAAAAAABgTvlYGAAAAAAAAAAAAADm1NK3ebPpdNpubm4G40tLw8XZ2NiI1z47Oxt13dba\n6DJVLi8vR9+3et6jo6PBWFXmVK5Hjx7F3HTt1dXVmFu5vr4ejK2trcXci4uLwdji4mLM/fjjjwdj\nqY0qVRu+evVqMFY97/b29mCsKnN63lSm1uq+tbm5ORh7+fJlzE39Mo3v1lr75JNPBmM/+9nPRt+3\nsrW1NRir2iGNl55+d3V1FeOnp6eDsdR+rb2/een29jbmpjH+9OnTmJv6fFUfqVxVmZeXlwdjVV2l\n3NQGVZl4v9K4X1lZGX3dTz/9NMb39/cHY6mvtZb7ajUXpTFf7SfOz89jPEn1/PDhw9HXrcp8cnIS\n49X8mqS6rNrwyZMng7G0L6vuW9Vlaodf/OIXMfcnP/nJYKzqdx999NFg7PDwMObu7OzEeBoP1bqe\n4j37iR7VmpHmparMPXNaz74tPVO19lZl7rl2eqa3b9+Ovu/3vve9mJvm//X19Zib9l5V37l3795g\nrGespDlrOp3G6zJb1TlQGgNVX0zjqxq3aT9Rjds096b9eGv5eWd5dpVyq7UkvfceHx/H3BcvXsR4\nauOe9+Kq76QzhB5VXab2r9a41A7VfdO8Xe2tq7FUnbklVf9JUn1UZ0hprar2k2nfXtVFzz6mR899\nq741dn6/SzxJfac620651XtMKnP1/tQzDpOe3y6Yvel0GvtG6lPV7wNJdcaQ5sHqHCHF0280rfXt\nZdKZdlXm1AbVPLe3tzcYq5732bNnMZ7mhWpNSW1Y5X7/+98fjKX3tdZyG1bnJqkNv/zyy5j7+eef\nD8aqM/wUT2VqrW7jNE6rfXBqw6pcPWdQKbe6b/pdshrDqV9W+6+Dg4PBWDX+Uxv2rqGp/at9QSp3\net5KNQ7T77TVutMzh4/9Tau1ui75zZTG5uPHj2Nu6ovVmpDu2zMGKtV6kqTflqoy9+Smuuw5w20t\nj+tqPulpw93d3RhPUrl62qHqG6nMPW1Y3bfnnSD97lCp9iLpmavfYXvWk3Tfqr+nNqzK3DPv9Oh5\nv6rKnOqy2gOleM/3hFVueq+r6qqnzybfxO9h/rIwAAAAAAAAAAAAAMwpHwsDAAAAAAAAAAAAwJzy\nsTAAAAAAAAAAAAAAzCkfCwMAAAAAAAAAAADAnPKxMAAAAAAAAAAAAADMKR8LAwAAAAAAAAAAAMCc\nWvo2bzadTtvNzc1gfGlpuDhHR0fx2tvb24Oxn/3sZzE33TddtyrXo0ePRt+353nPzs5i7urq6mDs\n/Pw85ibVfZeXl2N8cXFxMHZ5eRlzU13e3t7G3KR6pp2dncFYVeZUrocPH8bc1D82NjZibmr/VI+t\n9fWtHmneaK0eL0nql9XzJtfX1zGe+kfVDldXV6PK1FpuozQGW6v7dFK14V/7a39tMPZnf/Zno+9b\n1VWKV3NWT12+ePFiMFa1f9KTy2xNp9PY31J/Ojk5iddeX18fjD179qwu3IBqPUlz5OPHj2Nu2m8c\nHx/H3Gp8JQ8ePBiMvXz5Mubu7u4Oxg4ODmJuNX/2zCcpN/WNqlw9e68qN7X/b/3Wb8Xc1D+q/dPb\nt28HY69fv4651fy6ubk5GOtZt6v9RNpPrqysxNz0TD1lrt4nUv+o1t70vD1zQ7WH7dkTVu8in3/+\n+WDs3/ybfxNz0zNXbZjKVY3htD5U804aw1tbWzG3512V96c6B0rtXr1DpP5W7SfSHFjN+en9szpD\nmuU7VVKNzSTNF1VdVWtRmqt6zuOqfWwqV7X37jlTSc/76aefxty0FvXs29K7aWutPX36NMZTv+zp\ns1VuT79MenJ79sA9Z5eV9EzVHqg6b+u5djo3/+KLL2Ju2gNVbZjaqWev8fHHH8f48+fPB2NVmXv2\nuXy3pffIw8PDmJt+l6jWsqTnfHhvby/mvnnzZjDWMw/u7+/H+JMnTwZj1Ttoz1rW4969ezGezjfS\n2VdruQ3TdVvL81XVZ1Ndpvfi1vJ6VJ1BJj17817pvGeWZ/xra2szuW41Vk5PT0fnpnFanZuluqzG\nfyU9U9WGaQ6/uLiIuT3PlK5d7d3SfdN61lpel3r6e2qDWe6vqb179y72tzQXVfuYntwe6do//vGP\nY27aq1TzWHreav5Mv2ml8dNank+qtaQ6Z0jlrq7dk5tUc28VH5vbc+ZS9Z0Ur37TrMqV+nQ1r6e+\nV7VhqsuqjdKesWc89Mw71RhOddmzf6pUfSvdO807laoue/ZAqY2r9k/vV1V/T89UvbelcZr6+7t3\n7+J1/4q/LAwAAAAAAAAAAAAAc8rHwgAAAAAAAAAAAAAwp3wsDAAAAAAAAAAAAABzysfCAAAAAAAA\nAAAAADCnfCwMAAAAAAAAAAAAAHPKx8IAAAAAAAAAAAAAMKd8LAwAAAAAAAAAAAAAc2rpLv/QZDLZ\nbq39g9baT1pr09baf91a+/PW2j9srX3WWvtZa+3vTKfTw3SdhYWFtrq6Ohg/OzsbjKW81lq7ubkZ\njD169Gh07uXlZcxN1z46Ohp9342NjdG5lVTm6nnPz88HY1WZK7e3t6NirbW2uLg4GNvZ2Ym5Pf1u\nZWVlMLa8vBxzU31V963iSeqXP/zhD2Pu4WEc4rH/LC3l6aanT6f7VnWVcqs2THpyU59sLfedKjfV\nRzX+q2c6Pj6O8eTP/uzPBmNpfFf33drairk94/D6+jrGx1pbW4vx1MY944hf7ZvaA1XSelKNgdSP\nqz5xdXWVCxY8fvx4MHZwcDD6utXzJuvr6zGexlc1X6S6qur5yZMno6+d+kZreW9W5fasn6muqzW/\nZ+6tnilJ9fw7v/M7MffLL7+M8WoNTVIbVmvvmzdvBmPV/jm1YbWfSHr6TqqL1vI47dmHVGt61e9e\nvnw5GKvmpb/8y78cndvT79K1q3k49Y/t7e2Ym9qpmjtSmV+9ehVz+fV9W3ugnveTlNuzB6ru+/Dh\nw8FYNY+lfl7dt5rXx963GntpTqjK/PHHH8f469evB2PV3FvNN2P1vFNVuT37pxSv7ptyf/d3fzfm\nPnv2LMbTOKyeqWcv+uLFixhP0l606tNpPFT7p9T+1XtZqsuePVClOutNZ4xVG/7yl78cjFV7kZ77\npnao9oSzmnfu378f42mM94wFhn1b+6A0tqvfNKrzw+Tk5GQwVr2Dpnh1FpRyLy4uYm6yt7c3+r6n\np6cxN9Vz9bxPnz6N8fRbS9W+qb6qtezt27eDseo3zbQPrtb9njZO82/VhmksVed1X3zxRYyndqrG\nUqrrKnd/fz/Gk7ROVnWZ1uee9p2lnnmnGoc9YzidM1bzf2r/6r6bm5uDsVm+B6V32arMaY5P9TiZ\nTOJ1+dW+qT3QZDIZfQ5R9afUZ6r9fM9c9dlnnw3GetaLHtXaOyt/8Rd/EeO7u7sxntphVnVVSfvj\n1vIz9ZS5Z+9d9ef0G261f65+D0t61pOeMVy1w4MHD0ZdtypX9bzfxW83eueO9EzVWEr7zZ53oGof\nm/Tcd5ZSn03rznQ6vdP17/qXhf/b1to/m06n/1Fr7a+31v6f1trfa6398XQ6/a3W2h9//f8DAMwT\neyAA4ENkDwQAfKjsgwCAD5E9EAB8AMqPhSeTyYPW2n/eWvvvW2ttOp2+nU6nR621v9la+6Ov/7E/\naq39rVkVEgDg22YPBAB8iOyBAIAPlX0QAPAhsgcCgA/HXf6y8OettZettf9hMpn8n5PJ5B9MJpP1\n1trj6XT6VWutff2fv/JvlU8mkz+cTCZ/MplM/iT96wUBAL5jvrE9UPrXkgEAfMd8Y3ug6l/PDADw\nHeP3MADgQ2QPBAAfiLt8LLzUWvtPWmv/3XQ6/Y9ba+ft1/jXC0yn078/nU5/Op1Of/rw4cORxQQA\n+NZ9Y3ugnZ2dWZURAOCb9o3tgXZ3d2dVRgCAWfB7GADwIbIHAoAPxF0+Fv5la+2X0+n0f/v6//+f\n27/bKLyYTCbfa621r/9zfzZFBAB4L+yBAIAPkT0QAPChsg8CAD5E9kAA8IEoPxaeTqfPW2u/mEwm\nv/31f/U3Wmt/2lr7p621P/j6v/uD1to/mUkJAQDeA3sgAOBDZA8EAHyo7IMAgA+RPRAAfDiW7vjP\n/Tettf9xMpnca639ZWvtv2r/7kPjfzSZTP5ua+3nrbW/3VuYm5ubwdjGxkbMvby8HHXdSrpua61t\nb28Pxqoyn52dDcZevXoVc6trj73v7e1tzF1ZWRmMLS3l7lTFnz17Nhh79OhRzE2qukxWV1djfHl5\neTB2fX0dc1NdpjZqLfe7nv7+8uXL0bnVvatypXjVd1I7VWM4XbvK3draGoxVz3t6ejoYq8Zh1beS\n1LfW1tZi7uHh4ej7VtbX1wdjVTuksVTNHWl+WFxcjLk97ZBU9019No2FyWQyukwfuG9kDzSZTGJf\nPT4+HoyluabKreaiVKbz8/OYm/pbum5rfXuRNEaq3IuLi8FYqsfW8jNV+4VKzxo4qzZM83Klp99V\nc2sqV1VXqVxXV1cxt9LThilelasaa0nVP8aqypTuW6356R2oWj+r+SHpaYfqmdI4rOaWdO3qX/d3\ncnIy+r49++eUW7VhYg80E9/YHii1e887ZNpP9Mzr1bydVOtntd9I0lzTcw5QnT+kvWh1NnV0dBTj\nPeM+5b5+/TrmpjOVTz/9NOb29I9Ulz1rYCX1j54+2Vpu4571pOqXPe2Q6iOd81VmeXbdM8aTqt/1\nqPaaqb565vA0vlvLfbZq/573iZ6+le6bxtnCwl3+pZIMeO+/h1XvxukcN+31W8vvRtV706zeX3rm\nuXTW01peM9IZfWt9Y7e6dnUWn4zdX7fW2r179wZjP/rRj0bft/LkyZPBWM++/8GDBzE3/aZR3bda\nJ9O1q/bt6Zcpt+dsrMpN88Pe3l7M3d8f/4dI0xivypzm0qqNqnZIdVnNSx999NFgrOp3qdw7Ozsx\nt2c8JNWaNavz/N3d3cFYtRYy6FvZA6V2rcZmyu1Zt6t5Kq03m5ubMTfNJ9U81vMtStIzRqo2qtoh\n1Uc1F6X6qubeVK5PPvkk5qb2r9rhs88+G52b6iPNga3lb6+q3IODgxjvGcNJzxiupD5f3Te109On\nT2PuF198MRib1dlWpbpvtQfqqctZrd09e9Ged+9q3kl6fjtO9XjXM7U79b7pdPp/tdZ++itCf+NO\ndwEA+A1kDwQAfIjsgQCAD5V9EADwIbIHAoAPg/95OQAAAAAAAAAAAADMKR8LAwAAAAAAAAAAAMCc\n8rEwAAAAAAAAAAAAAMwpHwsDAAAAAAAAAAAAwJzysTAAAAAAAAAAAAAAzCkfCwMAAAAAAAAAAADA\nnFr6Nm+2sLDQNjY2BuNLS8PFOTs7i9e+ubkZjKV79t43xS8vL0ff95NPPhl93+p5j46OBmMrKysx\nN5W5UuU+evRoMFa1Q3rmra2tXLBge3s7xlO/u76+jrmrq6uDsaoNX716NRjrKXPVRs+fP4/xdO+q\nDZNU5ura1TOlaz9+/Djm3t7eDsaePXsWc1MbV8+7ubk5GKvqOc1LVV1V/TLd+/z8POamcZrGSmut\nLS8vD8aOj49jbqqPaiyl9j88PIy51TMlqX+kMi8uLo6+J/0mk0kcY2kMVHNCj7TuV+M2jZ+rq6uY\nm8ZtzzpWzYFpbFb7ttRG1fiq4umZqvZPa9WLFy9ibprHqj1hyq36zqeffhrjSepbVV2lNaHKreJp\nfPdeO6nGWpLauGr/VJdffvllzE1jqdprpHJVdfHmzZvB2P3792NuzztQNS+lMVw9U6qvnr7RM6dV\n9025aV5pbfw7vz3Qd1saXz3vH9U8llTzcuqrPetFz1xT3TftCXrWoaqef/CDH8R4Klf1zpTqq8pN\n52LVOUDaM1bvnw8fPhyMVe2f7lude71+/Xowtr6+HnN7zgmq+ui5b8/eq5pbktQO1d57d3d3MFY9\nbxpr1fpZveckPe3Q837VMx6q9k/voj3joWfvVZ0hJz3vqbx/ab2q1rLUbx48eDC6TNWZZrpvNe7T\nM6U5srXWTk9PB2PprLy11r744ovB2MnJScxN197b24u5lfRMlZ6z5fTMVV32lDnNz2trazG3Zz5L\nZU7nBHeR9hQ9vw9X0lpWtVHVxkm69p//+Z/H3J2dndH3ffLkyWBsf38/5vbsg3rGWU879Pw+WN03\nXbt63pR7cXERc1OfTbHWchumMi8s+Ht579NkMhnd7tXeuOddMKnWotTPq7GXrj3Ltffg4GAwVo3b\ntDer9m3VuE5tWLVDFU9S/6j2dT2/4aW6rvYDPc+bxlLqG3eRytUzhqt+Oatv1Xra8F//638dc58+\nfTrquq3lM8rqW6TUDj1nPa3lOaDam6W+U7VDyq3qcux1q2tX75NJNVemd5W0dlTnk3/FTgkAAAAA\nAAAAAAAA5pSPhQEAAAAAAAAAAABgTvlYGAAAAAAAAAAAAADmlI+FAQAAAAAAAAAAAGBO+VgYAAAA\nAAAAAAAAAOaUj4UBAAAAAAAAAAAAYE4tfZs3u76+bs+fPx+Vu7q6GuNLS8OPcnNzE3M3NjYGY2dn\nZ6PLla5bXTs9TxW/vLwcnVvVVbp2T5lba21lZWVUrLVc7p7c8/PzmHtxcTEYW1tbi7nb29ujytTa\n7PpdT39vLZe7av+ecqW6rHJvb28HY9fX1zE3qdphfX199LUPDw8HYz1tVM0dlereydUqEBgaAAAg\nAElEQVTV1WCsaodU18fHx6PL1FMf1fiv+uVY1dzB+/Pu3btyTRlSzRfpulVuGrc9+4lqzk/SfFA5\nOTmJ8b29vcHY4uLi6PtWZd7a2orxnr1ZUq1FSTWnv379ejBW1WWa19OaXhk7xu4i7Rd6c1P7Hxwc\nxNz79+8Pxt6+fRtzU31V61Qqc9V30rxUjaWe/UJ6J6jaqGccVn06Xbu6b3qmajz07HN7pLqu5o7U\n79J1J5NJXTBm5t27d3FPkfp5NX7SfFPtRXrei9J9q71Xej+pxny6dvXe8/jx49G5s5Ta6ejoKOb2\n7Bl6zmNS/+jZT/acqc2yDXvPCZKe87i0BlZlTvucnv1k1YZpb17tRdJ7Tk+/q/p7T7+sxmhP35rV\nOVBPfVTnz+l5l5eXc8GCnvcUZu/m5qa9fPlyML6zszMYq84W0+8SPWcy6brVtau+nMZBz/4r1XFr\nrX3++eejc9O8kM7oW2vto48+ivGkunZP3+nR0+9SvHo/TWtdVVfp2pubmzG3cnp6OhirxkOqy3Td\n1lp78uTJYKyqyxSv6rLnLDidye7v78fcVK6qnlO8mu+qsdTze3lq457fh6u5NLV/zxiu6qrnN80k\nta+zoPfr9vY2vsOktqv6Ys+3GT3n8Em1nqRxXc35PT777LPBWDUHJlXugwcPYjy1U9UOu7u7MT72\nvpX0zLM8Z0/Xrtqhp42rPt0zXlK5qjL37CeSFy9exHjq0z39vZJ+h+1Zt6t5p+ddtOo7Pe8TqVzV\nM/WMh1Suqv17vp8bW6a77oH8ZWEAAAAAAAAAAAAAmFM+FgYAAAAAAAAAAACAOeVjYQAAAAAAAAAA\nAACYUz4WBgAAAAAAAAAAAIA55WNhAAAAAAAAAAAAAJhTPhYGAAAAAAAAAAAAgDnlY2EAAAAAAAAA\nAAAAmFOT6XT67d1sMnnZWvvi3/uvHrXWXn1rBfjNpq7uTl3dnbq6O3V1d+rq1/Nt1dcPptPpR9/C\nffgV7IG6qKu7U1d3p67uTl39etTX3dkDfQDsgbqoq7tTV78e9XV36uru1NXd2QN9IP6DfZAxcnfq\n6tejvu5OXd2duro7dXV3/y979xfj15Ufhv17OTMUyXC4JCWSkGTtehcwdmN46028CFykCFKnKdw0\nqPPQAglawCgC+CUFUqBF4falaIEA6UvbPAQFDMddP7RJA7dpjCJobdgpXBuLrTdd1/Zm7f0jSKsl\nVQ4lkiJZzlCc4e2DxoWs1f1+f3O/uhzqN58PsJA4h+fec8+/+73nnB09zboSBx0ja0Et6mp16mp1\n6mp16mp16uponqm1oKd6WPj7bj4MXx3H8YvHVoCPEXW1OnW1OnW1OnW1OnV1NOrrZNLuq1NXq1NX\nq1NXq1NXR6O+VqeuTibtvjp1tTp1dTTqa3XqanXqanXq6mTS7qtTV0ejvlanrlanrlanrlanrk4u\nbb86dbU6dbU6dbU6dbU6dXU0z1p9nTruAgAAAAAAAAAAAAAAy3BYGAAAAAAAAAAAAADW1HEfFv65\nY77/x4m6Wp26Wp26Wp26Wp26Ohr1dTJp99Wpq9Wpq9Wpq9Wpq6NRX6tTVyeTdl+dulqdujoa9bU6\ndbU6dbU6dXUyaffVqaujUV+rU1erU1erU1erU1cnl7Zfnbpanbpanbpanbpanbo6mmeqvoZxHI+7\nDAAAAAAAAAAAAADAAo77NwsDAAAAAAAAAAAAAAtxWBgAAAAAAAAAAAAA1tSxHBYehuEnh2H4w2EY\nvj0Mw88eRxmeZcMw/MIwDDvDMPz++352eRiGXx2G4VuH/7x0nGV8FgzD8MowDP90GIZvDMPw9WEY\n/ubhz9XVhxiG4cwwDP/nMAz/92F9/WeHP//0MAxfOayv/2EYhtPHXdZnwTAMG8MwfG0Yhv/l8M/q\nacIwDK8Nw/B7wzD8zjAMXz38mXH4IYZhuDgMwy8Nw/AHh3PXv6iuThYxUE4MtDpx0OrEQEcnDlqN\nGGh1YiAixEEZMdDqxECrEwMdnRhoNWKg1YmBiBADZcRAqxMDrU4MdHRioNWJg1YnDkIMlBMHrUYM\ndDTioKMRA61ODLS6j0MM9NQPCw/DsBERfzci/rWI+OGI+GvDMPzw0y7HM+5LEfGTH/jZz0bEr43j\n+EMR8WuHfz7p9iPiPxjH8U9GxI9HxN847Evq6sM9ioifGMfxRyPiCxHxk8Mw/HhE/BcR8V8d1ted\niPjrx1jGZ8nfjIhvvO/P6in3L4/j+IVxHL94+Gfj8MP9nYj4X8dx/FxE/Gi818fU1QkhBlrJl0IM\ntCpx0OrEQEcnDlqdGGg1YqATThxU+lKIgVYlBlqdGOjoxECrEwOtRgx0womBSl8KMdCqxECrEwMd\nnRjoaMRBqxEHnWBioJV8KcRBqxADHY046GjEQEcjBlrNMx8DHcdvFv4zEfHtcRxfHcfx3Yj4BxHx\nU8dQjmfWOI6/ERG3P/Djn4qIXzz891+MiL/yVAv1DBrH8c1xHP+vw3+/H+8NsJdDXX2o8T0PDv+4\ndfi/MSJ+IiJ+6fDn6isihmH4gYj41yPi5w//PIR6Oirj8AOGYbgQEX8uIv5eRMQ4ju+O43g31NVJ\nIgYqiIFWJw5anRjoaMRBbcbgB4iBOCQOSoiBVicGWp0Y6GjEQG3G4AeIgTgkBkqIgVYnBlqdGOho\nxEAfCePwA8RBhBioJA5ajRjoaMRBqxMDfSSMww/4uMRAx3FY+OWIeON9f/7e4c/IXRvH8c2I916I\nEXH1mMvzTBmG4Qcj4k9FxFdCXU06/DX6vxMROxHxqxHxnYi4O47j/uFfMR7f819HxH8UEU8O//x8\nqKfMGBG/MgzDPxuG4WcOf2Ycfr/PRMStiPhvD/9zFj8/DMOfCHV1koiB5jFGCuKgmhjoSMRBqxMD\nrUYMRIQ4aA5jpCAGqomBjkQMtDox0GrEQESIgeYwRgpioJoY6EjEQEcjDlqNOAgx0DzGSEIMtBpx\n0MrEQEcjBlrNxyIGOo7DwsOH/Gx86qVgbQzDcD4i/seI+PfHcbx33OV5lo3jeDCO4xci4gfivf9H\n35/8sL/2dEv1bBmG4S9HxM44jv/s/T/+kL96ouvpA/7sOI5/Ot77T8n8jWEY/txxF+gZtRkRfzoi\n/ptxHP9URPy/4T/FcNKYS/jIiYNWIwZajTjoyMRAqxEDEWEu4SMmBlqNGGg1YqAjEwOtRgxEhLmE\nj5gYaDVioNWIgWYRB61GHIS5hI+UGGh14qCaGGgWMdBqPhYx0HEcFv5eRLzyvj//QETcOIZyfNzc\nHIbhxYiIw3/uHHN5ngnDMGzFe0HBfzeO4/90+GN1VTj8Nef/e0T8eERcHIZh8zDJeIz4sxHxbwzD\n8Fq8959E+Yl47/9VpJ4mjON44/CfOxHxj+K9oNM4/H7fi4jvjeP4lcM//1K8Fyioq5NDDDSPMTJB\nHHR0YqCSOOgIxEArEwMRIQ6awxiZIAY6OjFQSQx0BGKglYmBiBADzWGMTBADHZ0YqCQGOiJx0MrE\nQYiB5jFGPoQYaB5xUEoMdERioJV9LGKg4zgs/NsR8UPDMHx6GIbTEfFXI+KXj6EcHze/HBE/ffjv\nPx0R//gYy/JMGIZhiIi/FxHfGMfxv3xfkrr6EMMwXBmG4eLhv5+NiH8lIr4REf80Iv7Nw7924utr\nHMf/eBzHHxjH8Qfjvfnp18dx/LdDPX2oYRj+xDAM23/07xHxr0bE74dx+H3Gcfx/IuKNYRg+e/ij\nvxAR/zzU1UkiBprHGPkQ4qDViYFWJw5anRhodWIgDomDjs4Y+RBioNWJgVYnBlqdGGh1YiAOiYGO\nzhj5EGKg1YmBVicGOhpx0OrEQYQYaC5j5APEQEcjDlqNGOhoxECr+7jEQMM4Pv3fmj0Mw1+K907l\nb0TEL4zj+LeeeiGeYcMw/P2I+PMR8UJE3IyI/zQi/ueI+IcR8cmI+G5E/FvjON4+rjI+C4Zh+Jci\n4v+IiN+LiCeHP/5PIuIroa6+zzAM/0JE/GK8N+5ORcQ/HMfxPx+G4TPx3v9b5nJEfC0i/p1xHB8d\nX0mfHcMw/PmI+A/HcfzL6unDHdbLPzr842ZE/PfjOP6tYRieD+Pw+wzD8IWI+PmIOB0Rr0bEvxuH\n4zHU1YkgBsqJgVYnDlqdGGgecVBODHQ0YiAixEEZMdDqxECrEwPNIwbKiYGORgxEhBgoIwZanRho\ndWKgecRANXHQ0YiDEAPlxEGrEQMdjTjo6MRANTHQ0XwcYqBjOSwMAAAAAAAAAAAAACzv1HEXAAAA\nAAAAAAAAAABYhsPCAAAAAAAAAAAAALCmHBYGAAAAAAAAAAAAgDXlsDAAAAAAAAAAAAAArCmHhQEA\nAAAAAAAAAABgTTksDAAAAAAAAAAAAABrymFhAAAAAAAAAAAAAFhTDgsDAAAAAAAAAAAAwJpyWBgA\nAAAAAAAAAAAA1pTDwgAAAAAAAAAAAACwphwWBgAAAAAAAAAAAIA15bAwAAAAAAAAAAAAAKwph4UB\nAAAAAAAAAAAAYE05LAwAAAAAAAAAAAAAa8phYQAAAAAAAAAAAABYUw4LAwAAAAAAAAAAAMCaclgY\nAAAAAAAAAAAAANaUw8IAAAAAAAAAAAAAsKYcFgYAAAAAAAAAAACANeWwMAAAAAAAAAAAAACsKYeF\nAQAAAAAAAAAAAGBNOSwMAAAAAAAAAAAAAGvKYWEAAAAAAAAAAAAAWFMOCwMAAAAAAAAAAADAmnJY\nGAAAAAAAAAAAAADWlMPCAAAAAAAAAAAAALCmHBYGAAAAAAAAAAAAgDXlsDAAAAAAAAAAAAAArCmH\nhQEAAAAAAAAAAABgTTksDAAAAAAAAAAAAABrymFhAAAAAAAAAAAAAFhTDgsDAAAAAAAAAAAAwJpy\nWBgAAAAAAAAAAAAA1pTDwgAAAAAAAAAAAACwphwWBgAAAAAAAAAAAIA15bAwAAAAAAAAAAAAAKwp\nh4UBAAAAAAAAAAAAYE05LAwAAAAAAAAAAAAAa8phYQAAAAAAAAAAAABYUw4LAwAAAAAAAAAAAMCa\nclgYAAAAAAAAAAAAANaUw8IAAAAAAAAAAAAAsKYcFgYAAAAAAAAAAACANeWwMAAAAAAAAAAAAACs\nKYeFAQAAAAAAAAAAAGBNOSwMAAAAAAAAAAAAAGvKYWEAAAAAAAAAAAAAWFMOCwMAAAAAAAAAAADA\nmnJYGAAAAAAAAAAAAADWlMPCAAAAAAAAAAAAALCmHBYGAAAAAAAAAAAAgDXlsDAAAAAAAAAAAAAA\nrCmHhQEAAAAAAAAAAABgTTksDAAAAAAAAAAAAABrymFhAAAAAAAAAAAAAFhTDgsDAAAAAAAAAAAA\nwJpyWBgAAAAAAAAAAAAA1pTDwgAAAAAAAAAAAACwphwWBgAAAAAAAAAAAIA15bAwAAAAAAAAAAAA\nAKwph4UBAAAAAAAAAAAAYE05LAwAAAAAAAAAAAAAa8phYQAAAAAAAAAAAABYUw4LAwAAAAAAAAAA\nAMCaclgYAAAAAAAAAAAAANaUw8IAAAAAAAAAAAAAsKYcFgYAAAAAAAAAAACANeWwMAAAAAAAAAAA\nAACsKYeFAQAAAAAAAAAAAGBNOSwMAAAAAAAAAAAAAGvKYWEAAAAAAAAAAAAAWFMOCwMAAAAAAAAA\nAADAmnJYGAAAAAAAAAAAAADWlMPCAAAAAAAAAAAAALCmHBYGAAAAAAAAAAAAgDXlsDAAAAAAAAAA\nAAAArCmHhQEAAAAAAAAAAABgTTksDAAAAAAAAAAAAABrymFhAAAAAAAAAAAAAFhTDgsDAAAAAAAA\nAAAAwJpyWBgAAAAAAAAAAAAA1pTDwgAAAAAAAAAAAACwphwWBgAAAAAAAAAAAIA15bAwAAAAAAAA\nAAAAAKwph4UBAAAAAAAAAAAAYE05LAwAAAAAAAAAAAAAa8phYQAAAAAAAAAAAABYUw4LAwAAAAAA\nAAAAAMCaclgYAAAAAAAAAAAAANaUw8IAAAAAAAAAAAAAsKYcFgYAAAAAAAAAAACANeWwMAAAAAAA\nAAAAAACsKYeFAQAAAAAAAAAAAGBNOSwMAAAAAAAAAAAAAGvKYWEAAAAAAAAAAAAAWFObnczDMPxk\nRPydiNiIiJ8fx/FvZ3//+eefH1955ZXJ9FOnps8uP3nypCpLmj4378HBwWL3HcdxkbxVmaq6zCxZ\nV9kzVXWVXbt63iw965MR+TN3+kal04ZZ3m7f2djYmJ03q8uqHdZNVVeduTJL74yziOXGw5JzWvbM\n1ZyW9fcqb2Z/fz9Nz543u+/e3l68++67y01MJ8zTjIGqPpHlrcZ1Zz7JdOaTpeKjKr0Tp1Q6ddlR\nlbkzb3fqY6n2r3RijWpe78Qq2RjvvD+rMi1Z13MtOf473zFL1tVS81JV5mxeWjIWzTx+/Hh23mwc\niYE+ekeJg1544YXxU5/61Kz7dPrikmsIxxXHZJb8ZspU75rOOkAlq6/qvkutqWXfiFXeTizSmfO7\nbbi5Ob20XOWtvnPm6sRlS37HLDXvLLl20RlLnb615LdI1u+q593a2pp13eraVd6sPrK8u7u7YqCP\n2Jy1oE9+8pPZ9SbTllzPyfJ2vn2XfP/OvW43b6dcS35ndr4Fl4qDltwPW2o9pxtDZ++Fqv0fPXo0\nmZbFVxHLfjdklloLqHT6e6YzZ1U6sUynX3a+C6p4pBNTdt47WXo2jt59993Y398XB31EjhoDvfDC\nC7NjoE5/WvKbrGOp9d/Ovn1lyT2c44qBllqTWXIfptP+2bze3Q9bai2oeqYl482lLDWWlpwrO2Op\nY8n17ay+qrrMYv5qT6vzvpt7Jujhw4crrQXNPiw8DMNGRPzdiPiLEfG9iPjtYRh+eRzHfz6V55VX\nXolf+ZVfmbzm2bNnJ9N2d3fT8mQNVMkms/v378/OW+ksimd5qzJVdZnJ6vnevXuz80bkA6mqq04b\nZvWR9cnq2p2+UXn48OHsvJ1FiKrvnD9/ftZ9IyJu3749mVa1Q6Z6ps5YmnvdSvVSycZS1UZZemec\nReRzQGeOrvJ2xkP2Is36ZETE5cuXZ+fNVHmzNszyfvWrX51dJv64uTHQr/7qr05eM5vn7ty5k5bn\nzJkzk2nVuM7y7u3tpXkznflkyfkze6asLiLyclVl7sReHVW5OnFMVpdV3iy90+8q2fN2Yr6Iuv9k\nsjHeeX92+vRxOa74qZpnO4dYK51YNMtblfnBgweTadWctb29PZlWjZXMm2++OTtv1oZf+cpXZl+X\n73fUOOhTn/pU/NZv/dbk9Trf8tncveQ31XHFMZnqfbFULFK9P6v7VutImeqAXKazDpSVOftG7N43\n+9brxKLdNsyeucrb+W7OdNaQOnPHcc071VzZeW9X6y1Z3+v0rSW/RXZ2dmaVKSLi6tWrk2mdtZys\nTBH5M2X3/fKXv5xel6OZsxb0yU9+Mn7913998pqdmKKzl5blrd7Nnfuu2zp8parLpcq9ZFyQPVOV\nt/O8nft21r6qPn3t2rXJtKr9X3vttcm0559/Ps275HdDpjP+Ozr9PdOJc6r0ak2us0ad9enO+K9i\nmU5MmfWd6r7ZWMrG0R/+4R+m12V1c2Og3/iN35i8Zjb/ZmuWEb39sOPal8jmsc75mSpvtead6ewd\ndr7nKp13Qmfuzeoymx9XKVcmm7c77V89bzUOL168OJlWxQRvvfXW7HJ14pjjstQ68pJnB45rz3vJ\n9e2svqr+fuXKlcm0ak8ra4eqDbM5K8v7m7/5m+l1/0jn13X+mYj49jiOr47j+G5E/IOI+KnG9QAA\nPg7EQADASSUOAgBOIjEQAHASiYEAYM10Dgu/HBFvvO/P3zv8GQDAOhMDAQAnlTgIADiJxEAAwEkk\nBgKANdM5LDx8yM/G7/tLw/AzwzB8dRiGr7799tuN2wEAPBPEQADASVXGQe+PgW7duvWUigUAsKgj\nrwVl/4ldAICPCTEQAKyZzmHh70XEK+/78w9ExI0P/qVxHH9uHMcvjuP4xeeff75xOwCAZ4IYCAA4\nqco46P0x0JUrV55q4QAAFnLktaAXXnjhqRUOAGAhYiAAWDOdw8K/HRE/NAzDp4dhOB0RfzUifvmj\nKRYAwDNLDAQAnFTiIADgJBIDAQAnkRgIANbM5tyM4zjuD8Pw70XE/xYRGxHxC+M4fj3Lc+rUqTh7\n9uxkepa2u7ublmdzc/pRqryZKu/29vZk2t7e3uz7Vh4/fjyZltVFZX9//1jydvN36rpz36zPdmTt\nGxFx7ty5ybQ7d+7MvnbVd6q6evTo0az7RuR1WdXzvXv3ZufNVOP/5Zdfnky7f/9+69qZ6tqZTn/v\nlLmytbU1mVb1nYODg8m027dvp3kvX748K60qV1VX58+fn0zL6oJnwxIxUKaam7P0zrxevU+y3xRY\n5T1z5kyannnw4MFk2qVLl2Zft5LVVTUvV+3QmQM7smfqtFEli9uq+2Z5q7m3G6vOVd13qXiyoypz\nNtY67VCNleza1ffArVu3Zt+3qo9snFbv9eze1X2zZ67mpew7tpKVq2qHrO9Uc/iSsSirO2ocNAzD\n7HfKkvNjJ5a/evXqZNr169fTvJ11r853Qnbtqp6zusq+xY9TJ36u6iOry04bVvN2lrczP1Z1VcWi\nnT7QGeNLxcjVuzdbB9rZ2Zl9304s8vDhwzRvll7NK8e1htS5b2certaBsjWm6nkvXLgwmVa14enT\np2eViY/W014LWvL7tRMHZfNGZx22s97d2TvIxmbl5s2bafqS34IdWftX34nZM1Vlzp63qqusX3b2\nYTrfxRG9OfjatWuTaVV9ZG1YjYesnar4Kquv7rrK3LxVLNoZZ521oONaz636ZNaGWZwbkT/T22+/\nPfu+nW+CbA7f2NiYfV3+uDkx0DAMrfMqS+ns8WTrllXerJ93YqDO99zFixfTvNka73HFKd17Z3Xd\n2cNb8kxY9rydfZhq7s32Yat7Z+c2IvI9wOz8RER+Fum5555L82bl6pzryMoUsVyfrWRt2B1nnT2+\nLCaonrdzpqET93faIVPVVTZWsnnnyZMnq91/pb81YRzHfxIR/6RzDQCAjxsxEABwUomDAICTSAwE\nAJxEYiAAWC+njrsAAAAAAAAAAAAAAMAyHBYGAAAAAAAAAAAAgDXlsDAAAAAAAAAAAAAArCmHhQEA\nAAAAAAAAAABgTTksDAAAAAAAAAAAAABravNp3mwcx9jf359Mv3///mTa3t5eeu2tra3JtOyelcuX\nL6fpu7u7s9IiIs6ePTuZtrmZN02WXj1vll7dN3umKm8ly5/1jYi8Lh8/fpzm/c53vjOZ9iM/8iNp\n3sw777yTpp8+fXoybWdnJ827vb09mfbcc8+leTvtlN03Iu9bnfFQ6fTpLP3GjRtp3qzMVZ/tjOGq\nT8+9bzdvVq5sjq7ydp63ko3TbIxGRLz77ruTaVV/zt5p1Vjh42kYhnQMdeKJ7vt3yqVLl9L0KjbL\nPHjwYDLtypUrad5sfN25cyfN24kJz58/PztvJXtnVHPghQsXJtOq5/3ud787mfZjP/Zjad7s2ln7\nRuTPdOvWrTRv1g7Vu6bTZ6sYKBunnTi2Gt/ZtasyZ9euxlJ23zNnzqR5O+MwK1c1Z2XpnTaqVG2Y\nPVPVhlld3759O83bmXeysVa1Q+d7odN3OD7VOlCm0xc7qnWgbHx1ypS90yudb4jO91ZV5uraS62p\nVXlff/31ybTPf/7zad65MX2V3llDWGo9JaLXxlV9ZOOls/5Qya597969NG+2XtcZh1XerFyd9q/u\nW82HWX1UccxSc2kl6/OdWKMqc1bXBwcHad6s/bMyj+OYXpfj11kPXWotqJr3s77cmY86+w6dd0Jn\n7q7myGpeuHnz5ux7Z9eunimbu5d872c6cV9n7aPS6dPVN3lnTzNT9bss/fr167PzLrmnkY2Vahwu\nKWunqu9k9bXk/N/5Hsk8//zzs+9bxd/2y9ZT1o87fbyK57O11E4s0vmOqO6b6bwv7t69m6Zn7VCt\n/1ey+qr2cLJ7V+2Q7T1V+5LZfav5MytXtQ+TvU+qdfjs2tXce/HixTQ9e6ZHjx6leTPVt3HW56sz\nUhsbG5Np1b7kccVAWd+qYr7OOO3EMVW5srrulLka/1ldLnl2oPPuyNKzujp1arXfGew3CwMAAAAA\nAAAAAADAmnJYGAAAAAAAAAAAAADWlMPCAAAAAAAAAAAAALCmHBYGAAAAAAAAAAAAgDXlsDAAAAAA\nAAAAAAAArCmHhQEAAAAAAAAAAABgTTksDAAAAAAAAAAAAABravO4C/B+m5vTxXnppZfSvLu7u5Np\nZ8+eTfM+fvx4Mm17ezvNe//+/cm0S5cupXn39vZmXTeifqZMVs9ZPUZEbG1tzc6b3TciYn9/f9Z9\nq7yVz372s7PzZn2nktVX1e+yvFVdZe1Q9buqDTMXLlyYnbfqW1l9dfp01b7VtY9DNTdkbVzlvXfv\n3qwyVfeNqPt85uDgYDKteqbz589Ppt26dSvNm5V5Z2cnzZuVqzMOsz755MmT9Los6+DgIG3brF2v\nXr2aXjtr9+r9mN23GpdZP79y5UqaNxtfnbn1zJkzafqDBw8m0zpxSicOqe5dzWOd+vrRH/3RybRq\nLsqeuSpT9kxV36nKlcn6x507d9K82fuiUn0TZHWZfS9E5OO0M/6rd+CLL744+76Zqr93xmGnXFWf\nzmLGKp6s5q1MJ67L+vTdu3fTvFm/u3nzZpo3i72rcTj3W1QMdPyy8ZeNgerbdimd797Ot3xnTaUq\nc2cNKfsOrObWag7M8ndioKo+Pv/5z0+mVWXO+mUnTrl8+XKafvv27cm0Jdc9OvFEp991nqmjum+W\n3lkjrPrstWvXZl/77bffnn3frN9VqrxZ/6j6XXbtqm9k83Q1hrO1zevXr6d5s+et1tvmjuFxHNPr\nsrxxHNM26sxl3XWIKZ29tM67u7OOUNVjZ92kE290VHspnf2hz33uc7OuW1276qTYZu4AACAASURB\nVJPZtavn7cSjmU7cV+Vfcj8sGw/VM3XydtY+srpcMg7q7Gl11pmqvEvtS3a+C5eaK7t5s7p8+PDh\nZJq1oGfb3HWiKm/HCy+8kKa/9dZbk2nVWlA2rjvf3FXezp5GZy+t2tPI6qPaS+nMNz/8wz88mbbk\nnJ+p9sOydqjecdlYqtqoqufsbEbV77J7d2KgyqNHj2aVKaLX7zrfDNnzdsdhpvNurnT2w7Jnqq7b\nWQvK4rZqLzVrp2x8V7K6WDUG8puFAQAAAAAAAAAAAGBNOSwMAAAAAAAAAAAAAGvKYWEAAAAAAAAA\nAAAAWFMOCwMAAAAAAAAAAADAmnJYGAAAAAAAAAAAAADWlMPCAAAAAAAAAAAAALCmNp/mzYZhiM3N\nebe8f/9+mv748eNZ1+3ed3d3dzKtKtP29vasMlX3nVvH3fuePXs2zVvV5ZLlXsrDhw8n0w4ODha7\n79bW1uy8+/v7k2lVn9zY2EjTHzx4MKtMERH37t2bTKueN3umql9lffry5ctp3k47ZHmrsZK5ffv2\n7PtWc1Znnq36TtZO2TirvPvuu2l6Vl/VeMjyVn0nmy9ff/31NG9WV9lYGMcxvS7HK+sTnTmhkvWn\n6r7ZnL+3t5fmXSoGqmKRTDZ+qvteunQpzXvnzp1ZZaruW6nm7c57u/PunTuPReRt3HlPVW1Yqfr8\n3LzHFR9/5jOfSdOzuq7ioyzvkmMlu++FCxfSvJ1+WZUru3cWH0fkdV21w927dyfTOjFQdd8rV65M\npr3xxhtp3o/j9yLv6cz7mc68n/XVnZ2d2fftjPlq/Cy1DtSJNTrf25Uqb+fbtvpuznTaP5tfqzm/\nE+dmZa6u24mRK50xvNQ68LVr12bn7dZl5u2335593Wycnjt3Ls1b9cvs2lUbdeatzjdB9p1btWHW\n36v4KVsneu2119K8mWzN7MmTJ7Ovy0djGIb0fXXmzJnJtM467VLvjIh8HHTmo6rMnW/QuWWq7lvl\n7azTV/Nv553SWWfs7Id2vpuXylv1u6odMlW5sv5T9a2s/at2yPL+4A/+YJp3qZiy6s+ddsiunc3B\nEb027IyHKm82d3Ri96oNO99QWYx9/fr12dfl2TWO4+z9g6ofd/YlsrxvvfVWmjdbt67mk05s1tk7\nyJ63s59R6bybl9wPzZ65KnOnLrMzEtWcn+1bddZjqj5byc5BVfWRlbuzNtrxyiuvpOlLzTvVfLfU\neKjmpM7ZrKr9sz695HnCbA6v1nOyvFWZsv2wN998M82byeaOVc8E+c3CAAAAAAAAAAAAALCmHBYG\nAAAAAAAAAAAAgDXlsDAAAAAAAAAAAAAArCmHhQEAAAAAAAAAAABgTTksDAAAAAAAAAAAAABrymFh\nAAAAAAAAAAAAAFhTDgsDAAAAAAAAAAAAwJrafJo3G8cx9vf3J9M3N+cXp3Pdvb29ybQ7d+6keS9d\nujTrulX648eP07y7u7uTaVevXp2d9+zZs2ne+/fvz0qLiNje3p597UpVX0td9/Tp05Np1fNkfadT\nF5XOOHvw4MFHWJI/Lut72fiuVHmzNq7qKsvbKXOlGqdz7ezspOnV3HLv3r3JtHPnzs3O23FwcJCm\nb2xsTKY9fPgwzbu1tTWZVj3P7du3J9OqvpOlZ2njOKbX5Xhl7+ZK9s6o5osszqn64pUrV2Zdt1Ll\n7czbnXdgdt+qzNl8UV27asOs71T3PXPmzGRa5z1W9efz589PplWxRmesZLF3Fbcv9Z0SkbdDJSt3\nVeZs7qj6ztzrRuTfBJ26qPJm7XDjxo0070svvZSmd+oymz869XH37t00Pbv2Ut9WERHf/OY3F7lv\nVo9VPMjy5r5Tqj7RWQfqfH9cu3Zt9nWzZ6recVl6VqYqb+f7sipztQ6UfRdV82fnmbJyVe+xrFxV\nv+u8XzsxUCZrg4hemStLXbuzplq1YTbvVG2U9cuq3124cGFWmSLytY2bN2+meau5Jes/1fjP3t1V\n3qydOmOlsw5Yef311yfTqrGQlcs60LNtHMfZ/WbJ9fBsnFSxzOXLl2fftzOHdvbDOu+bLG/1vEuu\n53Ti4M76Rmf+zeqjU6ZOPVdt2FnP6zxTZal9qc5Y6YyHt99+O82bxUGduK+z/x/R28ftrOd35tKs\nvqr27/SPb3zjG7PzZrJ10SdPnixyT1YzDMPsebBap19qL6W6bzYndOaiaj8kq8dqnrp169ZkWrXu\n3NlLqcqV5a/asDOPdWRzb9X+WaxavUuW2g/LzkdE9NbTq2tn+4PVfTvf5EudU6zGQ7a+UcUi2T58\n59srmxsiIl588cU0vbOulj1z5zumGktzz9esUq7Md7/73dl5l/yeiPCbhQEAAAAAAAAAAABgbTks\nDAAAAAAAAAAAAABrymFhAAAAAAAAAAAAAFhTDgsDAAAAAAAAAAAAwJpyWBgAAAAAAAAAAAAA1pTD\nwgAAAAAAAAAAAACwpjaf5s3GcYzHjx/Pylvl29/fn3XdytmzZ9P0+/fvz0qLiLh8+fJk2u7ubpp3\nc3N+0925c2eR61Z5q2fKnDlzZva1q3Jlbfz666+nea9evTqZdunSpTTvw4cPJ9M2NjbSvNnzVmNh\ne3t71nUjIra2ttL0js4zVeM0c+HChcm0e/fupXk79ZHdNxujEXmfrsqUzaVZ31jl2ll61t8r586d\nS9OrcZr5xCc+MZn2zjvvpHmzutzb25tdps48nI2VcRxnX5e+cRxnxyqdOaGKRbK8nRioetZs/FTv\nouz9Ws1TVV3OVd23EyNVddmJgTJV38neGdX75O7du5NpVcyftWHVZ7N4spq3q7rs1PWDBw9m5810\nYsKq/bMYeGdnJy9YomqH8+fPz86btVH2PF1VubK4v/qe+PrXvz6ZVo2lK1euTKZVc2Un5svGaTUW\nsjbM0oZhSK/Ls2vJd2DWj6v3+s2bNyfTOt+mVQyUXbt63mxe77zDqrrqPFNnDWnJvFmZs3W+iLxP\nV/39uNY9K51yzV0jriwZA2V5b9++nRcsUb0/n3/++cm0zlpe1f6d+aGKgbJ7V+W6fv36ZFpVH5/7\n3Ocm027cuJHmzVT37cxLS41/ltdZC6p03inZ2M7WrCOO7/28ZIy1lE6cVL0jl3qHVt/z2XuwioOy\nMld11XnHZn2nqsfj2g+rdL5lsvrIvnMienvaHdmc1VmD6HwzVumd+qjKlY2Hqk+//PLLk2lZfBXR\ni0eyNdkq/s5YC3p2PXnyJP0eyNquMzd3vo2W3FvKxkD1vFnezlzTGXtVXXX2BztrI529+Vu3bqXp\n2Tp9Fad2+k72TFVdPffcc5NpBwcHad7qrFLWB6o+3dkPy565en9evHhxdpk656syne+2Tt5sbyii\nHkvHtUb56quvzr72K6+8MplWjf/Ot/dS3xNZGz158mSla/jNwgAAAAAAAAAAAACwphwWBgAAAAAA\nAAAAAIA15bAwAAAAAAAAAAAAAKwph4UBAAAAAAAAAAAAYE05LAwAAAAAAAAAAAAAa8phYQAAAAAA\nAAAAAABYUw4LAwAAAAAAAAAAAMCa2jzuArzf48ePZ+fd3t6eTNvd3Z193Up27atXr86+7t7eXpp+\n5syZybTbt2+neTc3l2n2/f39Ra4bUddH9kz3799P816+fHky7ezZs3nBEnfu3EnTL126NDvv1tbW\nrLRKlbfqO1kfWGp8V5bsl1n/qPpdR9Y/sn4Vkc9ZVfu//fbbafrBwcFkWtWGDx8+nJUWkbfDvXv3\n0rzvvPPOZNonPvGJNO+DBw8m06rn3dnZSdMzWZ8+f/78ZNrGxsbse9J3cHCQzgtZP67GdWeey/JW\n8VP2PFWZs/mmetdkdVW9P+eWKSJ/j1XvuCo9q+tqPsnasIqfsjmj0uk7nfgqy1v1nayNq3G0ZDzR\nic2zvJ0yV30na+MqbydezMZ4NVayclXxQhbjROTv2M44q+a0K1euTKZlcUpExK1btybTqj6ZfW9m\n36kRvRh57lx56pT/j/RxOjg4SMdYJybI2r0zB3bWVKr7dr5ts7q6fv16mrdT5ixv9c6v5vXO2kZ2\n76pcFy5cSNMz2TNV75POfTuy+Klqo07/qPJm5araMLt2Z+2qasPO+sNS6zFVv8ruW9VVNbdkOrFZ\nFS90xtJ3vvOdybTqO6Wzx9BZF83ue+7cuck0MdDxq+KgTNXPO9/VmSoOyvpjts8SEXHt2rXJtNde\ney3Nmz1vJ3brjOtq7q7aPsvfmTOq+sjm5876RCcO6sQbnT2t7npe1i873yOdNcol16+yct28eXP2\ndasyZ/FINe9kdbXkHl5njq7qIzt7UO07ZelVXWZzS7WH11mz78yHHJ8nT57MfsdWe0uZzrp01U+z\n58nWaCPy+bNzBubu3btp3rllqu7bXQvqfM9n9VXFQNk811nD7+yHdd7bzz33XJqexV7d/ZDMo0eP\n0vRODNzZH8yeqbOXWrVhde1MNsarPpvtD3XW3Kr0zlmlKjbL9p6qen7jjTdmXTdiubNZnfXLrMyr\nrgVZMQIAAAAAAAAAAACANeWwMAAAAAAAAAAAAACsKYeFAQAAAAAAAAAAAGBNOSwMAAAAAAAAAAAA\nAGvKYWEAAAAAAAAAAAAAWFMOCwMAAAAAAAAAAADAmto87gK8397e3mTa9vZ2mvfx48ez73vv3r3J\ntAsXLqR5L1++PJn25S9/Oc37hS98YTLt0qVLad79/f3JtKousrxnzpxJ8965c2cyrVPmKr3TvmfP\nnk3Td3d3Z197a2trMm1zMx9eDx8+nJ23U1fZ81bjrKqrrD6q9s/aqcqbqeoyq6/qvtm1q7F0//79\nybSqHbK82RitVH0na9+IfA7P+ntExIMHDybTqrnl/Pnzk2m3b99O8x4cHEymXb9+Pc2btX9nXqn6\n3TvvvDOZlrXBkydPZpeJvo2NjXRsZ+1ezWNZu1fjujP2suf5+te/nuZ96aWXZl03Ip8Ds/kgojdH\nZuO6ihcrnfmket8speqXc/N23r2VThyb9buIvFzZGK3yLhkDdb4JOjFQVh9V3J6VuVPPnftG9OLJ\nrG9dvXo1zZvJvnErVcyXtXEWW1WqPpvVVdb+4zjOLhN9p06dmv2tV/XFTsydjdsqFsme52tf+1qa\n91Of+tRkWhVPZM+brU1FROzs7EymVXNgNp9UbdR5ps73afVMmeq+Wf/o9MmOav7srANV39RZXVf1\n0Vnry5656peZqs9m963yduK6rK469VyVqfNtVsVmWbmqOW2pdeLOvFPlzfpHNc6ya2dzdCcu46Ox\nsbGRtn3Wtp33cyXrc9X4y8bBN77xjTRvNl9du3YtzZs9b1VX2VxVvTOyNuqupXfm506s03lPdt5l\nnbgv04mDOn2n0nmmzru90+86/aqKgzr78J13d+d7s5K1UzVWsrzVWlCWN9s7iog4d+7cZFoVj2T1\ndfr06TTvUrJ6thZ0vKr9sGx+7ZwnqebP7Bul2lvK9g9effXVNO8rr7wymdZZS6/yZnXVmae682em\n+o7MnrmaP5faa6nqo3N+Jkvf2NhI82b7YVXfefToUZqe5c/2nSN663mdNuzsw3cs1e+qes7yVs/b\n2Vus5uEs75UrV9K8mVu3bqXp2Tupmney+X+pPhkxfx4ehmGl+/vNwgAAAAAAAAAAAACwphwWBgAA\nAAAAAAAAAIA15bAwAAAAAAAAAAAAAKwph4UBAAAAAAAAAAAAYE05LAwAAAAAAAAAAAAAa8phYQAA\nAAAAAAAAAABYUw4LAwAAAAAAAAAAAMCa2jzuArzfmTNnJtPu37+/2H0vX748mba7u5vmzcr82c9+\ndnaZKlW5Mpub082+v7+f5j179uxk2t7eXpr38ePHecEasnJnzxsRcePGjcm0S5cupXnfeeedybSq\nja5evTqZVvX37Jmq593Y2JhMu3XrVpo3GysReTt02r/Km/W9bIwu6c6dO2l6NpYqWX1sbW2lebO+\nlfWNVWT9o+rTWX1kYzQiYnt7ezItG2cReX3dvHlzdt5qLu2Mh+zaWdo4jrPvyfKyufvNN99M82Zj\noJoTsvtW4za770svvZTm7by3s3JVYy9TjcusLqu8VUzQuXYnb9a3XnzxxTTvgwcPJtOqdui0U9U/\nMleuXJlMq97blU65sv7Rea9XscZSsXnWNyLy2KzqG1ldVc+bxblVDFS1w4ULFybTdnZ2Zl/7d37n\nd9K8WZxTxe1ZO1SxV8fcOCYib/8s7cmTJ3XBODbZ/Hnv3r3F7puNvc56y8svv5ymd66dzdudNbNO\nnFKp2rDzTZXN+9U7Lpubr127lua9ffv2ZFpV5qy/V2XuPG8Wt3e+1Ve591I68eTc90lEPsZfe+21\nNO+5c+fS9Ezn/dmJFzvjsNO3vva1r6V5szinioGyeef69etp3o7O/N/Jy8dX9r6pVN+n2fdLZ9wv\nGQdleZdcC+qonjdrpypvNtdV9ZGteVdxUDZPVnWZ9bsl49Hsmb797W+nebMYKiKv6yWfqdPn3377\n7dl5s/HfifsfPnyY5u3EQdlcWrVvJauPag7P7v0Hf/AHad5s/H/6059O82btVMVunT6b5a3i0W99\n61uTadk4sx92vJ48eZLu3XfOm2SWnFuzMnf2Us6fP5/mXWr9f8n1i6oNlzq7UcXA2VpQdZ4g2z+q\nnrezd/Tcc8/Nzpvth1X7zlWZO+M0s+T+UNaG1fNmc0u1t5i986vnza5dnWPr7HlW9ZHVdXXeLLv2\n7/7u76Z5s7njM5/5TJo3q+uqzHOvW6VXc2HWhh/FfpjfLAwAAAAAAAAAAAAAa8phYQAAAAAAAAAA\nAABYUw4LAwAAAAAAAAAAAMCaclgYAAAAAAAAAAAAANaUw8IAAAAAAAAAAAAAsKYcFgYAAAAAAAAA\nAACANbX5NG926tSpOHv27GT6/fv3J9M2N+cXtcq7v78/mba1tZXm3dvbm0zLnjUiYnd3N03PZM+U\nPU+lytu5b1WXjx8/PpZyXbp0aTLt9ddfT/NeuXJlMq163ocPH06mVX0j61vV83b6R1WuLL0aD51y\ndWTtlM1JERE7Ozuz73vmzJnJtDt37qR5szJn46hy7ty52Xkj8j5dycq9sbGR5s3aaXt7O8178+bN\nybSsjSIiHjx4MJnW6c9Xr15N07/97W/Puu44jrPy8dHY2NhI+2PWj7N3zTrKYquIfFxX8/ZS8VPl\nwoULafrt27cXuW/1TFldfvOb30zzvvTSS7PKFJHPr2+++ebsvJVO7F3J3gmVLBbtXLeSxRNVjJON\n0yoWydqwEwNX983iiSpur74n7927N5lWjcOsPqqYoNOns/mymrOy5+3EoufPn0/Tv/Od78y+Nsdn\nGIbZ3y9LfkNm963G/FJrV5WsPjrzQTVuqzmyk7fTDp01pMuXL0+m/d7v/V6a93Of+9xkWjY/VuXq\nrqnNvW9n3q7yV8/Uea93yp3VZdWGr7322qzrRuR9urpv5zsmq+dqnFXp2fpVJ36q5v9M9W2V1VcV\ne2UxcvUt2nnfZc+Uzf/WgY7fMAzpWMjar1rTPK619KzM1TyY5a3mwbn7ihG9OTTLWz1vZ2xXc2hW\nl1VcmMVB169fT/Mel04cnKV3Y/fOftjc60b0yv38889PplXtn421aixle09Lrtd16qozp1WxalaX\n2RiNqOfLufftxEGdcfisvu/oqWKgjiXXXTJLneu5e/dumjfbH+zsHXRioEpnf71zrqt6pmzNu9oP\nW2qfttoPrc5IZKoYuSO7dhUDPffcc5Npjx49ml2m6t2b7cPduHEjzXtc76LsHVnNSZ1zB9XzLtW3\nqv2hbLxUZcrmnaxvRORrX1W/y+qysweR1cWqa0F+szAAAAAAAAAAAAAArKnysPAwDL8wDMPOMAy/\n/76fXR6G4VeHYfjW4T/zo9YAAB9D4iAA4CQSAwEAJ5EYCAA4icRAAHByrPKbhb8UET/5gZ/9bET8\n2jiOPxQRv3b4ZwCAdfOlEAcBACfPl0IMBACcPF8KMRAAcPJ8KcRAAHAilIeFx3H8jYi4/YEf/1RE\n/OLhv/9iRPyVj7hcAADHThwEAJxEYiAA4CQSAwEAJ5EYCABOjlV+s/CHuTaO45sREYf/vDr1F4dh\n+JlhGL46DMNX33rrrZm3AwB4ZqwUB4mBAIA1c+QY6NatW0+1gAAAC7AfBgCcRGIgAFhDcw8Lr2wc\nx58bx/GL4zh+8YUXXlj6dgAAzwQxEABwEr0/Brpy5cpxFwcA4KmxFgQAnERiIAD4+Jh7WPjmMAwv\nRkQc/nPnoysSAMAzTRwEAJxEYiAA4CQSAwEAJ5EYCADW0ObMfL8cET8dEX/78J//eJVMBwcHcf/+\n/Vk33NzMi7q3tzfruhERu7u7s9IiIra2tibTqjKfPXt2VlpExO3btyfT9vf307xZuaoyV9fOPH78\neHbeqj6ya7/zzjtp3tOnT0+mXb58Oc17cHCQpi8l65dVG1V1mcn6e6UzlqoyZ3nnzjld29vbaXpn\nLGX1UY3hThtWsmt35uhqnGV1XdXzmTNnZt83c+/evTQ9a8Pr16+neV966aXJtBs3buQF46Ny5Dho\nf38/7ty5M5lejd3q2nPSInpzZKfMmUuXLqXpS9VjJbt2Vc9LvpuzGOjBgwez81btkL3Xq3bIytWJ\n+at6zp6pek9V5er0j+ze2Xuqum9nfFfxQnbfKgbK8nbuW8muXX2nVG3Y+Sbs9OnsN7bevXs3zZup\n4vYsRuqMpWx+j4i4enXyv3QYr7766mTakydP0utyJEeOgZ48eVL2qSmdubeSjfuqvNnYrNYQsmt3\n1oEqnW/IzrfrcanaMPtey+aaiPz9euHChTRv1u+qdsieqXqPZe+pbpyapVft0FknzOq6GitZmatY\nZO58FtEbS+fOnZtMe/jw4ezrdsd3VpeddZGqnrN4szOH7+zkZx+yPlv15yyur2Kga9euTaZla0jD\nMKTX5UgW2Q/L+k01H3W+97LxeVzfZNV3ZDY+O/twnTmj0nk/LxkXdOKRrD6qmCHrd1WZs7xVG2Zz\naNXfO3utnb7T2cOp3r9ZuTrju8rbifuyb6wq7sti+2r9oipz1uc760xVn87qo3o3ZGWu8r777ruT\nadXzZv2jasMs7j+u/d8TZlYMNI7j7HmwWg/v7Dd39lKqcmWyMXDx4sU0b1auTqxRvWs683bV9ufP\nn59M63yTV30jy9v5L6NtbGyk6Vl9VP2q0/7Zflj1rqn6ZedsxqNHjybTOnF99V2d1XUVx2blqvJ2\nYq9ObJadJ+m+P7O67O6XZ7JxWo3/rMxVfWRl7tz31q1bad5sDL/55ptp3lWUv1l4GIa/HxFfjojP\nDsPwvWEY/nq8FxD8xWEYvhURf/HwzwAAa0UcBACcRGIgAOAkEgMBACeRGAgATo7yKPo4jn9tIukv\nfMRlAQB4poiDAICTSAwEAJxEYiAA4CQSAwHAyVH+ZmEAAAAAAAAAAAAA4OPJYWEAAAAAAAAAAAAA\nWFMOCwMAAAAAAAAAAADAmnJYGAAAAAAAAAAAAADW1OZxF+D9zp49O5l2//79NO+ZM2cm0/b399O8\nu7u7k2lbW1tp3qzMjx8/TvNmquetnmmuznU7z1vduypXdu9z586leR8+fDiZtrGxkeZ99OjRZNpz\nzz2X5u3o9NlMlXdzM58ysnboXDt73ura29vbad7s2tn4jsjHaVVX2bWrvFmZs/4cEXFwcDCZVvX3\nyoMHD2bn7cylWTtU7Z/V9e3bt2fnrfp7p99l9ZFddxzH9Lo8u6p3YGf+zGSxVUTvvX3p0qXJtDt3\n7qR5546BiIgLFy5MpnXGbTVPddpwb28vzZu10/nz59O82bxdPVNWX52YsBOLVn02a8PqvkuOh7nX\njcjbuBM/V7FIdu1qHFbv5kynDbOxVMVAWfxUlavSiUWy9KtXr6Z5s3n4xo0bad6s71Rj5e7du5Np\n1Tw8d84SA318Lbk+kV27sw5079692XmrMd+JRTqWvHbWDlVdZnFd9R7LVO2f1Uf1Ldex1H077+WI\nup0yWft3xnCn/au67LzzM1W/y9Z6qu+Fjup5OzFwtpZT3TebL6s2XGotv4qBsnbqrJmKgZ5twzCk\n7ZfNwdX8muVd6vukUo37rMxVHNSRlasz73ffCdnYr/YHO3FBdu1qPlpqX7KSlauzH9bp79W9l4oZ\nIvI2rto/a8Msvo7I56Wqb2R1VbVDdt9OrNLZO6yu3Vmjrub/zr7k3OtGRJw+fXoyrbNu1umzWZo4\n6HiN4zj7zMCS3zdZn8nWSiPyflztj1+8eHF23qXius5aemfvoErvnDXo6MYEc69dtW+WtzqLlI3B\n7lpQZz0n06mPaj80K3Pne75zjqmjE6d0njci4tatW5Npnb327LoRvW+RzrmErL6q/p7NaVUMlLVD\ntk745MmT9Lp/xG8WBgAAAAAAAAAAAIA15bAwAAAAAAAAAAAAAKwph4UBAAAAAAAAAAAAYE05LAwA\nAAAAAAAAAAAAa8phYQAAAAAAAAAAAABYUw4LAwAAAAAAAAAAAMCaclgYAAAAAAAAAAAAANbU5tO8\n2TAMsbk5fcvHjx8vct/d3d00/fLly5Np+/v7s+979uzZNP327duTaXt7e7Pvm9VxN29WH1tbW7Pv\nW127oyrX6dOnZ1/74OBgVlrXhQsXJtOWGkcRdRt12jDLW42lnZ2d2XmzPl+Nw+3t7cm0+/fvp3mr\n9Lk647DKW9VHVtdVv8zmw6y/R+Rz/MOHD9O82Tit6iO7bzWXZnmrcZTVZVbmYRjS67KsKgZaSjUH\nZunVPJXlrZ71zp07k2n37t2bfd/qebPxU429TvtV5erMJ3fv3p1Mu3jxYpo3u/b58+fTvFkbLqnz\n3q6eKVP1j2y8VO2fqdr/zTffnEyrynzmzJnZ9+3ofOd0ZM9bqcp86dKl2XmzvpN9p0bkc1rV727c\nuJGmZ7K6zOakiLxvVd/t2fNmcfmtW7fS67KsKgbK5qrqGyKLfatYPhsjS4iKgQAAIABJREFUVV+c\nW6bq2tk3UUQ9J8xVvS+6az1zdb4Dq/dYJ57M7tuJnyudPpu1YdX+nfqoZO1UtWFW1525o1MfnffY\nkmt5c+fgVdI734TZnFfNO1l6Zw2xs5bT0RnD1oGebadOnVos5ui4du3aZNr169fTvFk8Uo2/bO5e\nsi6ya1dl7rwzlpzbl4rPOmVecl2tsw/T2ZNecl8q03mHVmMpi7GrvZRs/Gd1sUq55ur0u26ZOvmz\nNlzqm6G6b9Xvsv7Rib879ZjddxzH2delb2NjI527O+vD2brkkvvYnb2UBw8eTKZV65bZtTtrUJXO\nWnonJqzmoqyNqzLP3V+PqPctM9kzVeeJNjY2JtMePXqU5s36TnXfqj6yPYDOPm2n73RigqrfZX2r\nGmedGLhz9qqzFtQ5I9WZ3z/5yU+m6Vn7v/jii2neN954YzKtc0ay00adOTo747jqWpDfLAwAAAAA\nAAAAAAAAa8phYQAAAAAAAAAAAABYUw4LAwAAAAAAAAAAAMCaclgYAAAAAAAAAAAAANaUw8IAAAAA\nAAAAAAAAsKYcFgYAAAAAAAAAAACANbX5NG82jmPs7+/Pyru5+VSL+v97/Phxmr67uzv72nPrIqJX\nH1mZz549O/u6VZl2dnYWu/bW1tZk2jvvvJPmffTo0WTapUuX8oIdk6wNL168mOZ98ODB7Pt2+l01\nlrI2XHKs7O3tTaZtb2+nebN26OStnjery049379/P8175syZNP3GjRuz7huRzz3VvJTVZdXfs/5R\nlTlrpypvR9ZneXZVMVA2vqo2z/pxFadkZarmhMydO3fS9GxcX7hwIc2bzXOd+bOaazrvoqo+Otc+\nf/78ZFo1B2b9o5rzO/Nc1g7Vezurqxdf/P/Yu5/YurI8P+zniaREyiQlslpSq9TdUxlMG3A2sYGB\nYSA7J4vs4kWyDLww4E0WCZBNkF2ALOJNsjfgxSwCOEYSwEF2huEssnEwSDwJ3ONG17S7VC2pRbZI\nFckWKZHUy2J6Bg147venvF+9YvXT5wMMZqZ+de499/y7v3fObfbjWDb1QzVXqvZI7/3OfOjkXlUe\n25kPh4eHC5dNfVj1Q7p2Nd5TH3ZzoNQelTR21tbWYtnXr19Pxn784x/Hsg8ePJiMdXKNKgdO86Ea\nO0+fPp2MpT68vr6O1+VmpX6vftukNbJ6t6YxU9031fno6CiWTXV++PBhLJvWyOp90dkHSm1Z3bdq\nj45l5cBV2U5bVmMrSXXu/GbuvHvHyDlh1f/pndHJNavfE5251LnvycnJwtdelm7/d+Z4Zx8o3fcn\nP/lJLHvv3r3JWOd32e3bt2M85VfV8758+XIylsaVHOjmXV9fx5yjcxaTxmsnD+rUqfMOrdb99M6o\n7pvWjOr3S2dvrFO26odOHnRTeWHS2Yff39+PZdM6WbVV50yjc05T1Stdu8pH3rx5Mxm7e/duLJv6\nofotk/qhswexrLPyMer+73wf0NkLTGvL559/Hsum+dLJg6p9s2pvNEln/KmPZrPZwvek7/3793Fu\nd/ZzUryz9na+n6jqnNaL6kyjs0Yu6xuISudcqspFOnv8nX2VZJnvi/SbrvomKJ0dVKq51MnNO/dd\n9Jy90tlXS+csY+R+6OwTdlRrVpWLpPd6VbaTA6X4j370o1g2nZd21vBOLlqNuxcvXix87Q/hLwsD\nAAAAAAAAAAAAwIrysTAAAAAAAAAAAAAArCgfCwMAAAAAAAAAAADAivKxMAAAAAAAAAAAAACsKB8L\nAwAAAAAAAAAAAMCK8rEwAAAAAAAAAAAAAKwoHwsDAAAAAAAAAAAAwIpav+kKfKitra0Yv7y8nIxd\nXV3FslU82djYWErZo6OjWHZ9ffGu29/fn4ydn58vfN2qbNWHX3311cL3Tte+d+9eLJva+vT0NJbt\njJ2kGlcp/vbt26+7On+uet40DztzJV13jDF2dnYWvnYqW43p1B5VnVPZaq4kVTu/efNm4WtXHj16\nNBnb3NyMZV++fPl1V2eM0VsrO+th1f8dacweHBxMxubz+TKqwwd6//59+U6Zsre3F+NprJ6dncWy\n29vbC983Xbsqm9bAztpbrZ8pvsx5W61FFxcXk7HUR2PkZ0rXrVRjp9NeqQ+rtkrx4+PjhetU5RLV\n2Hr+/PlkrJoPi64NY+T3a9VH6ZmqeZj6oZMvVu2c6tVZK7vu378/GUu/vcYY4+nTp5Oxaj6k/n/w\n4EEsm/qhGjsp3+ysDZ3fk2m9u3XLf0b6Jr1//z72bVozdnd347XTdas9lTSeqrUordvVnO/UOc29\nqs6pLTu/xTrvmjF6a3Nqj+p38bJ+f95UPtnZm+r8Zh4jP3Nnr6a7x5ikedrZjz05OVm4TtWYTe1R\ntXNaszq5d3Xvqo/S/kWn/z/55JNYtrPudOZLZw9x0fVfDvTbrRpvaVxU61FnbyTNoc6a0vldXPns\ns88mY1VO0Hn/dtbnSurDql6dtayT63Tum8oucx+xqnPnPKzTlnfv3p2MVXVOZzidtqzWnTTeq37o\n9GFnD7q6dsopq7Kff/75ZOzJkycL16v6DZ28e/du4bIdnbkgD/rt1TkfSF6/fh3jaS+1Osde1plG\nlQ90vq9I+8PVfTs5UNUey9rDqNoq3bdqj+vr64ViY4yxtrYW40kal8s8w6neY19++eVkrMqBO9/1\npfao6nznzp3JWLV2JJ2y1ZhN47Jas9J5SXV2nM67xshjq6pXGjtV2dTH1flg57y8c5ba2QtatJ1n\ns1m87p+RKQEAAAAAAAAAAADAivKxMAAAAAAAAAAAAACsKB8LAwAAAAAAAAAAAMCK8rEwAAAAAAAA\nAAAAAKwoHwsDAAAAAAAAAAAAwIrysTAAAAAAAAAAAAAArKj1b/Jm8/l8XF1dLVT2/Pw8xtN1Ly8v\nY9nT09PJ2M7OTiy7tbUV48nBwcHC1120HccYY319uttTbIwxvvjii8lY1VadOlftkdpyf39/4ftW\n7ty5Mxl7/fp1LJueqeqH1JbVXOn0f3XtjY2NyVg1Dy8uLiZj1dhKqvtWz3QTbqpOm5ubrfJra2uT\nsaof7t69OxlLa3R138qy2jrNhUo1D1N7pPvOZrOF60TfrVu3Fl7Lnj9/HuNpzFTzOq29lc77pHPf\nTu7VWedevHgxGdve3o5lO89bOT4+nozt7e0tXLZax1K8WrdTP1T3ra69qGrMprYaI4/Lqs5pfFT5\nc6p3py07eXv1zl/mOzJJ9erkmpVqDe+MnY5lrUtV/6bca1ljQw50s2az2cK/Ezu/bat53Zlfad5W\necrJyclkrJoD6Zmqtkrxqs7Pnj1buGxHZz3p7E911qJqXKV6dZ63evemfqrKpjFbqfKJ6t5JeqZl\nzodU504O1NmbqMZdJ3/qqPo/zYe0z9v15s2bydjt27dj2Ztqy0XHrBzo268z9zu/BdPaXq2Dy3r3\nV23RWTOOjo4WqtMYvZyx856r1pv0zJ2zpWrsdN6/y9L5zVCpxk7nPKzz27jzTJ0xnd6hlTR2lnXe\nPcbiZylj9OZwNS7TufUyc9lUtpMHVWOjs+50xgc35/3793G8pfFU7cOns5jqnObs7GwyVp0dpXg1\nTjtrUWqrzn2rs6Mf/ehHC5etdL6R6ZyHHR4eTsbu378fy6ZvEd6+fRvLds7DlnV2VLVz9Z1Teqbq\n/ZmuXfVDJ/eq1pZFdXLvqs7pN1BVNtWraufO2KrOnR48eDAZq/oo1auTL3R+i3b2kTvn8Gkezefz\nD7q/vywMAAAAAAAAAAAAACvKx8IAAAAAAAAAAAAAsKJ8LAwAAAAAAAAAAAAAK8rHwgAAAAAAAAAA\nAACwonwsDAAAAAAAAAAAAAArysfCAAAAAAAAAAAAALCifCwMAAAAAAAAAAAAACtq/aYr8JvW1xev\nzuXl5WTs4cOHsezp6elk7Pz8fOE6VWVTnVNsjDF2dnYmY8fHx7Hs0dHRQtet4ltbW7Hs1dVVjKfy\nVXuktk7PW8X39/dj2bOzs8lYNZ5TvGqr1B7VfQ8ODiZj1fNW/ZD6sJoPqd7VfdMcrsblsuZ41ZY3\nVeeNjY3J2MXFRSy7ubkZ49Xas6iqPd69ezcZW+Ya3pHGe6cf0lyZz+d1xbgxabxVcyBJc76S3hdj\n9HKR6j2XpGd6/vx5LJvmXrXGJdW8rd7N29vbk7GqrVJbV2VTvHr3prLLyumr++7t7cWyP/7xjydj\n1e+FTm6e3vndsklnnlV9mOpVrTvLWu+qPkr16pQdI8/DTl5X9UMqWz1TWvOqNS3dt/Obryqb2iP1\nwfX1dbwuNyv1ezV/qnGedPYfkpcvX8Z4mj/VWpPaqnpfdNbeTj5Z6ewhpL2cqg9TvPMuWmYO1Nl/\n+MlPfjIZ++STT3LFCimPqfbjkmrcdfZU0lyqxt3JyclkrPPeXma+0HneTr06e5vV/nTnme7evTsZ\ne/PmTSybpLExRm99SGXTXHj//v3C9+Sb0Tkf6PwmT3N7mb9fl7VPW103xav1prP+dnKKqv/TmlPd\nt1qvkk6evKyyVT988cUXk7FqL6jSyWVTrlM9U6cPd3d3F75u2iuo5lIq28llOvsInX2zMXrzMI3p\n1EdV2WWeh62trU3GOnvjVT+k3yt//Md/PBmzF3Szbt26tfB7sNrj7/yuSmMxffNRxTt5W0d1Dtf5\nvdaZt53zsGo96fRDJxdJa8qdO3di2fRMaW0do5fzP336dDL24MGDWLbak+n8nkjnElUfvn79ejKW\nxlUVr+Z/Z91J466z59rZR+6eaR8eHk7GqpwwjY9qXKb7VtIzdfYRO2e4lTRX0v7Vh+4F+cvCAAAA\nAAAAAAAAALCifCwMAAAAAAAAAAAAACvKx8IAAAAAAAAAAAAAsKJ8LAwAAAAAAAAAAAAAK8rHwgAA\nAAAAAAAAAACwonwsDAAAAAAAAAAAAAArysfCAAAAAAAAAAAAALCi1r/Jm62trY2dnZ3J+Onp6WQs\nlavKpljX1tbWwvdNZa+urmLZi4uLXLFgf39/MnZ0dBTLPnz4cDJWPe/l5WWMv3nzZjJ2+/btWLZj\nd3d3Mlb1w8bGxmSset5kfX3xqXlychLjqf8rqa3GGOP8/Hwylsb7GHnsVfM/9VPVh4ted4zc/6kt\nxsjjvWNZ1x2jHtPpmR88eBDLpv6/vr7OFQu2t7djPK2l1ZhNqrGT1suqD8/Ozha673w+j9dluWaz\nWVzbU6wax4eHh5OxzhqY1rgxejlQet6qzmmt2dvbW7js69evY9kf/OAHk7Hq3Zvm7Rh5LaraY3Nz\nczJWrdupbCX1YZWnprHTqfPTp09j2ZTHVqqxlZ65ymPSuKz6qPObIN23mv/LyoGrsmk+dPLnrjSm\nqzW881s1tdfdu3dj2ZRvdMZdZw3vuMn+J6tyoKQql+ZAZz+mktb1KidIv6k786N6ntSWVVt19oGq\nZ6r2oJL0TNV9l/UeW+b7Mz1vNe4ePXoU40lnrlRll/VOqNojqfqhk8d27ptUa2V6N1d9VNUrjekq\nB3727FmML6qTE1T5U2rLas80PW+nnfl2u3XrVpxnab2qxlQqW62DaUxVc6izF5TOJTprd+fsoKrz\n7/3e703GXr58mStWSO/BzrlEVTb1QyeHqsqmfursX1ZjJ+Wy1fraGVvVO7Yz5pOqLdN9qz2m1B7L\nyuu6OnNlmfvqqb2q30hpna7GXVrz1tbWYtmket503865Y7rvbDaL12W51tbW4p5oOtOq9uE73wR1\nfiukeqXn6d43qfad0x5vdWaVzvWr90XVDzd1tpDaq7PmV98xdM7w7ty5Mxmr+vDTTz9dyn3HGOPt\n27eTsc4eRVU2xav2WNZ+3zJzoFTnasx2zuErKd+ozpZSex0fHy9cp86ZVmfMVjrjbtFzuA/9Jshf\nFgYAAAAAAAAAAACAFeVjYQAAAAAAAAAAAABYUT4WBgAAAAAAAAAAAIAV5WNhAAAAAAAAAAAAAFhR\nPhYGAAAAAAAAAAAAgBXlY2EAAAAAAAAAAAAAWFHr3+TN3r9/P87PzyfjJycnk7HNzc147b29vcnY\n1dVVLJvqdHFxEcum+OXlZSybVM97eno6GVtfX7xbt7a2Yjy1VfW8GxsbMX779u2F7vsh10467ZXG\nVlWnalwmnbHVed7qvunaVR+msVe11c7OTownnTGdVO28u7s7GaueN9UrXXeMvM6ura3FspX9/f3J\n2NnZ2cLX7axLnTU8tdUYYzx8+HAy9vz581g2jY/UjmPk/j84OIhluTnz+TzO7TQWqzUhzZFOPnF8\nfBzLVuM8SXVOOd0YuV6dtqocHR1Nxqq1puqHVH6ZOVAqu8x3YHqmTn5U5QPLyvkqaZ6NkevV6d9K\neqZO/1d1TnO8yhc67ZHG3TLb+enTpzGe1qVq/t+9e3cyVvXh69evJ2PVmpXinT6s5mi11vLbKY3z\n6r2dxlPK1cfIa3M197744osYT9La23l/VnVO167u22mrZe4/pP6/qb2tzm/5Tq7RUb0Dqz5OUv48\nRu6nauyksp08ptJZs9J+TbX/0HneVOfqt1dnL+/Zs2ex7LLyr6rOnbU0/d6o9mNSW1VjJ42PVOf3\n79/H67J88/l8aWtSWlOqMdX5LZjyoM57f5nvo46XL19Oxrr7NZ1+6LR1ssxztmXtQXT2Ajv5d3Xv\n6t3ekeZ/VedqjypJ167aMsWrPuyUTflotVZ29pHT2lFdu2rLNE+rcZfqVeWFSSdn7Px249vr/fv3\nC595VetU55uAztlSOg+7qTO8zj5CtdakvePO7/VKNeer9lqW9A3F9fX1wtetynbe2513TeeZlvle\n7/R/Z/4n1XhPde6cd1R5+7K+JxwjP9Ph4WEs2+n/1NbVXOm8d1KOVPVh5xu4Zf3m+zP+sjAAAAAA\nAAAAAAAArCgfCwMAAAAAAAAAAADAivKxMAAAAAAAAAAAAACsKB8LAwAAAAAAAAAAAMCK8rEwAAAA\nAAAAAAAAAKwoHwsDAAAAAAAAAAAAwIrysTAAAAAAAAAAAAAArKj1b/Jmt27dGltbW5Pxhw8fLuW+\nV1dXMX56ejoZ29zc/Lqr8+cuLy8nY+vruWt2dnYmYwcHB7Fsao/qvklqxzHq/k1jo7r2nTt3Yjy5\nvr6ejK2trcWy5+fnk7GNjY2F65SuW+nct3vto6OjyVg1tlL84uIilk1zKc2VMfK466iu+/Lly8nY\n3bt3Y9nUDycnJ7liC153jHoeduq1zHGbpHG3u7sby6a1thp3qS2ruZLa6vnz57EsN+fWrVsxp0ix\nzhqYYmOMcXh4OBl78OBBLJvyiePj41g2rQlVndMcePHiRSy7vb09GavW7TQ3q+fd29uL8dSWVR6b\n2qPKY9NaVN13Wao1MNWret507bOzs1a90hio3gnp2tW7N60PabyPkedaZz5Ua1Y1XxZVjdmU81fP\nW9U5jb3OmtbJJyrVtZPXr19Pxqp5WI2PJI27L7/8cuHrslyz2SyO8ydPnkzG0u/LMfKYqOZe+s3d\n+Y3YmZfVOpbqVbVVKls9b+q/6r77+/sxnlTX7qxjSdUey9pTq8qmMV39nu7Uq2qPZ8+eLVy2M6bT\nM1fjLs3Taj+uM5fStTu5d/VuTf1f3bczLjtrWjWmO3tfqR+qfeA0dqr+77RVUp0DcLPW1tbi/uJN\nnR+kfelqPzS9f6s6VflZkur1+eefx7KdtTup5l/KcyvVOlf106Kq9aiTjyxrHVxWTjhGXa/UT50c\nu5oryzqH6+YFScpXOnuBVVulfqjum9bK6tqd9b0zlyppj/revXuxbCfvX9Zc+eKLL2JZvr3S2VPn\nPKyScpXOGU61F5Tu2/kmqDoPS9funKVU9+20Zeed0FmLOqrfkZ398M57LKne+dW3V+lsuer/pDqH\nSfWuzsPSXOqsO9VcSs+0zN+DnfHROS+tyt6/f38yVs3hzvc1nT5M432ZZ9op/rOf/SyW/RDlXxae\nzWbfn81m/2w2m/3xbDb7l7PZ7D/79T/fn81m/2Q2m/3k1/978VkPAPAtIwcCAD5GciAA4GMlDwIA\nPkZyIAD4eJQfC48xrsYY/8V8Pv8rY4y/Mcb4T2ez2b89xvgvxxj/dD6f/3CM8U9//f8DAKwKORAA\n8DGSAwEAHyt5EADwMZIDAcBHovxYeD6fv5jP5//Xr//v0zHGH48xnowx/sMxxh/8+l/7gzHG31pW\nJQEAvmlyIADgYyQHAgA+VvIgAOBjJAcCgI/Hh/xl4T83m80+G2P8tTHGPx9jPJrP5y/G+NPkYYzx\ncKLM353NZn84m83+8Je//GWvtgAAN6CbA7169eqbqioAwNemmwMdHh5+U1UFAPhayYMAgI+Rb4IA\nYLV98MfCs9lse4zxP48x/vP5fH7yoeXm8/nfn8/nvz+fz3//O9/5ziJ1BAC4MV9HDvTJJ58sr4IA\nAEvwdeRADx48WF4FAQCWRB4EAHyMfBMEAKvvgz4Wns1mG+NPk4L/YT6f/y+//scvZ7PZ41/HH48x\nDpZTRQCAmyEHAgA+RnIgAOBjJQ8CAD5GciAA+DisV//CbDabjTH+wRjjj+fz+X/3G6H/dYzxt8cY\n/+2v//c/rq41n8/H5eXlZHxjY2Mydn5+Hq99dXU1Gdvc3Ixl9/f3J2MnJ/k/MHVxcRHjydbW1mTs\n9PQ0lq3aI0nPm9qxum+67odcO42N9fU8VG/fvj0Zq9oqxQ8Ocr77O7/zO5Oxqg9T/1fPm9pymWVT\nH31I+STN/xQbY4yjo6PJ2PHx8cJ1qqTnTXUaY4zr6+vJ2Js3b2LZu3fvTsZ2d3dj2dT/1Zit+uGr\nr75auGzy7t27GK/qnaT3Q7VmPXz4F/437Ywxxnj+/Hksm8ZO9V5Jz5vqVI1J/k1fdw7UWX+TNGb2\n9vZi2R/84AeTsWr9TGOqmvPpHVi9t9N/jWeV8+3s7Cx83/QOrNq5en+msZHaaplevHgR448fP56M\nVevnslTzKM2VqmwVT2Or0lkbOu+TNE87fdiZS9XakZ636oNUtqpzVa+zs7PJWMrbxuj9Bko5QVXn\ntbW1yVg1dj799NPJ2E9/+tNYttMP6XnTX21b5u+BVfVt2QfqqK7byZs7OVBnDrx8+XLh+6Z8ospT\n0r5YascPka5d5UCpvTrvsc8//zzGP/vss8lY932SpGtX1+2Urdqyk6uma1d7G69evVr4vmnMV8+T\n2qtaOzrjMq0d1W+gVLabt6dnrvKYzt52Z/6n/euqTinfrNaOzm/gNN5TnX71q1/F6/IX+zrzoPfv\n38f+TeOiOpdK161+G6V5sMxzqc48SHOs83u9sw5W52GVzvuok1+ntv75z38ey37ve99b6Lpj5Gfq\n9EPnebt7QZ1nSvEqP1+Wzv505/y3et7Ux53fDNWYreZ4Zz1Mz1T1w7NnzyZjVVt28qBUNtVpjPxM\n1XtHHvTN+TpzoMqy1sBqTUj7h9V+6JdffjkZu3///sL1qvYtO/u/6dyqmvNpv7s6D+vsad+UTg6U\n2mqM5Z3xpf39McZ4+/btZKzzzh8jj4Gqf9MexjJzkY4016o5nNaWat1JbdUZV1UfdfZztre3Y9lO\nP6X2qt4dKWeo+iGdh1dn6Z0cKNXr+9///mTsF7/4Rbzun/mQ2fbvjjH+kzHG/zubzf7Fr//ZfzX+\nNCH4R7PZ7O+MMZ6OMf7jD7ojAMBvBzkQAPAxkgMBAB8reRAA8DGSAwHAR6L8WHg+n/8fY4zZRPjf\n+3qrAwDw7SAHAgA+RnIgAOBjJQ8CAD5GciAA+HjcuukKAAAAAAAAAAAAAADL4WNhAAAAAAAAAAAA\nAFhRPhYGAAAAAAAAAAAAgBXlY2EAAAAAAAAAAAAAWFE+FgYAAAAAAAAAAACAFbX+Td5sNpuNjY2N\nyfjBwcFkbGtrK1775ORk4Xol1X2vrq4mY+vruXmPj48Xvu/5+fnC9130utW1q7KV09PTyVhq5zHG\nePfu3cJlU1v/7u/+biz7+vXryVinH6o6d8ous14XFxeTsc3NzVj28vJyoTqNUc+XpDNuU3ukta7S\nKVs9T+qjnZ2dWPbnP//5wteuninN/2rsLEs1JlO8M4crqT3Su/D6+noZ1eEDzWazuP6mOdBRjcW0\nZlRld3d3J2PV/Oncd3t7ezJ2dnYWyy5L9W6t6tVpj/QOrMZVWpu///3vx7Jpza/s7e1NxlJ+PEZu\n66qd0/rZyXHGyPVOz1up3p+d901qy87aUeUTaVxWOV1aW6ocKD1Tdd/Dw8MYT/eu+jCNrWrcpTFd\nzYeHDx8ufN/0u71ad6rxkaS2TPeVA327dX6PpbGYxvgYvVw+5UCVzvN2fiemNb/K29J9u797Otfu\ntEfy5MmTGO/sP6b3TTU2llW2ymOrfkjjZ1l9NEaeh8ucZ6n/O/ttlc7+c1LttxwdHcV4p17p3V31\n4Zs3byZjVR6TcpG0vzzGGK9evZqMVWOns5d/7969hcrOZrN4XW5eWlOqvLqTy6R4la+neDV30zNV\n8yDFO2cDldTOVZ07v1Erac2pcrvUXp999lkse1N9mJ6pys2WmY909gJSe9zUGe8y32XL+i2zzN8M\n1Tqcxl41pn/xi19Mxu7evRvLdn4XpPZI+VVXJ2f85JNPJmP/6l/9q8nYfD6vK8bSVOdhnf3QNDcf\nPHgQy3bevY8fP164bDo7qH6TdeZP59uLzruoOuNJz9TZD+/s1/zwhz+M8bTXXr1rUp0752zV/n8n\nR67201MfpzPcSmc+dH4DVfftfNfX+X6qo5O3VTlQZ9ymsp01q6pT55y2s3+V1ofO74V03w/Ngfxl\nYQAAAAAAAAAAAABYUT4WBgAAAAAAAAAAAIAV5WNhAAAAAAAAAAAAAFhRPhYGAAAAAAAAAAAAgBXl\nY2EAAAAAAAAAAAAAWFE+FgYAAAAAAAAAAACAFbX+Td5sPp+Py8vLyfje3t5k7Pj4OF77yZMnk7Hz\n8/NY9urqaqFYFa/uu74+3fypncYYY39/fzJ2dHQUy1b1StLzbmz1KEuqAAAgAElEQVRsLHzdMcbY\n2dmZjJ2ensayZ2dnk7Gtra1YNrVHVTapxk6nvdL4qK5b1atjc3Nz4fumtq7GdJLGVXXfzlyppD7s\njNlqrqQ+qsrevn07xpN79+7FeGdMp3jVh9UzL3rf9D4bI8+Hly9fLlyndN35fL7wdembzWbxvd9Z\nix4/fjwZq/KntEZW6/ay1sjUTmPktrq4uIhlO3M+3bdq5+3t7YXjr1+/jmXTvav7pvborGNVPxwe\nHk7Gqv5P115m2cqDBw8mYzc1l6p8Ir17O3nsMnPNdO3q91NSjdnOulTNw2XVu+r/lF93cq/vf//7\nsWyqc2cupOfttDHLV71vkrQP1On3as6nsVrNn3Ttag4sax+oWrdTnavcqlqL0rWrfjg4OJiMVfsA\nqT12d3dj2dReJycnseyyLHPfqxqX3b3AKdUcTm396NGjWDY9U2cvt7OHnPZqqrJVHtNZD6u5lKQ1\nq9IZV3fv3o3xtG5VZd+8eTMZq3K+1E/VWrroGm4f6OZVe0Gd/eHOfk4q28kpOu/9qs6dPKizd9zJ\ng6o1tHMelvaPq/X3pvpwWb/nq3dG59pV2dRenX2VzpnGw4cPY9lUr/SeG6M3H5Jq3C0r/+78dhsj\n17t6pirnSN69ezcZq/KRr776ajJ2fX0dy66trU3G0m/zMfI6vaz3jr2gmzWfzxf+/VOtgZ9++ulk\nrOr3zhlAWuc660n1ey7N6/R9zIdce1HVdTu/I6v3yYsXLyZj9+/fj2Xfvn07GavW7bQGVu/8qp8W\n1dnPqcZsNQ+r902S7l3N4dSWVZ063wR2vutb9NuAMXrfBHV0vpGrynbOtNPaU43pNHaq8Z7WtPRO\nGiM/b9WHiz7vbDaL1/0z/rIwAAAAAAAAAAAAAKwoHwsDAAAAAAAAAAAAwIrysTAAAAAAAAAAAAAA\nrCgfCwMAAAAAAAAAAADAivKxMAAAAAAAAAAAAACsKB8LAwAAAAAAAAAAAMCK8rEwAAAAAAAAAAAA\nAKyo9ZuuwG+6urqajO3s7MSyp6enk7H19fyYBwcHC993c3Nz4fum+MbGRix7dHQ0Gdva2oplUztX\nzs/PF75v5c2bNwuXTe2V6lxJ42qMMe7fvz8Ze/v27cL3rcbOt1UaW9UzpX6q5mG67/HxcSz78OHD\nGE+q8ZHcvXt3MtYZs1VbXVxcLHztjuqZdnd3J2MnJyexbFqH0/o+xhj7+/uTsWrMpnFXjY107TQ2\nxshrZRrPh4eH8bos13w+j2Om825OfVtd9/nz55OxTz/9dOE6dZ6nyoHSmpDWgzGW184PHjyIZat3\nUWfdT+tJ53nT2BgjrzedPGaZ76nt7e2l3Te1VyeP6fyeqN5FVb2SNKar3wQpfnl5Gcum5+304dra\nWoxXczQ909nZWSxbrXlJ6sMf/ehHsex3v/vdhe+b2rp63s54T/b29iZjv/rVrxa+Ln2z2SyO8ypf\nX1T1Dnz58uVkLP1GqFTrWFL9Nk3remcfqKpz2n968uRJLFutnyle9WF65k5uVf3+TPftjJ3qeVM/\ndd6fVdlqbHXmUhrTVb3SHkKqU1W2et40tjp5W2VZOX+lunaqVzWX0ruhMy4///zzWLZat5Lbt29P\nxqrn7Ug5X5pnnb1pvh7VXlDSWTOqOfTs2bPJWJWPLOv9W70z0hyrflOl9qjaOeVBjx49Wvi+1bWr\nZ0rxzr501YdpParKpvZY5nlounbV/1Ufds5LO2XTfKl+X6U5/sknn8SyKcfq7H0tcx+5k0NV8c48\n7Kyl6fwore9j5L2Tam/s+vp6Mpbm6Bi938mprdI63HknsXxpb7Gae52+ffHixWTs8ePHsWyaP9V+\naGcfvvMbtCPt/1fnYVUfVudlSVoTOr9/0tgYIz9zWh8r1dqbnqmaC8s6dxgjt1f6fmqMfNZajel0\nxldJc7h6Ty3rG6hl7uckVZ07Z23VuEzn+NV9U9kvv/wylk39X+XeKa+r1v/OOp3G+7179yZjH7rG\n+svCAAAAAAAAAAAAALCifCwMAAAAAAAAAAAAACvKx8IAAAAAAAAAAAAAsKJ8LAwAAAAAAAAAAAAA\nK8rHwgAAAAAAAAAAAACwonwsDAAAAAAAAAAAAAArav2mK/Cb1tenq3N1dbVw2fPz81h2b29vMra1\ntRXLnp6eTsYuLi5i2XTtdN0xxnj16tVk7JNPPollU1tWz/vVV19NxnZ2dmLZyvX19WSs6v9kf38/\nxo+OjiZjVXu8fft2MnZ5eRnLbmxsLHzfakwnnbasninNw/S8lapsNdeS1JbVPEztUfXhycnJZOzu\n3bux7Js3b2I8SXWu5nDV/+mZ0zyrdMZOZ65UZdPzduZw1b+pbJrf8/k8Xpflms/nsX/S+plilWoc\nP3z4cDJWjeOzs7OF6jRGXm+Oj49j2bTmV21VPVPSeX9++umnMZ7WyOodt6w1smqr1NbV+3Nzc3My\nVr2LFl0Dx8ht2enfMeo+TtIzVf2b3s3VM3VyoFSvzn07610aV2PUz5RU86HTD1V+lXR+A6ffXtU6\n/J3vfGcyVq1ZqZ+q+6ZnSjE50M2az+et301JZ+1+9OjRZKyTy1d1Su+5g4ODhe/bWaeqsmnN7/xm\nqupVrdvp2lU+ka69zNw7tWXnvh3Ve6iK7+7uLnzv1IdVe6S2rPKnNE87+y3VfTu5SEfn90JH2vfq\n6qxLqf+rvav0+7nKRTu/Y3/xi19MxtLzvn//Pl6Xm5fWwc6eZvX+ffLkycLXTvOk886o5l+KV+c/\nSScX7V47tUfV/52zuPQ+WmZu3tn77DxvqldV505OUUk5xzL3RlKdq98jSVXnzprW0TkP75zxVnlQ\nZy8o/Zb9/PPPY9n0PUS1n5Pu+/Lly1g2qfoh5UHpvp02pq9zHtb93iT5/ve/Pxlb5rcZaW0+PDyM\nZdPc7Kz5nbO0qmwVT2tR9bv5/v37MZ6kM83quqk9qrPSzns9lV3mOleN9+3t7clYlT+lslVbpTHd\nObOuziXSfdN4HmOM169fT8aqcdc9t1z0up05Xs3hTm6e3g/VfdO4q3KgdN/Od2pVO6ex86tf/Woy\nls7+fpO/LAwAAAAAAAAAAAAAK8rHwgAAAAAAAAAAAACwonwsDAAAAAAAAAAAAAArysfCAAAAAAAA\nAAAAALCifCwMAAAAAAAAAAAAACvKx8IAAAAAAAAAAAAAsKJ8LAwAAAAAAAAAAAAAK2r9pivwTdja\n2orx8/PzydjR0VEse3p6uvB9U9nKvXv3Fi67vj7d7aktxhjjk08+mYwdHBzEslV7XF1dTcZSnceo\n+2lZLi8vJ2PpecYYY2NjY+GyVXxRVR/dlGps7ezsTMYePnwYy6Y+rHT6MJV98+bNwnWqbG5uTsaq\nNSnVeYwx1tbWFqrTGLkfUp3HGOPi4mIy9uTJk4XrVEntlcZkFX/16tXCdUrX/eqrrxa+Ln2z2Sy+\nyzrvwDQWq3mb4s+fP1/4vp3cq1KtCYs6Pj6O8e3t7cnY4eFhLNtZxyqp3qnOY+RxV9X56dOnk7FO\nPlGNjU5bJdU86+RmnWufnZ3Fsunajx8/jmWrMf/bpsrp0juyyoGq+ZDi1ZhN8SqfODk5mYxV/Z/a\nq1o70ri8f/9+LHt9fT0Ze/HiRSybxntqq5v6rchvr5cvX8Z453dA+m1bvS+qayeL5qGVtA59SLxz\n7zS3O+vn/v5+LJtylSr3Tqq26OxdpDrv7u4u7b6VdO1q7Ny9e3cyVu0DdN4LaS5VbVXN8WRZ+4Cd\n32Vj5Ly/6sPOuExlP/3001j23bt3k7Fq/qd+qPbqUw7UOSNIda76gJuXxnLnvdDZk6nWhTSuqrmb\n1t+byoOqvC+1ZfU+6eznVX2YcsrOeUh13/TMnT3Iatzd1HlYtY523u1pvnTGVjrDHaP/7p/SOQ+r\ncqjOnlvSncNp/FTzodMPqd5VLpNUe1/pfKkqm/Kgah1O0vMu85yVWnUe1pm76f3ZuW61L5n20qs5\nsKzf1Z11qnMeVrXVMqV67+3txbJp7a1ygnQGWPVvasvq7KCTA6V6VW1VnUt1pHFbnbWmflrmeVi6\nb9WH1XnJojrnjlVbVL+9Ot8bdeZwunbV/6k9qudNa8eDBw8WLlv1Q5rDf+kv/aXJ2Id+s+UvCwMA\nAAAAAAAAAADAivKxMAAAAAAAAAAAAACsKB8LAwAAAAAAAAAAAMCK8rEwAAAAAAAAAAAAAKwoHwsD\nAAAAAAAAAAAAwIrysTAAAAAAAAAAAAAArKj1m67Ahzo/P4/xnZ2dydjp6Wksu7W1NRnb2NhYuOwX\nX3wRy+7u7sZ4sr6+eNel9qiu+9VXXy1836oPk7W1tRhP/XDnzp1YtlOvVPbBgwex7OHh4WRsf39/\n4TpdXl7G+NXV1WQstWP32p0xu7e3F+NpTFf9Wz3zslxcXEzGquc9Pj6ejG1ubi5cdnt7O5a9vr6O\n8aTTzp37pnfDGGMcHR1Nxqr1P433FKvue+/evVj21atXk7HUzrPZLF6Xm5XWsWo8VeM8SWvzw4cP\nF77v06dPFy5bSXOzek+lNbBydnY2GavWz1R2jLx2p/dFVbZae9O1q3GXrl21R7pvNTaq9kjS+Ojk\nKWPkfKPqh847MvXTixcvYtnOM6e2rN6fN5UDpdz78ePHsWw1h5Oqnat6L6r6PdHJn1MfVs/bmcNp\nbKV2vHXLf0b6Js1ms9h3abydnJzEa6dx3vkdWOUTaS/n2bNnC9+3UtUrSe+Lzu/8al+r6sPOuyi1\nZefdW+VAaTx391SWpZOnVHMp9XE1PjrzIdXr4OAglk39UPV/ilfv9LQP8OTJk1g2rS1VO6f7Vrl3\nldelOVyVTTpz5Xvf+16Mp/ZIsTHyM7179y6WTWO2et67d+9OxtKYtA908+bz+cLjuZOPVGt3yqGq\nd3daN5aZB3X2glJ7VOt+KlvlMcs8l0j9UL0XqvPSJNW5uu/Lly8XLlv1U9Lpw+pdtqxrV7/nU65T\nzeF036rOnX5I79hPP/00lu2M2dRWnXPYMZa3z1iVTfetnin1YTV2Utnqvmn+V/Ps9u3bC5WVB92s\n+Xwex0xaP1+/fh2vnb6/qHKCzhr4l//yX56M/dEf/VEsm+rc2Tuu1uXOPmxnD7/6Td7Zk0lnT521\nt/oWKemch1WW9U1YVafO+UDVHqmPq7GVzmmq89903+pd1NmjSHXuPG/Vzp3zsGWO2bQedu5b7W91\nzsOSqmzqw878TvtEH3oe5tQMAAAAAAAAAAAAAFaUj4UBAAAAAAAAAAAAYEX5WBgAAAAAAAAAAAAA\nVpSPhQEAAAAAAAAAAABgRflYGAAAAAAAAAAAAABWlI+FAQAAAAAAAAAAAGBF+VgYAAAAAAAAAAAA\nAFbU+jd5s9lsNjY2Nibjl5eXk7Gtra147dPT04XrdX5+Phk7ODiIZff39ydj6VnHyM9blU3x9Dxj\n5LZ6+PBhLHv79u3JWNVWOzs7MZ6cnZ0tXPbt27cxvr4+PQ2urq5i2d3d3clYVec0dqrxnOZD1f+p\nH46Ojha+7xi53lX/d8Z0qlfVh521I0njaowx9vb2JmMnJyexbFo71tbWYtl3795Nxqp15/nz5zG+\nubkZ44uq+j+1dTVm0/ioxk5Vr0XvW0nPm+o0n88Xvid9s9ks9l1aI4+Pj+O1U79fXFzEsmktquZ8\nKlutgUla48ao16okrfk/+MEPFi775ZdfxrLb29sxXj1z0skJkmrspD6uyqb3RVXn1JZV7tVpq+qZ\nOjrvsTR2OmtHdd+kygc6OVBSrTtpna3ey1WOnK5dtWXqhzdv3sSynfUw3bfz26uTe1e/F9J7Kd1X\nDnTz0vxMsWqMV3sQi6rGcYp3frtW0v5DtX6meLruGLkffvazn8Wy1bWTKj9Ka0YnB+r0YXdPJUlz\npbpv6oeq7E1Z1jvuQ+JJynOqOj958mQy1hk7Ve7V+W1WzfFHjx4tfN80x6s+SmO6m9clnbOLVLZa\nK1+9ejUZS7nV+/fv43VZvlu3brXW/mRZY7mzRnb2Yas1I8WrnOHZs2eTsd/7vd+LZdO103XHqOd2\nZy8ovXM6v6s7/d/JRzpnWtW76nvf+95krKpzp48678GOZeZBi/6uGyOfPVf7V4vWaYw8dqqy1e/N\ntOZ11sPOeVi17nz++eeTsc7+VTp3HCM/b7UXlO6b5pG9oJt169athc+Mq7OUtGZ0zqmXmQMl1e+X\nzncsqa3SHn2lWh+r38adM/J07eq6qWzne6JqL/3OnTsL3zfNh+pc8sGDB5OxZX0f09X5TbDMZ0pj\np6pzKlvVufMdS+cMtzO2qvmf7t15pqofOvdNqvdOZx3u5Mgfwl8WBgAAAAAAAAAAAIAV5WNhAAAA\nAAAAAAAAAFhRPhYGAAAAAAAAAAAAgBXlY2EAAAAAAAAAAAAAWFE+FgYAAAAAAAAAAACAFeVjYQAA\nAAAAAAAAAABYUevf5M3m8/m4vLxcqOz5+XmMb21tTcaOjo4WLruzsxPLnp6eTsaqZ033rZ530XYc\nIz9Tdd1Ur/X1PJzS81bX7tSro3qmzn2vrq4WLpvaoxqzGxsbk7HqeSvp2pWDg4PJWFWv1JZVe6Rx\nWY27dN9O/15cXMR49UzJ3bt3F77v7u5ujKd+SmtlpbN2nJycxLKdMZ/6oZoLaWxVbZX6KbXVbDaL\n12W53r9/H/tuc3NzodgYeSw+ffo0lk3zp1prjo+PJ2PVGpjuW5V9/fr1ZKxqqzRHOnOvWks678fU\nzmPk9WRvby+WTe3VeY913tvVu6jq4yTVq3reqi0777mqj5NU76rOZ2dnk7Fl9mHH9vb2Uq7b+d02\nRp7jnWtX963ynEXv2+n/atylsp15lOosB7pZ79+/j33bGYv7+/uTsWofqKOzL9KZt52ySbVOLdp/\nHxJPz1Ttt6R4Z92upD6u7tv5zZyu3fn9ucx3YDUf0jzt7HtWexcdnXqlslUfprascq/O2Enr7Bi9\nZ0pzuOrDtHZ0xuza2lose319PRnr/Cbs7C/bB/p2m8/nMf/t/Ebt7Gl33oNp7e68y6rcrbNmpLWs\naqtUr047jpH7uPO+qXRymWWdaVXS2Kr6v3Pu2HkfVePj5cuXk7HOmUXnfVS1R2dvNFnmvuoyz6WW\ndR5W7cl3zsPSvlrKc8bI+z23b9+OZTvfQyRpbMiDblZ1HpbGYrUWPXjwYDJ2eHgYy6Z52/n9Uuns\nw6dn6u7JJGkdq9aLzru5+n2bcpE7d+7Esp13VfVbMXn79u3CZdPZUXVWkupctXOaZx9SflllO3uB\nHcv6JqjSOQ/r5E/379+P8TSXqrUylX38+HEs++LFi8lY54y3OqPt/H5K913WediH8peFAQAAAAAA\nAAAAAGBF+VgYAAAAAAAAAAAAAFaUj4UBAAAAAAAAAAAAYEX5WBgAAAAAAAAAAAAAVpSPhQEAAAAA\nAAAAAABgRflYGAAAAAAAAAAAAABWlI+FAQAAAAAAAAAAAGBFrX+TN5vP5+Pq6mqhsltbWzG+6HXH\nGOP4+Hgytrm5Gcum+OXl5cJ1qrx582Yydu/evaXdN7Vz1Uenp6cxfnJyMhnb2NhYuF4d5+fnMV49\nc5LGR9VWOzs7C983PVO3HdO119fzcpOeqSp7cXGRKxakOlftXPXToqp1p5oPi167asdlzoekWkvT\nfauyacw/fPgwlj04OJiMLWtsjJH7ML3P5vP5MqrDB6pyoLOzs8lYNbdSv1frRRqr1X0763ZaT6q1\nKM3rav3c3t6ejFV1fvHixULXHaNeP9PYqJ5p0etW8c6av6y8bIzeOz+V7bRVde3OXKos633TmcPL\n/N2W1rTqnb/MHCi1V9VHqb2q+969e3cydnR0FMsuKwf68ssvY9lqbC1aNr1Hr6+vF74ny5f2Aar1\npPP782c/+9lk7LPPPotlO9JaVa2PaV5XbZXWzypf7PxmrtaxFN/d3Y1l09ip2rLTD6nOndw7Pc8Y\neUxX78BUtvs7P+1PVmX39/cnY1U/vHr1KsYXVfVhauuqH9K1q1wk9WHn3Vrp7E9WYyu1R2dcVmWT\nak/95cuXk7Fnz57FsqnOVR+metkH+u3W2ZPp7PGnfL66b2f/t3O21PkNmnTOtNJ77EOu3TkPq/KG\nRVW/I1O9Om1ZvUOrvDDpnNN2ynbeZdXzpvdR2icYo/f766ak92S13qV5Ws2jX/ziFzH+3e9+N8YX\n1cmhvvrqq1g27Y9Ua1q6dpUHdfLVd+/eLVyWmzObzWK/pxx2mTlQJ59Ie8vL+k5pjOX93quum/Za\nqzOrah1L175z504sm9qreqbUT53f8x2d39xVnX/5y18uVKcxemea1XyozlMXvW81LtO4q8ZOauu9\nvb1YNu33VPft5Gad89DXr1/HeKf/k8PDwxhPY6faV0tz7fHjxwvXq1rDk6p/05h9+vTpZOxD+6D8\ny8Kz2WxzNpv9n7PZ7I9ms9m/nM1m//Wv//m/NZvN/vlsNvvJbDb7H2ez2e0PuiMAwG8BORAA8LGS\nBwEAHyM5EADwMZIDAcDHo/xYeIzxdozxN+fz+b8zxvirY4z/YDab/Y0xxt8bY/z38/n8h2OM4zHG\n31leNQEAvnFyIADgYyUPAgA+RnIgAOBjJAcCgI9E+bHw/E/92d833vj1/8zHGH9zjPE//fqf/8EY\n428tpYYAADdADgQAfKzkQQDAx0gOBAB8jORAAPDx+JC/LDxms9nabDb7F2OMgzHGPxlj/MkY4/V8\nPr/69b/y8zHGk4myf3c2m/3hbDb7w1evXn0ddQYA+EZ8XTnQ0dHRN1NhAICvyaJ50G/mQL/85S+/\nuQoDAHwNvq69IHkQAPDbRA4EAB+HD/pYeD6fX8/n8786xvjeGOOvjzH+yl/0r02U/fvz+fz35/P5\n73/yySeL1xQA4Bv2deVA+/v7y6wmAMDXbtE86DdzoO985zvLriYAwNfq69oLkgcBAL9N5EAA8HH4\noI+F/8x8Pn89xvjfxxh/Y4xxfzabrf869L0xxvOvt2oAAN8OciAA4GMlDwIAPkZyIADgYyQHAoDV\ntl79C7PZ7MEY43I+n7+ezWZbY4x/f4zx98YY/2yM8R+NMf7hGONvjzH+cXWtW7duja2trcn4+fn5\nZOzq6moyNsYYl5eXC5ddXy+bYdLFxcXC993Y2Fj4vrdv356Mff7557Fs+uuGa2trsWxq50rq367U\nh8u8b+rjNNbHGOPg4GDh+6bnrcZdqlc1Jo+OjmJ8Z2dn4Xptbm7G+KJlj4+PF77uMsfOmzdvJmNV\nP3TmYerD6i+fnp6exvi7d+8mY9XaktbSSqpXNa7SfKjGeypbvVeqtkzSXOq8z/g3fZ050Nra2tje\n3p6Mn52dTcaq9bMjXbu678nJyWSsmtN7e3sLl01zr1rzO++pzvzqXLvT/1XZFK/eRZ339uHh4UJ1\nGiOPnU4fVc9Trdtpflf5RKp3Whuq+3berVXZND6q5+2Mu0WvO0ZeH6q8vZLqXV27M8dTrlKN6evr\n64WuO0Yes2lMjpHHdNX/qS3T89669f/rPyPNr31deVC1D9SRxkwnl085zhi9vas0f6rfeSmPqfYX\nnjz5C/9bQscYY7x8+TKW7eQp1Tuh8z5Jqrbs9GHqh0rnt3xn/ym1cxobY9Q5UPpvkavm0u7u7mTs\n1atXC5et+jA9U7Uv0hmzndxsWflCNZ47Y6sqW63TSerD6jdBqlc1R9PzpjE5xhjPnj2L8SSNrVSn\n2Wy28D0/Zl/nXtBsNlv4d2o1HtN1q/U3le3saVZ1XvRscIw8x372s5/Fsmltr3KozppR9UNyU2da\nHdXY6eTBSec3d/ddldbgzjulyoNSW6Zzp6psZz+n6v/OmE737eT91R7EvXv3Yjw9c5VTdvKgVLYa\n0+kMr/pdmHSetzP/U9n5/C/847cE32QOlMbqMr+vSTq/I6vfemnfsrNnls5Zxhjj8ePHk7Hq3KE6\n10+qa6exkfasx8i/Yas6d/aCFt2XHiOfS3TGTjUXOnOlc6ZVSc9ctcdNnaWmtqzOpdMzVe3Y+QYy\n9WG17lRjOtW72meq1q1Fy3b2t6o1K6n6MI2Pzjzq/Mb98/t/wL/zeIzxB7PZbG386V8i/kfz+fx/\nm81mPxpj/MPZbPbfjDH+7zHGP/igOwIA/HaQAwEAHyt5EADwMZIDAQAfIzkQAHwkyo+F5/P5/zPG\n+Gt/wT//6Rjjry+jUgAAN00OBAB8rORBAMDHSA4EAHyM5EAA8PHw38cJAAAAAAAAAAAAACvKx8IA\nAAAAAAAAAAAAsKJ8LAwAAAAAAAAAAAAAK8rHwgAAAAAAAAAAAACwonwsDAAAAAAAAAAAAAArav2b\nvNn19fU4PT2djK+vT1cnxSpbW1sxfn5+vvC1U72q+15eXi503TFGbMf9/f1YNrm+vo7xjY2NyVjV\njtUzpfaqrp3qVUn9sMyym5ubC5e9urqajHXmSrcPUz8cHBzEsqk90niv7O3txfjx8fFkrJrDi65n\nY4yxu7s7GTs5OYllUztfXFzEsvfu3ZuMVfP/wYMHMX54eBjji+rM746q/9M8TLEqflPPy3JdXV3F\n9WZnZ2cyVq2BqWw1jpPqPZXWuarOndwrXTu1RVdqj2rOd9751breyZ9TP1Rr0dnZWYwnqV5VHy4r\nB+qs22PkfOPFixex7Pb29mSseqaqXklnTKex03l/VmtDZ+1I7Vypfl91csI3b95Mxqp52Fnz0tpS\n3beTm6ffT50cKI2r+Xy+8HXpu76+jr9vOvsASWfeVtJY7fyWq9bedO2HDx/Gsp13flLtiVTx1P9V\neyxr7CxzDzG1def9WUn90NnXGmN5/VCNy04uktaHqj06901lu7lokvKFbq5xdHS08LXTOly1Zed3\nbmrLlJeNMcba2tpkrLOnWrVV53m5We/fv49jI82xzhr65MmTGE/79NV4S+M1rQldKQ+q8r5O/pXm\nblV2Wb9vxshnC53f1Z19lU7/L2td75bt5LLV74JlPXNnDhhlkp8AACAASURBVFfvwTR2qr3Pztly\nZy6lstV4X2YelMZWNS47e0GprdO6UpWt5n9nbbl9+/ZkLLXzbDZb+J70XV5exjPjtJderb1pjlRr\nUeecJp2R//SnP41ll/XtxuPHj2PZZe19VX1UPW/6XVV9q5CuXZXtnC2ltuzk7VUfLus7tu43Qems\npTo77MzDpKpz576Lnu8vU3V2nHLCqi2qb4JSe1TrTuf3VXqmzhyu8ueqrZNlfqfR5S8LAwAAAAAA\nAAAAAMCK8rEwAAAAAAAAAAAAAKwoHwsDAAAAAAAAAAAAwIrysTAAAAAAAAAAAAAArCgfCwMAAAAA\nAAAAAADAivKxMAAAAAAAAAAAAACsKB8LAwAAAAAAAAAAAMCKWv8mbzabzcb6+vQtz8/PJ2MbGxsL\n3/fy8jLGU51OT09j2a2trYXqNMYYV1dXC9VpjDH29vYmY1Wd031vUur/FKviOzs7sWxqj6p/q7G1\n6H2r/l+W6r6dei1rroyR63V8fBzLpnpV4y6pxt3BwcFkrFrv0rhbW1vLFQuur69j/OLiIsY7fdyZ\nS6m9On24TJ01vOoHvp3W19djv1drVXXtRa+7ubk5GUvr1Bhj7O7u5ooFaRxX62eq8+vXrxeu07Le\n6V9HPElrRvXeTv1QremdPCa1deedX0llq/um+VuV79S5eo9V8yVJ9areNendW9Wps94lnTlcqfKr\nTh+ntuzkhFUfprW0cv/+/clYtQ6nuVTVOc2zNO46+TF9a2trY39/fzKe8o3qXZTm3rNnz2LZVKcv\nvvhi4bKVk5OTyVj1vJ3fPcvab6vuWz1Tp16dHKgjrUVVPpHW7eo91sm9Uz+kMVmVHWN579/O2KrK\nprHV2UPutFUnB17mb42Oqj06eWxnPUz1qtaO1JbVXHr06NFk7OXLl7Fseqbt7e3J2K1b/k7MTZvN\nZnG8VnuAi+rMg04eVM2h9LzVmtF5l6X2qNb9Tv911pRK55mSTu7W0c0pk847o8q/0rWXmY+m9qj6\nv5OPpPtWbdU50+jUeZl7RZ1x2cntOu3x8OHDyVhnfnd+u1dr6bt37xaqEzdrY2NjPHjwYDJ+eHg4\nGat+J6Tx9vTp01g27UumOo2R17Fqn7VTtjN/OrlG6oeqndNvlDHGePv27WSsen+mtWpZ+cIYvTPN\nVOdOPlm1VWdPvHMuUUltWV03zZcq1+ich6WxVc3h1P+dOdo5/60s85x2Wddd5ndM6d3RyZ+qOqf4\n19EHdowAAAAAAAAAAAAAYEX5WBgAAAAAAAAAAAAAVpSPhQEAAAAAAAAAAABgRflYGAAAAAAAAAAA\nAABWlI+FAQAAAAAAAAAAAGBF+VgYAAAAAAAAAAAAAFbU+jd5s9lsNtbXp295fHw8GXv48OHC993a\n2orx09PTha99dHS08H1TW7x69WrhOlXSfc/Pzxe+7uXlZSue2mtnZyeWTX24sbERyyaprcbI7VXd\nN43pzpjsqOp8cnIS46kPNzc3F6pTdd0x6n5KUh9eXV0tfN+b6sPK9fX1ZGxtba117Tdv3kzGbt++\nHcumtt7d3Y1lUx925nBnXFU6a22aS53rslzv378fFxcXk/HDw8PJ2Keffhqvndabahyn3Kt6Jzx9\n+nQy9vjx41i2M47TelGtvXt7e0u5b+d9MUZuj6rswcHBZKyTP1fv7TR2qrJpTKfrjpH7qfO8VR9W\n8aTzPunkQNV9z87OFi6b+jiNyVWV2quTX1e/gTpjK8216rqpXlWd03078yytDe/fv1/4uvS9f/8+\nvp/THKjGYmc/ppM/de6bdHL5av509kXSXk513eqZUvnOb7lqLUr37fRh577Vfkvqh6rOnT6spLHX\nactHjx7FeLXHmKS27rRH1Ycdnf3HZe5tdKSxs7+/H8t21suvvvpqMpb2zMbIc7zqh/TuqKQ+THtx\n8/l84Xvy9ZjP53G9+tnPfjYZ++yzzxa+b2eNrN5laSxXZdN7oZrX6Zk6vyM6Zbs6vyPT2Hny5Eks\nm9q62odPOXT13k/x6nnTfatxl8ZO9Y7szKVOTnH37t0Y79SrI7VX5z23TJ1+6PzWqeZDaq9qHnbO\n+NOeXXU+mOZaNSY7a211tsi303w+X/gcpzpb6Pw27uyHpzO86jwsnQ2mWKWa853cq7OuV7+r7ty5\ns/C1U3t19iCq/u/8rk7nklX/p3lUrdupH6q26uwzdr4J6vRDVefOXEv37Vy3kq7dOcOr6lxdO7V1\nlZu/ePFiMlatpdX58aKqPCbN4apseu9UOt+xfQh/WRgAAAAAAAAAAAAAVpSPhQEAAAAAAAAAAABg\nRflYGAAAAAAAAAAAAABWlI+FAQAAAAAAAAAAAGBF+VgYAAAAAAAAAAAAAFaUj4UBAAAAAAAAAAAA\nYEX5WBgAAAAAAAAAAAAAVtT6N3mz+Xw+rq6uJuObm5uTsfX1XNXz8/PJWLpnZWdnJ8bTtTc2Nha+\n73e/+90YPzo6moy9e/culj08PJyMbW1txbLpmap2Tn00xhjHx8cLl031Tm01xhi/8zu/Mxn7kz/5\nk1g2jY/Ly8tY9uDgIMYXvW8l9WHVztWYTuWrsXV6ejoZS2vDGL05vixVW34bXV9ft8rfvn17Mla1\nR1rjq/U/zbXOOlyNq7S27O/vL+2+yyrLclU5ULVGJmmcX1xcxLLV/Eq2t7cXLpvqXM3blMfs7e3F\nss+fP5+MVX3QaatqbqYcKL0fxxjj008/Xei6Y4zxu7/7u5OxH//4x7FsykWqcZfqVfVDet6qrVIf\ndudKes9VeVsnZ0j1rurcGdPpvtV477zzO+toKlv1f6Vz7WX9vqrW0k7/f/nll5Ox6t3Que9vY37N\nGLPZLPZ7Gqud/KgaL53frum3fKfOVdn0+6OaW6lsp85dJycnk7FqzyTt5bx8+TKW/eyzzyZjz549\ni2VTe3VykarskydPJmOdPbPqvpVl/R6vdN6BKV7t5S1zj2HR+3by1E7uVdWrsw9U9UPnmVJuVu0/\nVnvMSScHevPmzWQs9dF8Pl/4nnw9ZrNZnCed/cM0T6p3+039jkhlO3Wu3NTviM7vuTTvxxjj0aNH\nk7FOHvSTn/wklt3d3Z2MVWt3FU9SHpTyyTF676pKKp/aaowxXr16tXDZ9MydOVpJY7oa753+7+wF\npT7q5n2pXp0cu+rD1P/LzL+/+OKLyVjnzLrqw2o95Nvp/fv3rXwjSWWrfP7s7GzhOqVrd/YCOqpc\nMp2lra2txbLpm4Fq//ft27cxnsZGqvMY+XzoxYsXsewPf/jDydi//tf/OpZNZ4/VOzA9UzU20n2r\nc4fUx2kujLHcb4I67ZHaeplnuCm+rBxnjN5ZWurj7l5QUq2H1TqddPq4821BWh8630rcNH9ZGAAA\nAAAAAAAAAABWlI+FAQAAAAAAAAAAAGBF+VgYAAAAAAAAAAAAAFaUj4UBAAAAAAAAAAAAYEX5WBgA\nAAAAAAAAAAAAVpSPhQEAAAAAAAAAAABgRa1/kzebzWZjfX36lvv7+5Oxq6ureO2tra3J2NHRUV25\nBaXnqZycnEzG3rx5E8vevn17MnZ2dhbL7u7uTsYODg5i2fS8Ozs7sWwltUeqc1U2jY0xxvjiiy8m\nY5eXl7Fsao/OmK3G1enp6ULXreLV86Y5OsYY5+fnMZ5U9U6W1R7pumPUfbwsGxsbk7GqDzuqa29v\nby/lvlU/3FR7pPnQuW81jy4uLha673w+X7hOLN/e3t5krBpPnfUzSXNrjPyuqsZxWj+rPKZz39TO\nh4eHsWzKc6r39vHxcYyn9qiunXK3qg+fPn26cNnUHp0caHNzM5b96U9/Ohl78OBBLJueqRo71bWT\nqg9Te1T9UL0jk1Sv9K4ZI69L1fOmtu7kbVUfpjqvra3FstfX1zGecqDXr1/Hskk1lzo5UOrjah6m\n+dDJgarxvOiYff/+/cJ1om8+n8ex/PDhw8lY5/dltX52ynb2Pjr7U2lf5OXLl7FsZ88sla3W7bRX\nM0Ze5548eRLLpme+e/duLJueuVrHHj16tNB1x8jtVe23pOet9sw6a3M13lMfVvft7Kmka1fjsvN7\norO2pOet3r1JJx+s+qDK69L46OzlVGU7901tnfbbx8hrS9WWaX2oxl26b5W3c7Pm83lcr5Z1HlaV\n7ay/nfsmnRypmkNpLavum96x1fvm5z//eYynuV3lMq9evZqMVf3Q2UfqnKV09i9TP3WuW+WqVY7V\nka7d+R3U0ckLOmflnfyqo7pvNT7Sb9nqrD3du1qXbuo8LO0FVftmSbV2pLmS9tztBd2s2WwWx2pn\nrz2N82We23d+s6WcvVPnzm+f6swqnaVVqv3w1B7V2EjneFUfvXjxYjJWvcfu3LkzGfvlL38Zy6Z6\nVXlMqnO1X1OdeSRVe3Rygvv370/GqvmwrN+/nfdn5wyvGrOdM83Od2xVe3TO2lO9qnWpc6aZzvCq\ntnz8+PFkrLMnV903rQ/pW4oP/SbIXxYGAAAAAAAAAAAAgBXlY2EAAAAAAAAAAAAAWFE+FgYAAAAA\nAAAAAACAFeVjYQAAAAAAAAAAAABYUT4WBgAAAAAAAAAAAIAV5WNhAAAAAAAAAAAAAFhRPhYGAAAA\nAAAAAAAAgBW1/k3ebD6fj6urq8n4+vp0dVJsjDFOT08nY7u7u7Hs0dHRZGxjYyOWTfHLy8tYNrVF\n5fz8/2PvbmLzytL8sJ9XJKtIhWSLVEuaktzV1eOx4Y8ZzywG3niVBMgiCRIvEiNAFgZiwNvs8rHP\nJqtkbTgLb4I4CGA4yCJI4GTsjTHIGMYAsZO423ZXV6vUoixSRdIiSyT1ZuGaZJD0/T/q+9RbrH71\n+226VUfn3nPP53PPufXWxawyjZHLdXp6GvM+efIkFyxIZa5UdXl5eTmZdnh4GPOmvlPp5E19uqqr\nnZ2d2fftXLfTZzuq9k86c0dHZxxWUt40Fsaoy9Xx+eefT6Y9evQo5n3z5s1kWueZqudNfb7qO6lc\n1RjutMP29vas6y4Wi9n3ZPU6MdDJycnsvKnPVHnT+KnGQGc9SXmrdSzN+dVcs7e3N5lW1VUlzesP\nHz6MedPcW9VHKnfVRqnfVT766KPZ1z04OJh931TPDx48iHlX1WfH6MWESdX+nWt3dNbA6l1lrqqu\nXrx4EdNTG6e5Y4wxNjY2Zt83qeo5lSuVaYwxbm5uJtPOz89j3s68k6T46M4d/470t1lq96pPpHm9\nGted94C5ZepKc2A15jt7SKvau6rSO/seVRt26iPtIVbtn+beao1Le1tVG6Vn6u4DpWfu7KlW9dGJ\nJ9IzVXvIqVxVv0trVfUukuq5M0a78WJK77yLrnLu6MzTqa7T3DDGGLu7u5Npnf6e6mK5XMbrcvs6\ne5qp36wyhuq8R3bydvbNOvNkZ53rrClV3s5allRz5NOnTyfTqnZIcVB1RtPdd5tSnR2u8iylI9XH\n3bt3Y97Xr1/Puu4YvRi604bpvqsa32PUz5TGQ2ccds6Wqvum9q/6TicOSmtL51uK1K+ch92u5XI5\nO97onC2ld64qb6XzTtY5w1uVKuZLZU7vNmP0YrPO2tu5b9WGP/vZz2aVaYz8TFUMlM7DOv25asO0\n//9tVY3/dG7ReQfq7AVW810qc9X+qT6qsdL5LqE6w03lrsZD2jur+vSqznir87B79+5NplXPm/pW\n+rbqXfeCnJoBAAAAAAAAAAAAwJrysTAAAAAAAAAAAAAArCkfCwMAAAAAAAAAAADAmvKxMAAAAAAA\nAAAAAACsKR8LAwAAAAAAAAAAAMCa8rEwAAAAAAAAAAAAAKypzW/yZtfX1+P4+Hgy/eHDhzFvcnZ2\nNpm2t7dXF27CxcXF7LyVzc3p6k9plaOjo5i+tbU1mfb9738/5u3UR9WGqVxV3u3t7cm0qsyddtjZ\n2ZlMq8qc0qv7pvTOfQ8PD2PeNM4qqX3H6PWtTjskVTt0rp3q4+rqanbeNMeOMcYHH3wwmZbqcYz6\neR89ejSZ9vz585i3Y2NjYzLt/Px89nWr/p7aoVOXVd7Xr19PpnXWDlbr7du3cZ47ODiYTKvG3tzr\njtFbT5Jqzr+8vJxM293djXnTGPn8889j3vS83/ve92LeVObO2lup4rpOG6a8VTsk1X1PTk4m06q+\n03ne9E5QPe+LFy9ielLN6536SM9cxVadWLRjVTFQ1UbpmdL4fhfpXaSKzTpSn67um+q60w6dvlOt\nWSk2S/15uVzOLhN9y+Uy9qn9/f2V3LezD1CtJ2ler+6b3rmrvKmunj59GvOm+aLaB0jlquaLSmqH\n09PTmPfu3buz79tpw84eUupbt7VnUu2Zdu5b1cdt7QOlvlXFbZ2+k/JWsUjKW43/zt5F9UwpvXvt\nJD1TNXckq9xDTPtTVV2lGCjVxWKxiNfll1tnbzGt36t8f0njpCpzylvt4aa8aT97jDynrPLsoJrL\nUn113ueq+KoTu6V5v6rLzjtoylvFQZ3xUPXpVF9VXNjZV0kxR1WXqb46/b2KCdLzdvZNqzaqpPez\nzhju7Kv87Gc/m503nTuNkc8WqzKnflnNw3PfocRBt+vt27dxvknzSTU20x5w50yj0vmOpfOtUtov\nrc7Dkqqu0tn7KnX2+CupT967d2/2dTvnYdV+eEcVTyQ3NzcxPY3T2+o71b7KbX3Xk1T9OZ07PXv2\nLOZN8VV3L7DzbtZphzRvvXr1avZ1K+l5UxuNkftlZ/ynPaY7d97tN4P9sjAAAAAAAAAAAAAArCkf\nCwMAAAAAAAAAAADAmvKxMAAAAAAAAAAAAACsKR8LAwAAAAAAAAAAAMCa8rEwAAAAAAAAAAAAAKwp\nHwsDAAAAAAAAAAAAwJrysTAAAAAAAAAAAAAArKnNb/Rmm5vj8PBwVt6jo6OYnq57dXVVlmvK9fX1\n7LzVfbe2tibTvvjii5j35uZmMq0qc1Llvby8nH3tTn28fv16dt7T09OY9+7du5NpVZl3dnYm0/b2\n9mLei4uLybSqnvf392P6XKlMY4zx4Ycfzr52Gitj5DZc5RjujJfU/lWZU79MdVFd+zvf+U7Mm2xs\nbMT0qg2T7e3tmJ7mtErKm9pojDHOzs4m06r1Ko2X4+PjmLfTZ9P8kPrOYrGI12W1tra2xoMHDybT\nU7umfjrGGA8fPpxMq+aipJP35OQkpqc54fz8POZNdVWtn52xV62RnbypXNWakOq6mtdTG1f33d3d\nnUw7ODiIeX/yk59MpqX+PEae16uxktqhaqNqPUn5O2Op03c6MVB136qNk6qd5kpz7Bi9/v7ixYuY\nnvpHFU+k+qj6Too3qveFTuyVVPFi6nedd810XzHQ7VosFrPf9ap3+c57cbpv1Y/THNl5v6zm3s46\nlvYnqneXdO3OO2JX6h9VO6RYtMqb+nMnXqjqMq0nVfun/t7Z9xojr2NVuVJdVu2Q2r8qc9LZu6zW\n/DTWOuvn3L32d7lvlZ5i9877UyW1/6NHj2Le58+fT6Z15rQqnlzV+pDmMzHQ7evEQdX63FkXkl/G\nd9/O2UEVb3bir86ZVnXtNKd05ptKmnOq2Dz16arfpfRO7FbtT1TlSveu1pTOM3Wk94LOfk31vOna\nnfOw6hw29cvOOBujdz64qjjp/v37Mf3ly5eTaR988EHM++bNm8m0Tp/t7Oemel4ul7PLRN/m5mbc\nP057gJ9//nm89uPHj2fnXdXc2/meqNoPTWcpHdU8lc7pqnm7OuNLZ0vPnj2LeVMsUp1Ldt73Ut70\nPGP09oK++93v5oIFnb326ruOdO0vv/wy5k39o7pv0tnf6IzhTgzUUZ3RpX5Z9Y3Ovlol1UfVDqnv\n3Lt3L+Z99epVLljQmTs64zDNaV/HnrxfFgYAAAAAAAAAAACANeVjYQAAAAAAAAAAAABYUz4WBgAA\nAAAAAAAAAIA15WNhAAAAAAAAAAAAAFhTPhYGAAAAAAAAAAAAgDXlY2EAAAAAAAAAAAAAWFOb3+TN\nFovF2Nycd8u9vb2YfnV1NZl2cXEx657vkndra2v2tVdlZ2cnpqe6SmljjPHy5cvJtEePHsW8VV2l\nvnFzcxPz3r17d/Z90zNXdXl8fDyZ9vDhw5g3PW+nv3fquXreajzMHd9j1H1vruqZrq+vJ9OqMqX0\ndN0xxrh///5k2tHRUcybbGxsxPTLy8vJtO3t7dl5x+i1YWqn09PT2dfd39+P6Z35MJW56ndJVc9J\nKvNyuZx9Xfrevn07u20763qnP3VU80lHml8762fl/Px8Mu3g4CDmrcqV1tdq7X3w4EFMT1IcU8UT\nJycns/OmGKmKJVJ9VP3utuL2TmzWUfW7pBoraRxWfTaVq4qBOm2Y5sMqXrh3797s+1ZSv+3MWVUs\nmmLGTj2fnZ3F9FSu3d3dmLcTI3N7qn2g1CeqsZl05rHO+0d13zRHVuO2M69X105SmTvvPZ37jrG6\nvY1qXX7+/Plk2ieffBLzpjZcZZzSuXbVdzrvxat616/a8PDwcDKts4ecYusxcn1U913V3mWVt9N3\nqmunmKHqd6lcnfipE7dXMVDK24mBUoxrH+jbL/W5zhrbmcuqvpx09uE7a3dn/7fSuW/nXKrzXl2t\ng2ne6ORNZ3Rj5Prq9Pcq79x3kXexymvPve8q469032oPelVrd/W8KXZ/8uRJzNt5t+v06c6+SvXO\n+MEHH8zO21kfUl1W806nz/Ltldq9OmtJY6/z/tKZizrrZyXNJ9V9O+M2qeqq+71Bktq46jvpTKvS\nOZdKZ3hVn62+kUrSe2ZnD2KMfLZQfauS2qkqV6rrajykvJ2+0fmepLOPUPW7Z8+eTaZ9/PHHMe+q\nxkqlmhtWdZZa7cl04vrUTp091/TNwrt6518WXiwWG4vF4u8vFov/4as//2CxWPzuYrH44WKx+OuL\nxWI6wgQA+CUlBgIA3kdiIADgfSQGAgDeV+IgAFh/7/yx8BjjPxxj/B9/6M//+Rjjv1gul39sjHEy\nxvhLX2fBAAC+JcRAAMD7SAwEALyPxEAAwPtKHAQAa+6dPhZeLBZ/ZIzxb4wx/upXf16MMf6VMcZ/\n99Vf+WtjjD+/igICANwWMRAA8D4SAwEA7yMxEADwvhIHAcD74V1/Wfi/HGP8R2OMt1/9+f4Y49Vy\nubz+6s8/HWM8+XkZF4vFX14sFr+3WCx+7+XLl63CAgB8w8RAAMD76GuJgf7ZP/tnqy8pAMDXZ3YM\nNIY4CAD4pWYvCADeA+XHwovF4t8cYxwtl8u/94f/8c/5q8ufl3+5XP6V5XL528vl8rfv378/s5gA\nAN8sMRAA8D76OmOg7373uyspIwDA160bA40hDgIAfjnZCwKA98fmO/ydPzfG+LcWi8W/PsbYHmPs\nj3/xbxXdWywWm1/9m0R/ZIzx+eqKCQDwjRMDAQDvIzEQAPA+EgMBAO8rcRAAvCfKXxZeLpf/6XK5\n/CPL5fKTMca/N8b4X5bL5b8/xvhfxxj/zld/7S+OMf7mykoJAPANEwMBAO8jMRAA8D4SAwEA7ytx\nEAC8P97ll4Wn/MdjjP9msVj8Z2OMvz/G+K+qDG/fvh2Xl5eT6Xt7e43irMbW1lZM39ycrsKUNsYY\nP/3pTyfTvvOd78S8R0dHMT25vr6enffRo0eTaRcXFzHvzs5OTH/58uWsMlVev349O2/V/in97Ows\n5k39/fPP87+U9/jx48m0qt9tbGzE9KQao+mZq7ypX/7oRz+KeR8+fDiZdnV1FfNW6UlnLKUxXLVR\n6ndpjq3ynpycxLy7u7sx/cWLF5Np1fh/8+bNZFpVz6nPV/NSqo8q723Nw9UY5xvxC8dAy+Uytvv2\n9vZkWtUX03Wr/pLmjFX20/Pz88m0aq559uzZ7PumZ6qeN7VRNX+mvNW9q3k9PXP1TJ05MN13lTHQ\nxx9/HNOTzvxZ1eXBwcFkWtWGqT6qvpXW16rMqVxVDJyuXeVN62cVL6R+2Ynbq3quYrNXr15Npt27\nd292uao4tXrmuao+m8rVWXeqdpi73i0WP++/mMhMs/aB5o7dzlpU6cwnq7rv4eFhzHt8fDz7vqen\np5Np1fOmcj1//jzm3d/fj+kpfycWXWXetG5X+1ppT62qyydPnkym/bK+I6b+8cMf/jDmvXv37mRa\nFU+k8VD12c44TDFQiiXHyOO0E/NXcXtVl6k+qn3AdO1q/p973TF6sdcXX3wxO29qp/R+XPllHf+/\nhH7hGGiMf7EXlPpGJ57vxDKrGmPVdTtzRrp2NQ46z5vioGpNqNaUp0+fzirTGL39rbnXrVTzYKqP\nKg5KMVQ17yedPYhK1S9TfaSz4zFWN3dUOnvQabxU70EpXqnaMNVzNYY7e/LVM3Xum3T6dPUuc3Nz\nM5lWjf/OM3XOTfjafO17QamvdvbDq7OlTtzdmXvnng2Okd8jqzKl563m7VTP6XuAMep26Kwnab7p\n7C1XVtX+VV3+4Ac/mH3fpFovqrrqvOOk9aQao6lvVXlTPFH1u5RerYGpjauzo3SWUpU57TNVe0GV\nNNYePHgw+7pVv+ucpSeduWOV716rjnN+od2k5XL5O2OM3/nq//+TMcaf/fqLBADw7SIGAgDeR2Ig\nAOB9JAYCAN5X4iAAWG93brsAAAAAAAAAAAAAAMBq+FgYAAAAAAAAAAAAANaUj4UBAAAAAAAAAAAA\nYE35WBgAAAAAAAAAAAAA1pSPhQEAAAAAAAAAAABgTW1+kzdbLBZjc3P6lp9++ulk2sOHD+O1j4+P\nZ5drZ2dnMu3y8jLmvbi4mEzb2tqKeb/zne9Mpl1dXcW8H374YUxPOmU+OTmZTKvKfH19HdPv3r07\nmXZ2dhbzpvbf29uLeZPqmVLfqZ439a00TlbpzZs3MT097xi5rqu+lVTjP1079fcx8jNV/S7lre67\nvb09mVb1u2RjYyOmd9rh5uZm9r2rvpVU/S6p2qEz1lK/rMZ/Z82qrs2303K5jGM7ra8PHjyI167m\nqqQzBqrxlezu7k6mvXjxIuY9ODiYTKuep7p2ktbtRu5KrQAAIABJREFUzrw9Rl4/O7FoZ/5cpaOj\no8m0ap1KbVw9b8rbjb06MWFqw1XGsSkWqcqc8p6fn8e8nX65v78/mXZ6ehrzpjJX46yKgTpjOKWn\n5x0jt9Pr169j3iTVVaUTa1b9rjOH8+31/PnzybRPPvlkdt6qLx4eHk6mVfNJiuU7963eEVLeas5P\n6VVMl8Zmdd+XL1/G9DTPVfWRytVZ1zt5q3m703c6903t1Im9qvxV30r18ejRo5i30y9Tmavxn+KY\nqi5TmVf5np/ilKqNOnsqnbmlihdTG6d3jUrVhmnvupJi5GqcdfbUuF3L5TL29TROqrk95a3mlDTG\nqjm0sxeUYpkU143R24ft1FUnhuqcD3Su3V3bk1XFhdW8f1v7W9WeTKrrqj7Sfm613qQ27Iz/zrpf\n7X109s1WVeaudO+qXCnmrPp7qq/OudMHH3wQ0zt70GmsVG2U5o503cViEa/Lai0Wi9g+6TzsV3/1\nV+O10/5g1Z9SejVvp/tWe6kpPdXFGDl+6nyLUunEfK9evYrp6Xxwle+RHal/VPWcytw5/6neF5LO\n9zNj9PYZUn2kvlHlrcbhbX1fce/evcm0zjlclbcTt1U61079o2rDdO1O+3bmjipvZ83qxE/vwi8L\nAwAAAAAAAAAAAMCa8rEwAAAAAAAAAAAAAKwpHwsDAAAAAAAAAAAAwJrysTAAAAAAAAAAAAAArCkf\nCwMAAAAAAAAAAADAmvKxMAAAAAAAAAAAAACsKR8LAwAAAAAAAAAAAMCa2rztAvxhOzs7k2nX19ez\n815cXMS8W1tbk2mbm7mKUnpV5ocPH06m/fCHP4x5b25uZt83ubq6mp1e5e3ce2NjY3bey8vLmDdd\nO/WNKj31ySq9qsvUp588eRLzpmtXz1uNh7n3HWOMg4OD2ddO9VGVOY2Xzviv8nbaoZM3qebK6pke\nPXo0mXZ2dhbzvn79OqbPtbu7G9NfvHgxmVaN4TS3nJ6exrzp2kdHRzHv9vb2ZNrJyUnMy+25c+dO\nbPdqrZqr6sdpPqniib29vdl5033TdcfIddWJgW5r3q7SqxgoOT8/j+nVHJmkvlXV5ccffzyZVo2F\n9ExVLJH6R9V3Oteu6iP1n6qNqjZOOvNO53k7UqzSiYGqOOXevXuzr12N/zTnVeVKdV3VR7r23bt3\nZ+etxlIqc9Wf5757L5fLeF1uV1pPOjF1pRMzHB4eTqZV47aT9/j4eDKtmnurd70ktUNn7q2u3Xkf\n78Rmnf3HStqvqdootX/qV2P01ubqeVO5qzXwtsZw57ppjayeN73LVzqxV8pbzTvVu9mq5vAq79zr\njjHGj3/848m0tK81Rq/fdZ73gw8+mExL+0BioNtX7QWleaOzT9uZ96s1tHMOl/KmdW6MPF91nrfK\n+/Lly9nXrqzqvbrTdzrnQ1Xe/f39ybTOeVgVB6W1rNNnK512qNbfTmzfOcddVZ/txDKdve9u+6c+\nX81paTxUeVO5qnGYrl09b9qjrvpV2t9MZ3Rj5Dgo3VccdLsWi8XseaE65+y8V6W8nT38zntV9Y6R\nrr3Kd+rOuUNVl9W4X9V9k87+RdUnO/v/X3755WRa9bydvlM9U+d8oHMu2f0ebRU68XMVi3Rir85Z\nenXtTnyd8lbzf2eMp3nno48+inlTf3/16lXMm/p757ueFNOlb0n/ML8sDAAAAAAAAAAAAABrysfC\nAAAAAAAAAAAAALCmfCwMAAAAAAAAAAAAAGvKx8IAAAAAAAAAAAAAsKZ8LAwAAAAAAAAAAAAAa8rH\nwgAAAAAAAAAAAACwpjZvuwBfl62trcm0s7OzmPf09HT2fS8uLmbn/eEPfziZdvfu3Zj3+vp6Mu34\n+Hh2mSo7Ozuz0sbIZR5jjP39/cm0o6OjmPfw8HAy7fXr1zFv6jspbYwxrq6uYvpcVV2mcn3xxRcx\n76/8yq9MplXPU7Xh5ub8KSX12851Ly8vY3q6dtX+KW9VV6nPrnIMpzau+l0lzbXVOJx73THGuHfv\n3mRaNZemNq7qI83/aT4bo9fGqcypfZfL5ex78surWj9Tf9re3p597Wr+fPHixWTa7u5uzJvK1Znz\nq7mmeqZO3vRMqa7GGOPg4GD2fdNaVa29Kb1qh3Tfqt917vvgwYNZZRqj7h+pXNV6cn5+PpnWiUU7\ncVun/TtjqXov64zDVFcplngXNzc3k2lVn055q7rc29ubTEuxZnXtzvtTNQ6T6l0kPe/Jycns+7Ja\ni8Ui9pnOnkpSxUCdeTuNn2r+/NGPfjT7vp33z7nvEFXe6nmrcqV7d+aTTt5q3u7sA3X6TprXq3GU\n8nb3vTr7CKk+quuuag+pepdP1+7se3T6bDXOOqprd+69qjj24cOHMW+n76T0Ko5NfavaI0rjIY3h\nxWIRr8vqLZfL2Nc7a2yKdTrzYDX/pr5e5U1xULUedfbSO7Fomp+rd65qXUhzXeddsJLu24lHV6kT\nb6xyPy/1n85Y6ty3Wo86e0Greq/r5F3l3lclze+rHCudd9lOHJTaqeqzaS7tvF+Jg769lsvl7LOH\nzj5DtT/YOQ9LfbVal3/yk5/Mzpvqo5o/NzY2JtM673rVXno67xgjz1Wdb1Wq9u/Mn537pryd9q/a\nsHMe1tlnrNaEFKtU58PpvLTqd6s6D6vum+pylfs5q4xF0rtK1ac79ZGeqbpvZ9+ts3Z0pPGQ9pjS\n3P+H+WVhAAAAAAAAAAAAAFhTPhYGAAAAAAAAAAAAgDXlY2EAAAAAAAAAAAAAWFM+FgYAAAAAAAAA\nAACANeVjYQAAAAAAAAAAAABYUz4WBgAAAAAAAAAAAIA15WNhAAAAAAAAAAAAAFhTm9/kzRaLxdjc\nnL7lwcHBZNrZ2Vm89t7e3uxyXV5eTqal8o4xxtbW1mTa8fFxzHv//v3JtJcvX8a8X375ZUxPdnZ2\nJtOq53369Olk2v7+fsxbtWFSte/r168n01IbjZHbv+Pw8DCmX11dTaZdXFzEvKkNP/zww1ywmdcd\noy5XUvWthw8fTqYdHR3Nvvb29nbM22n/ztyRpL6xyrzdurq+vp5975ubm8m0qlxJNQ931o6kav80\nP1T9PbVxmu8Wi0W8LqtVxUCpL1brZyd+6swZqb9V993d3Z193048keqqGrfn5+eTaVU9VrFImj8f\nPHgQ83bue3p6OplWPVOqjz/+x/94zJuet4o10ljprL2VTlxXrY9pPJycnMS8qT6qdTuNparM6dpV\nPJl0YtGqv1fpSWeu7LRDFQOlPv/8+fOYN/Xpaix9/vnnk2nVnJWet2qjNO/w7VXFQKkvrvL9s7MW\nJdX7x6qet9LZB0rjtipzlZ72kTrvl9UzraquHz16NDtvFeN21rGUtxvHdtbmNB6qdSzFQFXfSWWu\n6qOz/5hUZU7pnTZMe3HvUq6kKleaL6uYMD1T2ruurt2JRavxn+qjum8nFuV2LRaL2XNDtValOTS9\n64+Rx3ZnPqrGXxonVT9P60JV5k4clN7J0prwLuVKOvNzNaekmKOKR9K103nnGKs786jaMKV3zuHG\n6MX2KQ7ulKvav6rmh7k671+VlLezF1i1b2c+rKR26OyrVWdLaW7pPE9V5pTeeadMc8NyuYzXZbWq\nvaDO2L2t8/c0Rl69ehXzpv3SVX2nMkZ+b6pi1FRXq/xeoJL2Aqr5JJW70zceP34c09O1q/OfdKZZ\nSWOlaqPOuUQ1HjprQmeNTO3fWfM7MVDnXLoTA1dn9FV9dL5LSM/U+SaoGktzv9EYI/edznlYdd9n\nz55NpqV30bdv38br/gG/LAwAAAAAAAAAAAAAa8rHwgAAAAAAAAAAAACwpnwsDAAAAAAAAAAAAABr\nysfCAAAAAAAAAAAAALCmfCwMAAAAAAAAAAAAAGvKx8IAAAAAAAAAAAAAsKY2v8mbXV9fj5OTk8n0\nzc3p4lxcXMRrV+lz8+7t7cW8Kf3p06ezy3T37t3Zea+urmbnvb6+jun7+/uzr31wcBDTUztcXl7O\nvu/NzU1Mr9o4efjw4WRa1Q6prnd2dmLelF71nVSuNAbHqOuq6j9Jav/qup3xv729Pfu+qb6q9j87\nO5tMq8bZ6enpZFrV3zu++OKLmN6Zt1Jddtp/a2sr5k3t1Gn/1L7Vtau+05njuT1VDNSZP1+8eDGZ\nVq0nx8fHs/Om9FSmMer1JknxxPn5ecybxmbVBru7u5NpVZzSmceqZ+pIz1RJ7VA9b5rHqlgj9buq\nX3Vir+ranTWhMx460n07ZarGQ6qPqh1SG1b33djYmEyr4oVqHKZyV9dO6Z126MRlKdYcI4//qr+n\ndjo6Oop5547/xWIRr8tqvX37dva7Xuc9rxp76dqd9+IUW42Rx0j1vOm+1djrvEN01t5K530sqeqy\nE8ek+uj02cPDw5ie6rraQ0j13Fl7x8j9cpV7uR3VM68qb+fdK9XVKveBq5ggzbXVPNyJnzpzeCdu\nT2Wu8r5582YyrVo7+OV1fX29svbtzKFpbFdrShoH1XlYNbZXlbcTb9y/f38y7fnz57PLVN27M7dX\neTtrd4pXqryp3z158iTmnVumMXLfqdbfzjlN1Q6dmLJzltbZK3j9+vXsvKs60+ic4VR5q/k79b1V\n7vV19jc7fafzPpruW+Xt9DtuT3UellR7vJ3321SmzvvL559/HvOmMnf2jjvxYOd7gs7cW6meqXPG\n14mBVrUX9Cu/8iuz83bqucpbjaXUp9O3N9W9V3kemspV1UealzrnoZVOG3f2VTt7I5VOOySdvaBq\nvUplrr6HSH1nld9SvAu/LAwAAAAAAAAAAAAAa8rHwgAAAAAAAAAAAACwpnwsDAAAAAAAAAAAAABr\nysfCAAAAAAAAAAAAALCmfCwMAAAAAAAAAAAAAGvKx8IAAAAAAAAAAAAAsKZ8LAwAAAAAAAAAAAAA\na2rztgvwh52dnU2mHR4ezr7u0dFRTN/b25tMu76+jnmPj49nXfddrr0qV1dXt3Lfra2tmH56ejqZ\ntrGx8XUX552kPjnGGJub00Ooav+dnZ3JtMePH88uV1XP6b4prbrvGGM8fPhwMq0ah8mHH34Y09Mz\nd/p7at8x8jNVddlxc3MzmdYZK1XfuXv37uz81XxX9a25Li4uYnqnnS4vLyfTqrpMnjx5EtP/zt/5\nO7Ovze1ZLpexz2xvb0+mdeKJamzt7+/Pzpvs7u7Oznt+fh7TUz2uMsZJ963Wi2ouSlLfqO5d1WVS\nzdudOTDNvVV/T89btUO6b5W3qo8HDx5MplVjKdVXVZepHSrpmav7phioylvVdZLGeDXvpPHQWbcr\n1fjvzFspb3XdNP+nWLNKr8Zw6rN/+k//6Zj393//9yfT0hhdLpfxuqzWzc1NnAc776dpPkl7NdW1\nq3Gb+ltV5jQ2O/te1ZhPZV5lHFPVR9oH6qzNnbyd501z6xi5XFX7r6odumtgyl/FQKkdqnKl+uis\nrVWfff78+WRaVebO3JHmtCpvum9VV6vc20qqcqWxVMXtKe8q9xDfvHkzmfYbv/EbMe/v/M7vTKal\nebSK6Vi9O3fuxHGU2q9aF1JfT3PVGHm9qsZBmus6sUylE7t1rHJe6KyDSSeW7ZyHVu2f+l2Vd1Vn\nB1UbVWMp7eM/ffo05u3sUd2WTvx9W2d4aTxU46za3+hI5eq8U1Y6Z3gpvXMuWZ2HvXz5cjItPc9i\nsZhdJvru3LkTzzVevXo1mXbv3r147dTun3/+ecx7cHAwmVatn2lPsxMDdd5fKp33gVTPVZmq9JOT\nk9l5k6ou0/zZyVuVOfWdah9pVXFuVeYq9krnYdW5ZLp3tZ6sMu5P5u5tj9HbC0p1WZ2Hdc4OO3vy\nVXzVmf/TM3X6bCVduxMv/pk/82di+u/+7u9Opn0d52F+WRgAAAAAAAAAAAAA1pSPhQEAAAAAAAAA\nAABgTflYGAAAAAAAAAAAAADWlI+FAQAAAAAAAAAAAGBN+VgYAAAAAAAAAAAAANaUj4UBAAAAAAAA\nAAAAYE1tfpM3u3PnztjZ2ZmV9+rqKqZfX19Ppm1vb8+6Z3XdMcbY2tqaTLu4uIh5Nzenq//169cx\n7927dyfTvvzyy5g31WUqU1fVhjc3N5NpVRuenZ1Npu3t7c0uV1Xmo6OjybSqr+/v70+mVf3u4cOH\ns8pU5T04OIh5O/2jaodqvKxKaqeqHeZed4zcZzv3raRxltLGyPNdpaqP1P4bGxsx7wcffDCrTNV9\nd3d3Z1/38vIypqe6PD4+jnk//vjjybS/+3f/7mTamzdv4nVZrY2NjTjHnpycfIOl+X+leb0z51d5\nO3FbunZnLanG7SrXwE48sar1M61TY9QxUpLGQlXPqS6rcdSJgar6SO1QXTu1f1UfnfGQynV+fh7z\nzi1TpepXaZxWYyXNLdV9V3ntFBN03mOrMqd+l8bKGHmsVeMwPVM1ztIz/YN/8A8m08RAt2tzc3Mc\nHh5Opqe5qrPWVNLY67yPVXNgivXn7peN0XtXq563U65qLUr3ruoyzRmdtagqc6qPznqSxkl17Wr+\nTPN6dd/q/TQ9U2dftNN3qvGQ4snqeTvzQ1oDT09PV3bfb6v0TFW/7LwDpfumvdoxcjtV4zDtbVX9\nLp0DpPtW+3ys3mKxmL0mVf0i9ddHjx7FvJ34K+WtnjXlrebuap6cq/O+VtVzVeb07tTZR6h0Yrv0\nTNX76/379yfTqvZPsUw1/6Z2qtabTr/rvJN3VOOwcx6W6qPKm8pV9Z3Ovtmqzv8qq7x2Zwx33qHS\nM1UxR+edIc0df/tv/+3JtHWMn9dJGvdV23X2h5Pqvp0yp732zt5HJb2DdL4nqs67OuedVUyQ5ozO\n/tarV69ywYIq74MHD2aljZGft6rnzn1fvHgR01NdV3HsqvrHKvc3k859O+d/ndiraoNqjHfius73\nN53zsjROqzKluq7eJztn2mmc/sN/+A8n0941BvLLwgAAAAAAAAAAAACwpnwsDAAAAAAAAAAAAABr\nysfCAAAAAAAAAAAAALCmfCwMAAAAAAAAAAAAAGvKx8IAAAAAAAAAAAAAsKZ8LAwAAAAAAAAAAAAA\na8rHwgAAAAAAAAAAAACwpja/yZvduXNnbG9vT6ZfXV1Npm1tbcVrX1xczC7Xzs7OZNrl5WXMm8rc\n8cEHH8T0s7OzybTNzfnNWj1PqqtK1Uapb1Q6edMzX19fx7wHBweTaVU7pPqo2iFd+/DwMObd29ub\nTKvGWco7xur6ZSX1y6ouO3NHqq/qeVN6VabU76q8Nzc3MT2p6rIzDlO//eKLL2LeThvu7u5Opr16\n9SrmTePh5OQk5k1lrp4npe/v70+mHR8fx+uyWovFIo77lFatvam/VeO2M5+kNbJaPztSbFatY6k+\n0nwwRq6P7vNW5Z6bt+o7nWdK167ihaSKvdNYSf15jFzmat2u6jLFQJV07fPz89l5qzas1qok9btO\nvFh5/PjxZNqqYrp30emXqdxVLJLuW7V/ir1OT09j3hTzVe2bxnjVhul5U7/78ssv43W5XWltXuU6\nluLmTl+s8nbWos78mXTquVNXY/TiyU5drioGquoy9ffnz5/HvGneru7bKXMaK2PkNeO29oGqfpne\njTsxQaeuqvumeKLzLtJ5fxojr7+dWKTav+icISRVDJT6XTVHp7i+et43b95MpqVxtlgs4nW5fWne\nqN6rPv3008m0ah6szg+S1OeqOSPNC9V6tKp9k2qtOjo6mkzrng2mub2aU1J9dc5Sq3kw9Z27d+/G\nvKm+qvumsVL1nc6eaydOrtrhyZMnk2lPnz6NeTvvMp29k6SKg16/fj2Z1nl368T9nbxj5PFQzQ8p\nb2f8d874q3O473znO5Np1RjuvFPO3TP45//8n8frslqLxSLOg2nP8969e/HaaW+xmk/SGVBnfqzu\nm8bIw4cPZ9+3u6edpLHX/eYn1VfnXLLah++cpaa+U50tJv/oH/2jmP69731vMq2KU1I7VGtg9Uyd\ncZjKVfXp9J1L5wyvs39VzVlpL6Cqq/ROWPXZ1EbVWKnOaTvl6sQxqS6rNkzt9Nlnn8W8Dx48mEzr\n7AVV7Z/aIT1Pirv/ML8sDAAAAAAAAAAAAABrysfCAAAAAAAAAAAAALCmfCwMAAAAAAAAAAAAAGvK\nx8IAAAAAAAAAAAAAsKZ8LAwAAAAAAAAAAAAAa8rHwgAAAAAAAAAAAACwpnwsDAAAAAAAAAAAAABr\navNd/tJisfjxGONsjHEzxrheLpe/vVgsDscYf32M8ckY48djjL+wXC5P0nWur6/Hycn0X9nb25tM\nu7i4iGW8vr6edd3q2pub71RFv/B1xxhjZ2dndt70TKkuxhjj7OxsVpnepVwd6d7VM21tbU2mvX79\nenaZDg4OYvrh4eFkWlWXqcxVv9vf359Mq9ooXbuTd4xeG6Z7f/nllzHv3DKNkcdD9bwpPc1171Ku\npKrLZGNj41byXl5exvTOM11dXU2mbW9vx7ypXFXedN/OWOrMWem+b9++nX3d99nXFQPd3NzEeaHq\nb0nqT9U8lsqU1qkx8rit7puuXc2f6dqdMd8Zt1Ws+eLFi5ie4o1q/kx1meapMfJaVK1T6Zmr9k/t\nVOXtvC/s7u5Opp2fn8e8nXKlWGOMPJaqvJ3xn+5btX+6dqfMnXixE0tUqvkwlevm5ibmTeO08y5a\njf80t1RzWrp21Q5p/Fdz5are2/n5vs59oOPj48n0hw8fTqZ19oGePHkS8z5//nwyrRoDc8s0Rm/+\nTOWq5qnT09PJtM78mfZExhjjpz/9aUzvrOud9aTzPp507luVqbOHmPpHGp9j5P2nMXprQurznfZf\nVftW1+7sXXZUa2Cqy6quqniiendLOuXqrPudGHhVqvt+8cUXs/JW7zhM+7rioOVyOXvvpJoH0/hM\n8dUYdcyxKilu6JylVbFbWus6c3MVB1X13FlTklXGI539nM4c2+k7KZap4qCqjZOqPj799NPJtKpc\nnT259F7QUcUMSWf/qrOP1Im/x1jdXFr1u/Qu24nt7t69Oztv59uBqg2r8cDX6+s8D0tj5N69e5Np\nVZ9I6Wn/f4zeedjcMo0xxscffzyZVp3/dPZNkuobmPQu0T0PS/FE50yrypvWmwcPHsy+b+f9tTrT\n7ORN5araqLM32vkmpHtON1cnjq2et6MTX3XO8Ku+leb3znloVa40x1fzcOpb1fhPVvWN0xi5LtO5\n43K5fKf7/yK/LPwvL5fL31oul7/91Z//kzHG31oul39sjPG3vvozAMC6EQMBAO8jMRAA8L4SBwEA\n7yMxEACsuV/kY+H/r397jPHXvvr/f22M8ef7xQEA+NYTAwEA7yMxEADwvhIHAQDvIzEQAKyZd/1Y\neDnG+J8Wi8XfWywWf/mrf/ZouVw+G2OMr/735/63jRaLxV9eLBa/t1gsfq/znyYDALgFX0sM5D+X\nBQD8khEDAQDvq68lDnr58uU3VFwAgK+FvSAAeA9svuPf+3PL5fLzxWLxcIzxPy8Wi//zXW+wXC7/\nyhjjr4wxxq//+q8vZ5QRAOC2iIEAgPfR1xID/cZv/IYYCAD4ZfO1xEG/+Zu/KQ4CAH6Z2AsCgPfA\nO/2y8HK5/Pyr/z0aY/yNMcafHWM8XywWH40xxlf/e7SqQgIA3AYxEADwPhIDAQDvK3EQAPA+EgMB\nwPuh/Fh4sVj8S4vFYu8P/v8Y418bY/zvY4z/fozxF7/6a39xjPE3V1VIAIBvmhgIAHgfiYEAgPeV\nOAgAeB+JgQDg/bH5Dn/n0RjjbywWiz/4+//1crn8HxeLxf82xvhvF4vFXxpj/GSM8e92C3N9fT2Z\ndnFxMTvv0VH+F5wODw9nXbeytbU1O+/Ozk5MPzs7m0x78+ZNzLu5Od3s1fOmZ7q6uop5K+mZ0/NW\n967aIbX/8fFxzJvqq9N3vv/978f0VK6HDx/GvKlce3t7MW81DlMbVnlTO1R5X79+PZlW9ctOmZM0\nzqr0zn07887l5WVMr/pHaodOuao5bXt7e/a1O9JYqto/qerq/Px81n2/Wsf5xXxtMdByuYx9prO+\npjWy0xc781glPVNnTFdrb2dtTnPgyclJzFu14e7u7qwydaVnquoqrZ+den78+HFM//zzz2fnTe1U\nrXHVM6XxUMX1af09ODiYnTetF2PkZ65i7zROq/t2dMqcdGLcMXL7V/WR5v9qPry5uZlM67zXVXW5\nsbExmbbKdSfpzDv8XF9rDJTWwTT+qrGZrvv06dOqaLPvm1Rrfrp2NQbSuO3smd3WO/MYY+zv70+m\ndfaYqrpM962eKbVDtV4kjx49iumpTz958iTmPT09nX3fqh069ZHW9bRHNEZ+pqrMq9qP6fTZKu9t\n7U9XMXKKGaoyp2t3YrPUN8botX+6b4qPuj744IPJtFTPy6X/+vNMX1scdHNzE/tkWo8qaWxXZxqp\nL3fmss6Z1irnm6Qa9525Ku2Vj5HX4CqWSc9cvUd2Ysq7d+/G9Ln3rWKZ58+fT6ZVsUwag1W/68TJ\nlfTMVTt03qFWtZ9XxRvpvtWclXTi/m77prq+rbpc5Rn/3LONyirPIPiFfWPfBCXVWXXqqy9evIh5\n0/ip+lpn77l6prmqeSrNr9V6kc4lqv3uai5K52Gdeu7s4XfK3Dn/++ijj2LeZ8+eTab96q/+aszb\nib0rad6vzjtTfX344YcxbzoPqaQYuRqjqe9UfbYTL3bW19QOnfPOKn9V5s4zpTas5v/O+E+q/t6Z\n/1cdA5VXXy6X/2SM8Zs/55+/HGP8q6soFADAbRMDAQDvIzEQAPC+EgcBAO8jMRAAvD/u3HYBAAAA\nAAAAAAAAAIDV8LEwAAAAAAAAAAAAAKwpHwsDAAAAAAAAAAAAwJrysTAAAAAAAAAAAAAArCkfCwMA\nAAAAAAAAAADAmvKxMAAAAAAAAAAAAACsqc1v8mZ37twZOzs7k+mnp6ezr729vT2ZdnV1FfNeXFzM\nShtjjLOzs8m09KxjjHF9fR3Tk83N6aZLaWN2FO/9AAAgAElEQVSMcXJyMplW1VV6puq+l5eXMT3V\n9cHBwexrV/0qtWFVH6lcW1tbMW9S9bu9vb3JtKrfpXaq7ls9U6qvqr+ne+/v78e8qQ07z1T16VTX\nnbmjum+qy6rPpuetxmjl5uZmdt7OeEn3TWOlylvVZVI9T2r/u3fvxrxpDk99Y7lcxuuyWovFIo7t\n1CdSjDNGnouOjo5i3ocPH06mdebtKm8njunET0k1B6ZyPXjwoHXtNK6rGOj8/Hwyraqr1O+qvOmZ\nqzZMqrjt8ePHs++b6rJ63mocpjau1pNU7modW9V9q3gy9Z1KJ45Z1dpctX+1rldxX5KeqXPdzly6\nsbER83bi5zRnVf09zZWdeYfV2tjYiO9znT2VNEaq+SKVqRp7Kb3qi53x07lv51095a3GbbWup2e6\nf//+7LzVfY+Pj2ddd4wx/uSf/JOTaZ0+m8o0xhiHh4eTaVV/f/LkyWRaVVfVM6V7d+bmqm912rB6\nprmqdkhjbZXrWKrLqt9V7ZB0nqlqw1SXnXFYxXzpvtWeWCpX1Xdev349K699oNtXxUGreheo3tdS\nf6zGwfPnz2ddd4zeuUSqq8683tlLr+7biUc681G1B5XSq3ZI/bk6w0mq+6Y4qGqHuWNwjN55WCXl\nTfu1Y4zx6aefTqZ19mQrqcydOKijc90qb2dOW+Xe6KrioFW10Ri5f3Tmu1TPi8WiLhi3Jq1F1T58\n50yjE4t0ypzuu8qxl8ZXNW+n5/3www9j3qo+Xr16NZlWnbWlvJ02rKRyVXvpqa7TXvkYY3z00UeT\naZ0zvOp9odoLSPeunilJ539jjPHZZ59NpqWzgzHG2N3dnVWmMfIYXuXZUWd+SDFB1Uarqqsx8jN1\n9uQ7Z7gdnXevjjTvvGsM5JeFAQAAAAAAAAAAAGBN+VgYAAAAAAAAAAAAANaUj4UBAAAAAAAAAAAA\nYE35WBgAAAAAAAAAAAAA1pSPhQEAAAAAAAAAAABgTflYGAAAAAAAAAAAAADW1OY3ebO3b9+Oi4uL\nyfSdnZ3JtOvr63jtq6ur2eVKtra2Yvre3t7sa6dnSnVRefr0aUzvXHtzc3VdJtX10dFRzJueqfO8\nd+/ejempP1d9NuV9/PhxzJvqqtNGVV1Vz5Rsb2/H9FTuVFdV+v7+fsyb5o6qLlN9VHlPTk5mXber\nM1e+fv16dt6qDTv9dmNjYzLt7Oxsdt6UNsYY5+fnk2nVWEr1UdXV3H63WCzidbldaY6s5oSUt5p7\nq/6WpH5eXbcT86X0NLdW963qKqnixc61q2dK967mos7ce3l5OTtvum/nut///vdj+irX1/RM1ftC\nWk+qdTulHxwcxLyprqu+kZ6pGv+pT3fi9lU6PT2dnbdqw06/THmr+6Y27MSxu7u7MW/qd526SOPo\n7du3s69L32KxiH2qM+47eyqdcd2R6qIzBjp7JlUck8pc5a3ex5Oqjap7J+mZDg8PY960zlVzb3o/\nreKYpPP+2dkzGaPXDilv9S6fdJ6pqstU5ioGSnsqVT2m9FXtiY9Rx+adcs09Ixgjz5dVO3Ten9K1\nq5g/7ddX8//ctWO5XMbrsnrL5TKOhc6ckvpr1R87e0FPnjyZTDs+Po55VxX3VfdNVnkecv/+/Zj+\n8uXL2dfunKV0+k7SmfdTvxojxwWd2K0z71f5V/WuX+mM/84YrWL3zjN13qFS3mrvo3MuWenM/53x\n33kP+tnPfjY77/Pnz2N6Ig5aT+m8pHoH6ZxLvXjxYjKt2tPsnOF15qJODJTmmlW9y49Rn4elZ0p7\nvNW1V7kf3lk/U7+rvglK9fHRRx/FvGkPv/u+kPpANZY638ik9Oo8LPWPqj5S3up5U/t39qAqq/oG\nqkrvjKVOuaq1o3OG2xlLnTPvuXPlu56H+WVhAAAAAAAAAAAAAFhTPhYGAAAAAAAAAAAAgDXlY2EA\nAAAAAAAAAAAAWFM+FgYAAAAAAAAAAACANeVjYQAAAAAAAAAAAABYUz4WBgAAAAAAAAAAAIA15WNh\nAAAAAAAAAAAAAFhTm7ddgHd1dXUV07e2tibTdnZ2Zt93czNX0cXFxey8KT1dt3J4eBjT37x5M/va\nyenpaUzf29ubfe3t7e2Yfn19PZl2dnY2+75VG3byfv/7359Mq9o/9ek0Frp5Uz1X+atnqq49974d\nVd9JbXxychLzpnao5ruU9/j4OOZNdXV5eRnzVlK5q3lnVfNS5e7du5NpVV2mPt0ZS1X7p/RUprdv\n38brslp37tyJ62Bq12p+TNetYqDO/NmJVdIzda7biTU6qjn/4OAgpqf66KyPVblSfFWtCamdqnb4\n6KOPJtOq+KmzfqZrV2Wurp3qqxNPdsbo+fl5TO+0f1LFTw8ePJhMq8qcdOKn6nmrd5GUvxrDnTGe\n+lbV71KZO7HIxsZGzDv3umPkeSflXS6Xs8vE6qWxWe0xpL2PzntRNX46ewyrUu0DdWLN1EZVXVSx\naCrXkydPYt5076rv7O/vT6atcm/iT/yJPzGZVtVVKleVN6VXa1wnvqr6Ryfu7+ypdvauktevX8f0\n1O+qPpt093I6UrmrsVT1rbk6Y6lq/5cvX06mpf2lrs68xO1aLBaz3+k6c9mq9srHyHN7575VP7+t\n9+qkGwfdv3//6yzO/6Oqy7Qeddb96r6ffPJJTE/Snk3nebv7iGmcds7Dqv6eyt2ZO6p4pBNDPXr0\naPZ9V3VO2zmHHaNXl+nenfmwqqtOHHRzczOZ1jnf6+y58u11586d+K6Z+mLV5ru7u7PL1TlbSOWq\nyrSqs4NqLyjpzIHVuVNVH51vtzrfBHXquvNO9qf+1J+aTKvKVO3ZJCle6NTFGLn/VGO4U5cffvjh\nZNqrV69mX7dzPlSdaXXOw1a199W57xjfzm/zqnP4VK6qzCm9M59V5u4Ff/HFF+90fb8sDAAAAAAA\nAAAAAABrysfCAAAAAAAAAAAAALCmfCwMAAAAAAAAAAAAAGvKx8IAAAAAAAAAAAAAsKZ8LAwAAAAA\nAAAAAAAAa8rHwgAAAAAAAAAAAACwpja/0Zttbo6Dg4PJ9LOzs8m0nZ2deO2Li4tZ1x1jxDJdXl7G\nvKlcV1dXMe/W1tas644xxtHR0ey8yZs3b2bn7ZS5e+3NzemuXLVDunZ139SG19fXMW/qlw8fPox5\nU39PaWOMsb29HdOTzjispPpK9TxGbv+qHVKZq+dNfWtvby/m7fTZVc2VGxsbMW8l1fXh4WHMe35+\nPpmW6mqMMV6/fj2ZVj1Tp9+luq76XcfctaPqV6zWYrGIfbnq58nJyclkWtUXU3+q+kwqc7UeVHNC\nkubA3d3dmLcz16S6rPL+5Cc/iekpFu2oypXqoypTiieqflfF10ma56rrpmfqxItj5LqspPHSeZ+o\nYr5OPJHmncePH8++b9VnVxUDVXNH1adTOz148CDmTXXZUY2HzjtB575JNY5SG6fY+8svv5xdJvoW\ni0UZV0+Zm6+rGvOdPZfOu2uaazplqqR2qObtH/3oRzH90aNHk2nHx8e5YA1pTdjf3495UxxbxcCd\nvcvUxp28VZmrvY3OPlDq053YvBoPnVik03eSu3fvxvTT09PJtE4bVXmrdT3ND1V9PH/+fNZ1x+jN\npUknb9qbGiM/0yrnO25XtReUVPsmq3qPrKxq/HV06qqjWm9+/OMfx/TO3lhS9bnUP6q+keb2tFaN\ncXt9Np21VX1jledhHZ0zjbT+Vmt3uu+TJ09i3qp/JCkeqfp7yls9b9Uv0zNV4zut/dUzreq9oJLy\ndmKZahzNPYN4+/bt7DKxep2+mPYPqzFQvf8kaf+4ioHSfasyd84O0xzYWVurs/ef/vSnMf3evXuz\n751U7dDZD097/NWedpqrqjJ3+l2n71TnJav6Jqhzpl31q1evXk2mVX0jtXF13865dOc8LKnOf6t5\nqXMe1jlLTe1fnbN1zrQ70lip6mJVZ3h/wC8LAwAAAAAAAAAAAMCa8rEwAAAAAAAAAAAAAKwpHwsD\nAAAAAAAAAAAAwJrysTAAAAAAAAAAAAAArCkfCwMAAAAAAAAAAADAmvKxMAAAAAAAAAAAAACsKR8L\nAwAAAAAAAAAAAMCa2vwmb7ZcLsf19fVk+sXFxWTazs5OvHa67sHBQV24ma6uribTtra2Zl83Pc8Y\nY+zt7c3Ou7k5v9nfvHkzmba7uzv7umOMcXZ2NplWlTn1j4cPH8a8nXZKfba6b0qvypTqI/XJMer+\n0ZHuXd13e3t7Mu3k5CTmTe1Q3Xd/f3/WdStVn03P1Bmj1fOmvlX1napfduaA4+Pj2de9ubmZTNvY\n2JhdpirvqsZS1Q5prPDtVcVAnTUwpad4YYzePJeep4rb0n2r503P1Bk/l5eXMW+aizptVN27eqaq\nrpM0r3fWscePH8e8KTbvPE81L5+fn0+mVWtNde2U/8WLFzFvJ65b1VjqrHHVvJL6TtX+nRgpPW8V\na3bmlqo+Uv948OBBzJvWjipuS21crR3pvp13kap9xUDr6fT0dDKtmhNSX+zMJ9W47fTjTt7Ou2sa\n1+ldrPLo0aPZecfotWGS6mqMXB/VfX/4wx9Opn3yyScxb7p2Z93uxPSd/dZKGt/VvY+Ojmbft5Lu\n2xn/1fr56aefzs6bdN4X0hh8F6vcv0qqZ05SfXTeYyur2mM8PDycTPvyyy9n35NvRmcvqLMnk6wy\nDkrvKJ0zmlXOg526fPLkSUxPdV3VRypXNb924pEf//jHk2m/9mu/Nvu+VT132iG1cXffNJWrirFT\nXVd5U4yV1oUxxrh///5kWmd9raS9z1XuX3fe+yrVO0eS+mU1dzx//nwyrTNWqvGfvg+o8qY+3Xkf\nSf29855L32KxiGtZ2g+t9uk752Gr+r6iGgNpvvnoo49i3lRX1ZlW2kvtxEDVNzDp3H6M3vyb+lWn\n/au56J/+0386mfaDH/wg5k3l6nwDVcV8nT236rwk5a/ypn757NmzmDelf/e73415O3tBqZ2quL3z\nDWTS+a6nGv+dPcpKund1/pPOeKu8abxUeTtrR8rbOYf/OvhlYQAAAAAAAAAAAABYUz4WBgAAAAAA\nAAAAAIA15WNhAAAAAAAAAAAAAFhTPhYGAAAAAAAAAAAAgDXlY2EAAAAAAAAAAAAAWFM+FgYAAAAA\nAAAAAACANbX5Td5ssViMzc15t9za2pp93+vr65i+t7c3O2+yvb09O+/l5WVM39nZmUw7Ojqanffk\n5CTmTXV1dnYW8+7u7sb0i4uLmD5X1XdSfT1+/DjmPTw8nFWmMXJ9da67v78f01Of7rZBuvbV1dXs\n61blSn26GsOnp6ezytSV5sL0PGPUY23ufaux0mnDSvXMyYMHDybTXrx4EfOmvrWxsRHzdubDTju8\nevVqMi3V4/n5ebwuq7WxsRHXwdQXq/GRxmY1Bg4ODibTOnNNp8ydOb+ap9LYq+6b4rrOmK+kuWaM\nXqya4r7vfe97MW8nzq3izbmquuqsY1XeNMdWbZT6dDWGU9+q7pv6bbUWpfav8qbn7fSrapyl96tO\nHFKp3uvSM1dt2ImBUn1V/b1z36Rq/9Rn0/g/Pj6eXSb67ty5E8dYeh/rzCdVu3feIZNO3iqeqNa5\nJJWrKnNqhyp+WuVeXlLN66l/PHr0KOb95JNPVnLfdN0x5s+BY/T2eqo2TM9U5e3E5mnfrNrnef78\n+WRataeW2rjzPlHpxCqr2m8dozfGO/siqf2fPn06u0ydfvfjH/845u28x97W3iV9i8Ui9ufOOOjE\nv505pbO/kVTjoDP/dt59ks7+1RirO6ep7pvWwSoe+bVf+7U5RRpj5LruxFDVWVpnD7KKR1fVTlW/\nTPFKp8yd8d2pi1XGKqk+uudhq5oPq3U/va+k8V2p+s79+/cn016+fBnzduL+uWepd+74vbzbtLGx\nMe7duzeZnsZX5x2j2pfs7Kus6ozns88+i3nTueK3df+qevdN475q/5S3+hYpPXP1TcAPfvCDybRq\nvUj1lfbZq2tXdZXOJar+XI2VdMbXWV+rcn33u9+dTLu5uYl50/xQ3bdzPtzReW/r1HNnfqjOwzrS\nufWzZ89i3s5ZavqWojrv7pxBVHPalCou+wMiJQAAAAAAAAAAAABYUz4WBgAAAAAAAAAAAIA15WNh\nAAAAAAAAAAAAAFhTPhYGAAAAAAAAAAAAgDXlY2EAAAAAAAAAAAAAWFM+FgYAAAAAAAAAAACANeVj\nYQAAAAAAAAAAAABYU5u3XYB3tb29HdM3N6cfZX9/P+Y9PT2ddd3K5eVlTL+6uppM29nZiXnPzs5m\n5033/fDDD2PedO2qrk5OTmJ6aqdU5jHGuL6+nkyr6uPg4GDWdccYY2trazJtY2Mj5k1SnxxjjMPD\nw9nX3tvbm0y7uLiIeVO/GyPXV9X+yZs3b2J6quvqmVL/qJ63Mz+kdqju21HVR3L37t2YnsZpNYZT\nG56fn8e8Nzc3s647Rp6nq7ypLqt6Tn2nqit+OS2Xy3JNmdKZax4/fhzT03yT5qlKFQN11u259Vjl\nTWv6GLlcDx48iHk79VG1f7p2inEqVZmr2DxJz/vixYuY9+OPP55Mq/psqsuqX1X1kcZSde3OvJ/6\nZbUWpT5fxSKprqs+m/pldd8UI1dzR6qPKm8lPXM1VlJdHh8fz75vNR4675OpHar+fO/evcm0zz77\nLObtxLHcnrdv386Om6v33jQGHj58GPOmMdCJvTpzfmfcVjr7XmlOWOW7S1Ufq5rXq2dK167aP+17\nPX/+POZ98uTJrDKNUce5SbU/taq8nXW9Wi9SO3T2kKp6vn///mTay5cvY96OzrtG1adTnFPFz6kd\nnj59GvOmcVqtHSm+qvKmcdp5P6rGSmdd4nYtl8vYXztx0NHR0ey8Kabo5K2k5636+aryVlI8UsUq\n1ftcUrVDJw7qrIPpmas1I6V3zsM67wzV2l2ld85D0jNX903tVOV9/fp1TE86Z7ip71R5q5giWeVe\nUGdPLr2vpvm9Un0P0YmDOnNaKldn7kh9Z7lc1gVjZd6+fRvPdtMZQNr/HyPP651zms57cyXNCdUZ\nTspbvYN05s/OvnM1rtM3I9W3Ssnu7m5MT3vP6VuDypdffhnTU9969uxZzJvGQ9pnH6PX36tzunTt\nag3snPF2rpv6dPU+0cmbVGO0E8esqp7H6O0FpXm6Gg9zyzRG/t6omjvSeOic/1fStVNc9q739MvC\nAAAAAAAAAAAAALCmfCwMAAAAAAAAAAAAAGvKx8IAAAAAAAAAAAAAsKZ8LAwAAAAAAAAAAAAAa8rH\nwgAAAAAAAAAAAACwpnwsDAAAAAAAAAAAAABravObvNnbt2/H5eXlZPr+/v5k2tHRUbz24eHhZNrF\nxUVduAlbW1sx/fr6ejLt4cOHMW96pk6Zq7yprqrnTXZ2dmL6yclJTL+6upp97fTMH374Ycz74MGD\nybTz8/OYN/nggw9ieipzaqMxxjg7O5tMq/pdum/Vd6r+0em3m5vT09F3vvOdmPf4+HgyLY3R6r4d\n1X1XVVfVddM4u3v37uy8Y4w4v29sbMS8jx49mky7ubmJedP8ULXvq1evJtM6fefg4CDmTfN/1Q6p\nnlP7v337Nl6X1apioM64TnNztfZub29PpnXmsWotSs9b3TelV3n39vZmpVXXrvI+e/YspnfWot3d\n3dl501yV+usqpbhsjNzvqrk3tWFnrIzR69Od+6ZYtWrDVObqvklVl0lVVylGrvJ23nM682H1HpP6\n7YsXL2aXq7pv6h9V30n9o6rn9ExVv0vt//u///uTaZ0xyOqtai8nvTOP0dt/SNeu3uU77+OdtSa9\nb1XS+3Zn72KMXgyU5ptqLkqxW+eduuo7p6enk2lVG6Vydda4H/3oRzG903duSzUeqnaaK7VvV2rj\nVe0vjVHXZTXGkzQOqzbq9PlUX2m+q+5b7V0+ffp0Mq163rTHbB/o2225XMZxlM7DUp8ZozcvrGpv\nOT1PV2cvqIpXkk4c1GmHSspbzSlpz64zr3fW11WtzZVq7a7Wm5S+yj2KVF9VG96/f38y7eXLlzFv\n53lfv349O2/nPWhV539j5LrunGm/efMm5k1zbdWn07U7sWwVB3X2CtPed3qXsRd0u96+fRv7eXoX\nqM5S0vnBKueTFANVZxrpLLrSiYHS+Ll3717Mm/Zwq+et2rCz7qfvfqo5/3vf+95kWvVNUNq37uyN\nVWVO46g6G0zXrs4dqmun9E482fk2r2rDdA5TlbkTeyVV+6f7Vt/tzL3uGPUzpfOj6oznttbnTt9J\n9dX5PqDzTcNnn302mbZcLuN1/4BfFgYAAAAAAAAAAACANeVjYQAAAAAAAAAAAABYUz4WBgAAAAAA\nAAAAAIA15WNhAAAAAAAAAAAAAFhTPhYGAAAAAAAAAAAAgDXlY2EAAAAAAAAAAAAAWFM+FgYAAAAA\nAAAAAACANbX5Ln9psVjcG2P81THGr48xlmOM/2CM8X+NMf76GOOTMcaPxxh/YblcnhTXGZub73TL\n/5/9/f2YPve6Y4yxs7MzmXZxcTE77/Hxccy7vb2dCzZTKlPl6upq9rVfvnwZ8x4cHMT0k5Pp7rOx\nsTG7XDc3NzHv9fX1ZNrnn38e8/7Wb/1WTJ9rb28vpqd+mZ5njNzv/ugf/aMx7z/+x/93e/cW6ld2\n3wf8t3SzNNZtNCOJudkzApOZ4OswFJcUk9qluG2I89BASwMmBPISQgotxe1LaSEPfWmbhxIIdjLz\nkF6M2zSmD6HBTWmfpnFql6SZ1k2MHEmjkTQjyZJGd83qw/kHBs3Zv7X+/3X+5xyd/+cDw+j8l/Z/\n7/3ba6393RdJf5q2Z/2nNUazfWr1y2yfW+vN+k6rlpnr168vvGxrva16ZPbu3bvwsi0j23Xjxo3J\nttacNtJ3svHQqlX23bdv306XHTFyvmN+G5WBWkbGZtYnRrJGq69l5/XsnN7Syl4j55qszq3cls1F\nly5dSpd97rnn0varV69OtrX2KTtntObArB5nzpxJlz116tRk28GDB9Nls3otc87P8lXrGH3ve99L\n2zOt7coyQ+t8MjIeRq4ZRjLSyHksO/7LzDitbc6OU5ZxIvLj37p+unbt2mRbay7Nrq8ee+yxdNmb\nN2+m7Zms74wcw+x81zoGrG87ZKCR64vWNXXrvJ85ceLEwt+7rP0dmVuzuSQi4tixY0tZtqV1nsrq\nNXKeunDhQtr+8ssvL/zdmZE+O5L5XnrppXTZVj0yrWN48eLFybbWPmXnuVYts3vMp0+fTpfdKiP9\nfWR+aC07Mm9lx791DLP81MpA2T61+mw2543cQ2pZdJt37fL3xCxqs3JQpvU8LLPMvpxtV+uckW3X\nyL30kXtBrSyT7W82j0VEnDx5Mm3P1t06hsu6F3Tu3Ll02U984hOTba3jkGWZkczY2t9su5555pl0\n2R/84Adpe+uckxm5HhnJulnfat0LyGq9rPsEo0buM7TG4Ug+z/rOkSNH0mWzcdoah/v27UvbF9Ua\nCyPPcLN+mR2jd999N/1e1reR7wRl42/k2fzIOxLZPdxWX8yeeWTPd1rfvcwMlF1XtZ5pHT9+fLKt\n9cxi5J2gkfdJWnNR1idbxyGrx0i/e+GFF9Jls/vaIxk425+I9vPBzMizlNb5M1u2dRyyfV7mM7yR\nnLOs+witsdJ6tyDrl616ZON/mXNHtk8jfXaZz0qz9g996EOTbaWUrvX33jH6lYj4nVrrixHxqYh4\nIyK+EhHfqrV+LCK+NfsZAGAnkYEAgFUkAwEAq0oOAgBWkQwEACug+bJwKeVwRHwuIr4WEVFrvVtr\nvRoRX4qI12a/7bWI+KllbSQAwGaTgQCAVSQDAQCrSg4CAFaRDAQAq6PnbxY+FRGXIuI3SinfKaV8\ntZTy4Yg4WWs9HxEx+/+6/w5jKeXnSynfLqV8+5133tmwDQcAWLINy0Aj/7QcAMAm27AM9Pbbb2/e\nVgMAjPM8DABYRTIQAKyInpeF90TEyxHxq7XWz0TEuzHHPy9Qa/21WusrtdZXnnjiiQU3EwBg021Y\nBjp27NiythEAYKNtWAZ68sknl7WNAADL4HkYALCKZCAAWBE9LwufjYiztdbXZz9/I9aCwoVSylMR\nEbP/X1zOJgIAbAkZCABYRTIQALCq5CAAYBXJQACwIpovC9da34qIM6WUH5l99IWI+OOI+GZEfHn2\n2Zcj4reXsoUAAFtABgIAVpEMBACsKjkIAFhFMhAArI49nb/vFyPiN0sp+yLi+xHxs7H2ovHXSyk/\nFxF/FhE/vZxN7HP79u2Fl71169Zk2/3799NlDxw4sFBbRMT169fzDUvs3bt3sq21zZmbN28u3P7g\nwYOF1xsRsWfPdHfct29fuuzdu3cn2+7cubPwej/+8Y+ny2b1OHLkSLrsiRMnJtuyPhmRH/+WrN+1\n+uSVK1fS9my7Dx06lC6b7VOrHot+b+u77927t/B3Z8c3IuLixek/eJn1yZbWNo9ofXc297SOQ7bP\nrXkp267Dhw+ny2bHf/fu3Quvt3UMs3NWq1ZZnY8dOzbZ1qojkzYkA+3atSv2798/2Z7Nr63+NDLu\ns/7UyhMHDx6cbGtt88gcuKzzRStLjuSrGzduLLxsVufWd7f6RtZ+6tSpdNmjR48utE2t724dw6x/\ntObPS5cuTba1jn8rI2Xb3bomyNpb/S5rb613ZO7IjsPx48fTZbPj0DKSgUeMjP9l1Tkir0eWCSIi\nLl++vNA2tdbbGivZPrXGf3YeffzxxyfbWtcwTNqQDFRKSY/7yDXEyPjK5siRPNGSfffI/rbmqZH8\ntKzzVEQ+J4xcF4/cb/vkJz+5tPU+88wzk20j9x9bzp07N9nWuncxkoFa94Gy9tZ5aqR/XLhwYbKt\nNQ636j5wVufs/Bgxds3Xas/2aSQDt+albD48efJkumzWt1rnlZFtXlaOzbZp166ef1SSCRt2L2jR\nfjPSH0fOKdn9moj8OmPkPnxrXh95hqJ6CAMAABExSURBVJd9d2t/s3PVMu/XjcwZ165dS9uzer34\n4ovpsiPn3+eff36ybSTntGqV5aDWuXkkj7RqtazrkZF+2ToO2Xa1nsOM3AvO+uwyx9lWPZds1SNr\nbx2HrE+PPJcauZ878twxu746ffp0+r1M2pR3grI+07pP37r+WdTItdHIc5iRc2vLyDx29erVhb83\nu0/b0rqPkBl5DvfUU0+l7Yve24yIeOGFFybbRt6faO1vlnOW+QynNUZHng9k7260+s7IeBi5F5TV\ncrteT4w8Lx+5zzyyv61+l+3TyLumreuJbLtGnjtm93Nb17h/ruvsU2v9bkS8sk7TF7rWAgDwCJKB\nAIBVJAMBAKtKDgIAVpEMBACrwR8vBwAAAAAAAAAAAIAdysvCAAAAAAAAAAAAALBDeVkYAAAAAAAA\nAAAAAHYoLwsDAAAAAAAAAAAAwA7lZWEAAAAAAAAAAAAA2KG8LAwAAAAAAAAAAAAAO9SezVxZKSX2\n7Jle5e3btyfbsuUiIu7fvz/ZdujQoXTZvXv3TrZdvnw5XfbWrVtpeybbruvXr6fLZvU4cOBAuuxb\nb7012Xb16tV02Y9+9KNpe6Z1DDM3b95M248cOTLZ9sMf/jBd9s6dO5NtrW3Oan3v3r102ZFjmC17\n7dq1dNlsrIwco4i8T2frbWkd/2y9rTF88ODBhbYpIh+nFy5cSJc9fPjwwusdmTuyftmqc0vWb1vj\nMBvD+/btW3ibWnN0a5xmsnNHdj6LiNi/f/9kW2usZO3ZMSilpN/LctVaF84qrbk5G/et80mmNX6y\n/Wn142y7Wrlt5HySnRNa8+dHPvKRybbWXNKa81vn7kx2HhvJqcePH0/bs3plc1zEWBbJ+k7rGGbr\nbdUqm/N72jMjfXokE4zMD1euXJlse/PNN9Nljx07tvB6s37ZWu+i58+Idr/M+nzrOBw9ejRtz2T7\n1MrAy8pALSPX7efPn59sy47hrl3+jPSjqjX2sn7c6qcjc2/23a3vzc43J0+eTJfN9nfk3tVI5hu5\n3xaRZ6CRPNk612T7PHJ+bNVjpM9m+zRyH6C1bEu2XSMZtyWrV6tPZ9m8tWzWfvr06XTZrFatfpe1\nL/M+YCvXZ/vUmpdGMuFIPbLxMPL8oXVPLVtv63ox++7sGLkPtP1lfa7VH7P5aOT6dfTaaFGtcZBt\nc2vcX7x4cbJt5B5Eq86ta67su0fu57S2a1kZauQ+fOs4jBz/bO5uLTvS30eybKuWI9uVLTuy3nPn\nzqXLjjw7PHHixGTbMp/hj1xTtvrWyPFf1twx8kx7ZL5rnXdaz1rZnnbt2pXm1JF3JLJr2JHrm2Xd\nJ4rIx0jrPkImu0cfEXHjxo3Jttb8+fTTT0+2ta4TW+0j54Ss1q13PrLvbj0Py577t+bAkWepWa1G\n7ueNvMcwamT8Z/u8e/fudNmRvvP2229Ptp05cyZdttW3Fl229V7fyDPc1lya1evSpUvpsiNz3rLe\nr2ntb9beuq+ajbVWv2s98xzlqRkAAAAAAAAAAAAA7FBeFgYAAAAAAAAAAACAHcrLwgAAAAAAAAAA\nAACwQ3lZGAAAAAAAAAAAAAB2KC8LAwAAAAAAAAAAAMAO5WVhAAAAAAAAAAAAANih9mzmyt577724\nffv2Qsvu2ZNv6v379yfbWutsffeism1qtR86dGjh9V6+fDltf/bZZyfbHnvssYXX27J37960/fr1\n65NtDx48SJe9d+/eQtsUkR//AwcOpMu+8847k22tbW59dybrOyNjpdXvWsew1fcyt27dmmzbt29f\nuuzdu3cn21p1vnPnzmTblStX0mVHxunI8c9qNTKftcZRa364efPmZNuRI0fSZbO+06pVtl3ZvBIx\ndu7Yv3//ZNvIcWgtm633xo0bC6+X5bp//35cunRpsv348ePpsplsbh45J1y8eDFd9tixYwuvN5s/\nW/ubac1jTz/99GTbyPhpzRdZrVrLt85Fp06dmmxrnbczrVoumukjIk6cODHZlp3jWu2PP/54umw2\nBlt9ttU+ch7LjnHrPJbt88i5KKtVRL5PrX6XbXPr+Ge1aq03a2+tt3UMR/J1NveM9MvWGM6uVVrj\nO9un1v6OzB3ZPmW1KKUsvE7GPXjwIJ3LRuaT1thdlmy9rbGX7e+1a9fSZUfO69nYbNUxG1+j82e2\nT61zYJavWrUcuYeUfXerHs8///xkW2ubsnq0suay+mxEnoFG8lPr/tLJkycn20aOb2u92VhqjdHD\nhw9PtrX6bKZ1rybLKSPXXhF5rVuZIOuXI/PSyD615p1sPLTuXWbHf6TPZtmq1rrw97Ixaq3p8c36\n67KeHUTkfW7k3N6a97P1joyD1vybnTNa+zsyh7bmwWzOGTkOrb4zchyWlWVbxzDbruweU0R+f7O1\nbOsYZ8dp5Nw+chxGntG1ls3OZS1PPPHEZFv2nDVirJbZOBw577e0ll3WtWyr74zcK8z2KXtmFTF2\nPy8bh9l63QvaWvfu3Yvz589Pto/c0x4Zm5mrV6+m7a1+vqiRe6WtWmV1HnmPqaV1jEaeh408W8ju\nw2fPaCPyc0brGGbfPfL+1FNPPZUumz3jaR3f1jF88803J9tGntO1annw4MHJtpFz69mzZ9P2o0eP\nTra15oZs2db+Zu2tc/7IveCWkedhWZ9u9ctszms94x95vy4zMt+N2IjnYf5mYQAAAAAAAAAAAADY\nobwsDAAAAAAAAAAAAAA7lJeFAQAAAAAAAAAAAGCH8rIwAAAAAAAAAAAAAOxQXhYGAAAAAAAAAAAA\ngB3Ky8IAAAAAAAAAAAAAsEN5WRgAAAAAAAAAAAAAdqhSa928lZVyKSJ+8L6PnoyItzdtAx5tatVP\nrfqpVT+16qdW89msen201np8E9bDOmSgIWrVT636qVU/tZqPevWTgVaADDRErfqp1XzUq59a9VOr\nfjLQingoBxkj/dRqPurVT636qVU/teq3mbWSg7aQe0FD1KqfWvVTq35q1U+t5rOt7gVt6svCH1h5\nKd+utb6yZRvwCFGrfmrVT636qVU/tZqPeq0mx72fWvVTq35q1U+t5qNe/dRqNTnu/dSqn1rNR736\nqVU/teqnVqvJce+nVvNRr35q1U+t+qlVP7VaXY59P7Xqp1b91KqfWvVTq/lst3rt2uoNAAAAAAAA\nAAAAAACWw8vCAAAAAAAAAAAAALBDbfXLwr+2xet/lKhVP7Xqp1b91KqfWs1HvVaT495PrfqpVT+1\n6qdW81Gvfmq1mhz3fmrVT63mo1791KqfWvVTq9XkuPdTq/moVz+16qdW/dSqn1qtLse+n1r1U6t+\natVPrfqp1Xy2Vb1KrXWrtwEAAAAAAAAAAAAAWIKt/puFAQAAAAAAAAAAAIAl8bIwAAAAAAAAAAAA\nAOxQW/KycCnli6WU/1tK+ZNSyle2Yhu2s1LKr5dSLpZS/uh9nx0rpfxuKeX/zf7/+FZu43ZQSnmu\nlPJ7pZQ3Sin/u5TyS7PP1WodpZT9pZT/UUr5X7N6/ZPZ5y+UUl6f1evflVL2bfW2bgellN2llO+U\nUv7T7Gd1mlBKOV1K+cNSyndLKd+efWYcrqOUcrSU8o1Syv+ZzV1/Ua1WiwyUk4H6yUH9ZKD5yUF9\nZKB+MhARclBGBuonA/WTgeYnA/WRgfrJQETIQBkZqJ8M1E8Gmp8M1E8O6icHIQPl5KA+MtB85KD5\nyED9ZKB+j0IG2vSXhUspuyPiX0XEX4uIH42Iv11K+dHN3o5t7tWI+OJDn30lIr5Va/1YRHxr9vOq\nux8Rf6/W+lJEfDYifmHWl9RqfXci4vO11k9FxKcj4oullM9GxD+LiH8xq9eViPi5LdzG7eSXIuKN\n9/2sTrm/XGv9dK31ldnPxuH6fiUifqfW+mJEfCrW+pharQgZqMurIQP1koP6yUDzk4P6yUB9ZKAV\nJwc1vRoyUC8ZqJ8MND8ZqJ8M1EcGWnEyUNOrIQP1koH6yUDzk4HmIwf1kYNWmAzU5dWQg3rIQPOR\ng+YjA81HBuqz7TPQVvzNwn8hIv6k1vr9WuvdiPi3EfGlLdiObavW+t8i4vJDH38pIl6b/fq1iPip\nTd2obajWer7W+j9nv74eawPsmVCrddU1N2Y/7p39VyPi8xHxjdnn6hURpZRnI+JvRMRXZz+XUKd5\nGYcPKaUcjojPRcTXIiJqrXdrrVdDrVaJDNQgA/WTg/rJQPORg4YZgw+RgZiRgxIyUD8ZqJ8MNB8Z\naJgx+BAZiBkZKCED9ZOB+slA85GBNoRx+BA5iJCBmuSgPjLQfOSgfjLQhjAOH/KoZKCteFn4mYg4\n876fz84+I3ey1no+Yu2EGBEntnh7tpVSyvMR8ZmIeD3UatLsr9H/bkRcjIjfjYg/jYirtdb7s99i\nPK75lxHxDyLivdnPT4Q6ZWpE/OdSyh+UUn5+9plx+EGnIuJSRPzG7J+z+Gop5cOhVqtEBlqMMdIg\nB7XJQHORg/rJQH1kICLkoEUYIw0yUJsMNBcZqJ8M1EcGIkIGWoQx0iADtclAc5GB5iMH9ZGDkIEW\nY4wkZKA+clA3GWg+MlCfRyIDbcXLwmWdz+qmbwU7RinlYET8+4j4u7XWa1u9PdtZrfVBrfXTEfFs\nrP2JvpfW+22bu1XbSynlJyLiYq31D97/8Tq/daXr9JAfq7W+HGv/lMwvlFI+t9UbtE3tiYiXI+JX\na62fiYh3wz/FsGrMJWw4OaiPDNRHDpqbDNRHBiLCXMIGk4H6yEB9ZKC5yUB9ZCAizCVsMBmojwzU\nRwZaiBzURw7CXMKGkoH6yUFtMtBCZKA+j0QG2oqXhc9GxHPv+/nZiHhzC7bjUXOhlPJURMTs/xe3\neHu2hVLK3lgLBb9Za/0Ps4/VqmH215z/14j4bEQcLaXsmTUZjxE/FhE/WUo5HWv/JMrnY+1PFanT\nhFrrm7P/X4yI34q10GkcftDZiDhba3199vM3Yi0oqNXqkIEWY4xMkIPmJwM1yUFzkIG6yUBEyEGL\nMEYmyEDzk4GaZKA5yEDdZCAiZKBFGCMTZKD5yUBNMtCc5KBuchAy0GKMkXXIQIuRg1Iy0JxkoG6P\nRAbaipeFfz8iPlZKeaGUsi8i/lZEfHMLtuNR882I+PLs11+OiN/ewm3ZFkopJSK+FhFv1Fr/+fua\n1GodpZTjpZSjs18fiIi/EhFvRMTvRcTfnP22la9XrfUf1lqfrbU+H2vz03+ptf6dUKd1lVI+XEo5\n9Oe/joi/GhF/FMbhB9Ra34qIM6WUH5l99IWI+ONQq1UiAy3GGFmHHNRPBuonB/WTgfrJQMzIQfMz\nRtYhA/WTgfrJQP1koH4yEDMy0PyMkXXIQP1koH4y0HzkoH5yECEDLcoYeYgMNB85qI8MNB8ZqN+j\nkoFKrZv/t2aXUv56rL2Vvzsifr3W+subvhHbWCnl30TEj0fEkxFxISL+cUT8x4j4ekR8JCL+LCJ+\nutZ6eau2cTsopfyliPjvEfGHEfHe7ON/FBGvh1p9QCnlkxHxWqyNu10R8fVa6z8tpZyKtT8tcywi\nvhMRP1NrvbN1W7p9lFJ+PCL+fq31J9RpfbO6/Nbsxz0R8a9rrb9cSnkijMMPKKV8OiK+GhH7IuL7\nEfGzMRuPoVYrQQbKyUD95KB+MtBi5KCcDDQfGYgIOSgjA/WTgfrJQIuRgXIy0HxkICJkoIwM1E8G\n6icDLUYGapOD5iMHIQPl5KA+MtB85KD5yUBtMtB8HoUMtCUvCwMAAAAAAAAAAAAAy7drqzcAAAAA\nAAAAAAAAAFgOLwsDAAAAAAAAAAAAwA7lZWEAAAAAAAAAAAAA2KG8LAwAAAAAAAAAAAAAO5SXhQEA\nAAAAAAAAAABgh/KyMAAAAAAAAAAAAADsUF4WBgAAAAAAAAAAAIAd6v8Dqx/gGLQGQZcAAAAASUVO\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x236ddabb358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show training sample\n",
    "total_step = time_length\n",
    "idx =10\n",
    "print(\"label of {} th sample: {}\".format(idx, train_set[idx][1]) )\n",
    "f, axarr = plt.subplots(2,5, figsize=(50,20))\n",
    "for step in range(total_step):\n",
    "    axarr[step//5][step%5].imshow(np.squeeze(train_set[idx][0][step,:,:,:]), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear tf graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "time_length = 10\n",
    "width, height, channels = 64, 64, 1\n",
    "# make placeholders for data we'll feed in\n",
    "imgs = tf.placeholder(tf.float32, [None, time_length, width, height, channels])\n",
    "labels = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# define convolution layer\n",
    "kernel_shape, in_channel, out_channel = 5, 1, 8\n",
    "with tf.variable_scope('conv_kernel'):\n",
    "    w_shape = [kernel_shape, kernel_shape, in_channel, out_channel]\n",
    "    conv_w = tf.get_variable(name='conv_kernel', shape=w_shape,\n",
    "                             initializer=tf.glorot_uniform_initializer(seed=1234))\n",
    "conv_out = []\n",
    "for t in range(time_length):\n",
    "    tmp = tf.nn.conv2d(imgs[:,t,:,:,:], conv_w, strides=[1, 4, 4, 1], padding=\"SAME\")\n",
    "    tmp_flatten = tf.contrib.layers.flatten(tmp)\n",
    "    conv_out.append(tmp_flatten)\n",
    "conv_out = tf.stack(conv_out, axis=1)\n",
    "\n",
    "    \n",
    "# define the lstm cell\n",
    "hidden_size = 200\n",
    "lstm_cell = tf.contrib.rnn.LSTMCell(hidden_size)\n",
    "\n",
    "# define the op that runs the LSTM, across time, on the data\n",
    "batch_size = 2\n",
    "init_state = lstm_cell.zero_state(batch_size, tf.float32)\n",
    "lstm_outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, conv_out, initial_state=init_state, dtype=tf.float32)\n",
    "\n",
    "# output dense layer\n",
    "stage_num = 4\n",
    "out = tf.contrib.layers.fully_connected(final_state[-1], stage_num, activation_fn=None)\n",
    "# loss function\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=out)\n",
    "loss_mean = tf.reduce_mean(loss)\n",
    "# optimizer\n",
    "trainer = tf.train.AdamOptimizer()\n",
    "gradients = trainer.compute_gradients(loss_mean)\n",
    "gradients_clipped = [(tf.clip_by_value(t[0],-1,1),t[1]) for t in gradients]\n",
    "optimizer = trainer.apply_gradients(gradients_clipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 0: [[-0.23040813  0.2235131   0.38369185  0.22390381]\n",
      " [-0.15495509  0.98969078  0.10728611  0.10541344]]\n",
      "Minibatch train loss at step 0: [ 1.17725205  0.76320291]\n",
      "Minibatch train loss mean at step 0: 0.9702274799346924\n",
      "Minibatch train prediction at step 0: [2 1]\n",
      "Ground truth at step 0: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 1: [[-1.17549336  3.47151065  1.5872581  -1.61407721]\n",
      " [ 0.23965044  0.2550658   0.92317051 -0.2041602 ]]\n",
      "Minibatch train loss at step 1: [ 0.15505156  1.53427458]\n",
      "Minibatch train loss mean at step 1: 0.8446630835533142\n",
      "Minibatch train prediction at step 1: [1 2]\n",
      "Ground truth at step 1: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 2: [[-1.74270403  5.65132189  0.60440838 -2.19483519]\n",
      " [ 0.68301594 -1.36912405  1.80986071 -0.94416857]]\n",
      "Minibatch train loss at step 2: [ 0.00740782  0.35722172]\n",
      "Minibatch train loss mean at step 2: 0.18231476843357086\n",
      "Minibatch train prediction at step 2: [1 2]\n",
      "Ground truth at step 2: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 3: [[-1.90987849  6.59098482  0.11475682 -2.39297223]\n",
      " [-1.79688525  6.52351761 -0.27637395 -2.14737201]]\n",
      "Minibatch train loss at step 3: [  1.86650502e-03   8.32193089e+00]\n",
      "Minibatch train loss mean at step 3: 4.161898612976074\n",
      "Minibatch train prediction at step 3: [1 1]\n",
      "Ground truth at step 3: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 4: [[-1.09850132  5.42990398  0.93405902 -2.39783335]\n",
      " [-0.74054629  5.67749929  0.08536685 -2.14915967]]\n",
      "Minibatch train loss at step 4: [ 0.01293118  6.42378712]\n",
      "Minibatch train loss mean at step 4: 3.2183592319488525\n",
      "Minibatch train prediction at step 4: [1 1]\n",
      "Ground truth at step 4: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 5: [[-0.40906298  2.89530277  2.3603971  -2.81492281]\n",
      " [ 0.65091461  4.60406494  0.03545916 -1.98713899]]\n",
      "Minibatch train loss at step 5: [ 6.19620037  3.98362017]\n",
      "Minibatch train loss mean at step 5: 5.089910507202148\n",
      "Minibatch train prediction at step 5: [1 1]\n",
      "Ground truth at step 5: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 6: [[ 1.4862206   3.43911362  0.17960131 -1.83069909]\n",
      " [ 1.10078251  4.46781111  0.1965611  -2.18671179]]\n",
      "Minibatch train loss at step 6: [ 2.12298584  0.04854679]\n",
      "Minibatch train loss mean at step 6: 1.085766315460205\n",
      "Minibatch train prediction at step 6: [1 1]\n",
      "Ground truth at step 6: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 7: [[ 0.2825928   1.03555799  0.06430287 -0.80255234]\n",
      " [-0.55040425 -2.62179685  4.11706734 -1.41190851]]\n",
      "Minibatch train loss at step 7: [ 1.66873956  5.54342127]\n",
      "Minibatch train loss mean at step 7: 3.6060805320739746\n",
      "Minibatch train prediction at step 7: [1 2]\n",
      "Ground truth at step 7: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 8: [[ 2.33432984  3.34455276 -0.22475085 -1.8499583 ]\n",
      " [ 2.86063123  1.7993598  -0.10371204 -1.40650547]]\n",
      "Minibatch train loss at step 8: [ 0.33494261  0.34474635]\n",
      "Minibatch train loss mean at step 8: 0.3398444652557373\n",
      "Minibatch train prediction at step 8: [1 0]\n",
      "Ground truth at step 8: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 9: [[-0.17962387 -3.4443655   2.6586802   0.32596609]\n",
      " [ 0.8498556   0.27403361 -0.25198022 -0.86161911]]\n",
      "Minibatch train loss at step 9: [ 2.47922969  0.73001045]\n",
      "Minibatch train loss mean at step 9: 1.604620099067688\n",
      "Minibatch train prediction at step 9: [2 0]\n",
      "Ground truth at step 9: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 10: [[-2.02862    -1.34049225  5.29369974 -2.16178918]\n",
      " [ 3.6339395   0.88862866 -0.30583858 -1.15180624]]\n",
      "Minibatch train loss at step 10: [ 0.00255033  0.08803719]\n",
      "Minibatch train loss mean at step 10: 0.04529375955462456\n",
      "Minibatch train prediction at step 10: [2 0]\n",
      "Ground truth at step 10: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 11: [[ 3.6528008   0.74815655 -0.4029865  -1.12445521]\n",
      " [ 3.29322362  1.94892669 -0.37846681 -1.5593195 ]]\n",
      "Minibatch train loss at step 11: [ 0.0774324   1.60200799]\n",
      "Minibatch train loss mean at step 11: 0.8397201895713806\n",
      "Minibatch train prediction at step 11: [0 0]\n",
      "Ground truth at step 11: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 12: [[-1.61777067 -2.22456241  3.73998761 -0.13549556]\n",
      " [ 3.72053909  0.67694229 -0.45706609 -1.04662776]]\n",
      "Minibatch train loss at step 12: [ 0.02763847  0.06906204]\n",
      "Minibatch train loss mean at step 12: 0.0483502559363842\n",
      "Minibatch train prediction at step 12: [2 0]\n",
      "Ground truth at step 12: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 13: [[ 3.2746489   1.09551966  0.04482812 -0.76836169]\n",
      " [ 4.01981401  0.43288994 -0.46701348 -1.02000654]]\n",
      "Minibatch train loss at step 13: [ 2.33634591  0.04441352]\n",
      "Minibatch train loss mean at step 13: 1.1903797388076782\n",
      "Minibatch train prediction at step 13: [0 0]\n",
      "Ground truth at step 13: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 14: [[ 3.76099896  0.69288224 -0.57676917 -0.9193334 ]\n",
      " [ 3.11252189  1.343701   -0.02374537 -0.71668434]]\n",
      "Minibatch train loss at step 14: [ 0.06658357  1.98046279]\n",
      "Minibatch train loss mean at step 14: 1.023523211479187\n",
      "Minibatch train prediction at step 14: [0 0]\n",
      "Ground truth at step 14: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 15: [[ 0.09259456 -3.78827     1.16554344  2.24143529]\n",
      " [ 3.96271825  0.53007931 -0.48625711 -0.84469873]]\n",
      "Minibatch train loss at step 15: [ 1.45434129  0.05084612]\n",
      "Minibatch train loss mean at step 15: 0.7525936961174011\n",
      "Minibatch train prediction at step 15: [3 0]\n",
      "Ground truth at step 15: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 16: [[ 5.84468794 -1.92811275 -2.08770323  0.34101251]\n",
      " [-1.69932735 -1.65816677  4.57134485 -1.69317985]]\n",
      "Minibatch train loss at step 16: [ 0.00484009  0.00574746]\n",
      "Minibatch train loss mean at step 16: 0.005293772555887699\n",
      "Minibatch train prediction at step 16: [0 2]\n",
      "Ground truth at step 16: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 17: [[-0.36473569 -3.7659204   1.53193974  1.9727602 ]\n",
      " [-1.9372381  -2.58404183  4.13082886 -0.06726106]]\n",
      "Minibatch train loss at step 17: [ 0.99659824  0.01838254]\n",
      "Minibatch train loss mean at step 17: 0.5074903964996338\n",
      "Minibatch train prediction at step 17: [3 2]\n",
      "Ground truth at step 17: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 18: [[ 3.67583346  0.97605759 -0.54050928 -0.65766418]\n",
      " [ 1.2045871  -0.25055045  0.37023717  0.71384281]]\n",
      "Minibatch train loss at step 18: [ 0.09084082  1.3147862 ]\n",
      "Minibatch train loss mean at step 18: 0.7028135061264038\n",
      "Minibatch train prediction at step 18: [0 0]\n",
      "Ground truth at step 18: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 19: [[ 2.59407544  2.46855307 -0.21119781 -0.57908928]\n",
      " [ 2.51781368  3.01793218 -0.51467311 -1.01475573]]\n",
      "Minibatch train loss at step 19: [ 0.81083751  0.50284231]\n",
      "Minibatch train loss mean at step 19: 0.656839907169342\n",
      "Minibatch train prediction at step 19: [0 1]\n",
      "Ground truth at step 19: [1 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 20: [[ 5.85773039 -1.31308174 -2.13158607 -1.00021446]\n",
      " [-2.02342057 -2.60784841  4.38178682 -0.26419818]]\n",
      "Minibatch train loss at step 20: [ 0.00215643  0.01210086]\n",
      "Minibatch train loss mean at step 20: 0.007128645665943623\n",
      "Minibatch train prediction at step 20: [0 2]\n",
      "Ground truth at step 20: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 21: [[ 2.07442164  3.45516181 -0.39422598 -0.76732218]\n",
      " [-1.1401943  -3.16848278  2.5456655   1.32657123]]\n",
      "Minibatch train loss at step 21: [ 0.25258377  0.28056094]\n",
      "Minibatch train loss mean at step 21: 0.26657235622406006\n",
      "Minibatch train prediction at step 21: [1 2]\n",
      "Ground truth at step 21: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 22: [[ 5.94329309 -1.32756662 -2.19607806 -0.99960196]\n",
      " [-1.33629835 -3.00685453  2.75478601  1.13877022]]\n",
      "Minibatch train loss at step 22: [ 0.00195086  0.19766639]\n",
      "Minibatch train loss mean at step 22: 0.09980862587690353\n",
      "Minibatch train prediction at step 22: [0 2]\n",
      "Ground truth at step 22: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 23: [[ 0.14577615 -3.81273246  0.81892896  2.74255323]\n",
      " [ 2.64395285  1.94566226 -0.50601327 -0.2095125 ]]\n",
      "Minibatch train loss at step 23: [ 0.2004991   0.46871072]\n",
      "Minibatch train loss mean at step 23: 0.3346049189567566\n",
      "Minibatch train prediction at step 23: [3 0]\n",
      "Ground truth at step 23: [3 0]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 24: [[ 3.08271408  1.63157713 -0.48852447 -0.11009713]\n",
      " [ 1.88195801  3.4490037  -0.35141492 -0.50553608]]\n",
      "Minibatch train loss at step 24: [ 0.26503837  0.22329527]\n",
      "Minibatch train loss mean at step 24: 0.24416682124137878\n",
      "Minibatch train prediction at step 24: [0 1]\n",
      "Ground truth at step 24: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 25: [[ 0.19188002 -0.15188412  0.37970987  1.92430842]\n",
      " [ 1.57253933  4.09455204 -0.47412646 -0.70075017]]\n",
      "Minibatch train loss at step 25: [ 0.41585016  0.09434425]\n",
      "Minibatch train loss mean at step 25: 0.2550972104072571\n",
      "Minibatch train prediction at step 25: [3 1]\n",
      "Ground truth at step 25: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 26: [[-2.13083816 -2.63221169  4.74457884 -0.58862185]\n",
      " [ 1.50846398  4.2251749  -0.51990253 -0.7013064 ]]\n",
      "Minibatch train loss at step 26: [ 0.00646606  0.07884645]\n",
      "Minibatch train loss mean at step 26: 0.04265625774860382\n",
      "Minibatch train prediction at step 26: [2 1]\n",
      "Ground truth at step 26: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 27: [[-1.98530459 -1.60788441  4.42684364 -1.50307453]\n",
      " [ 2.95900369  1.88966227 -0.66502446 -0.18886188]]\n",
      "Minibatch train loss at step 27: [ 0.00667201  0.34561124]\n",
      "Minibatch train loss mean at step 27: 0.176141619682312\n",
      "Minibatch train prediction at step 27: [2 0]\n",
      "Ground truth at step 27: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 28: [[ 0.0271645  -0.2085983   0.35403398  2.37312961]\n",
      " [-0.2232632  -2.72008491  1.80784225  1.16016912]]\n",
      "Minibatch train loss at step 28: [ 0.26556948  2.54108405]\n",
      "Minibatch train loss mean at step 28: 1.4033267498016357\n",
      "Minibatch train prediction at step 28: [3 2]\n",
      "Ground truth at step 28: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 29: [[-1.39169109 -2.69878316  3.05873847  0.88817155]\n",
      " [ 0.02957423 -0.22146557  0.29439396  2.50217056]]\n",
      "Minibatch train loss at step 29: [ 0.12128388  0.23106885]\n",
      "Minibatch train loss mean at step 29: 0.17617636919021606\n",
      "Minibatch train prediction at step 29: [2 3]\n",
      "Ground truth at step 29: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 30: [[ 2.70543313  2.21243596 -0.79991025 -0.21351327]\n",
      " [ 1.35786974  3.95051479 -0.4425399  -0.48049861]]\n",
      "Minibatch train loss at step 30: [ 0.52757651  0.09448005]\n",
      "Minibatch train loss mean at step 30: 0.31102827191352844\n",
      "Minibatch train prediction at step 30: [0 1]\n",
      "Ground truth at step 30: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 31: [[ 2.32010055 -2.00810575  0.01697616  0.23426689]\n",
      " [ 1.35636854  3.9969964  -0.47775236 -0.46341205]]\n",
      "Minibatch train loss at step 31: [ 0.21296456  0.09008496]\n",
      "Minibatch train loss mean at step 31: 0.15152476727962494\n",
      "Minibatch train prediction at step 31: [0 1]\n",
      "Ground truth at step 31: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 32: [[-1.04949462 -2.61499619  2.96937585  0.67871362]\n",
      " [ 6.13661289 -1.34619927 -2.36335611 -1.01636434]]\n",
      "Minibatch train loss at step 32: [ 0.11594022  0.00154745]\n",
      "Minibatch train loss mean at step 32: 0.058743834495544434\n",
      "Minibatch train prediction at step 32: [2 0]\n",
      "Ground truth at step 32: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 33: [[ 3.32956553  1.88601303 -0.91019911 -0.13111041]\n",
      " [ 1.38346684  4.81208181 -0.79748845 -0.66854101]]\n",
      "Minibatch train loss at step 33: [ 0.24834888  0.03947179]\n",
      "Minibatch train loss mean at step 33: 0.14391033351421356\n",
      "Minibatch train prediction at step 33: [0 1]\n",
      "Ground truth at step 33: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 34: [[ 3.43129015  1.80203438 -0.94788116 -0.12160107]\n",
      " [ 6.16881657 -1.34229994 -2.38070345 -1.02894521]]\n",
      "Minibatch train loss at step 34: [ 0.21289347  0.0014877 ]\n",
      "Minibatch train loss mean at step 34: 0.1071905866265297\n",
      "Minibatch train prediction at step 34: [0 0]\n",
      "Ground truth at step 34: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 35: [[-0.93650907 -2.63336253  2.8903029   0.29913798]\n",
      " [ 3.55980015  1.68451929 -0.98435086 -0.11734036]]\n",
      "Minibatch train loss at step 35: [ 0.09594896  0.17331132]\n",
      "Minibatch train loss mean at step 35: 0.13463014364242554\n",
      "Minibatch train prediction at step 35: [2 0]\n",
      "Ground truth at step 35: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 36: [[-1.24988151 -2.58606958  3.48458314 -0.18333828]\n",
      " [ 3.72373462  1.55843604 -0.93917739 -0.05046096]]\n",
      "Minibatch train loss at step 36: [ 0.03597154  0.13724594]\n",
      "Minibatch train loss mean at step 36: 0.08660873770713806\n",
      "Minibatch train prediction at step 36: [2 0]\n",
      "Ground truth at step 36: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 37: [[ 6.23670769 -1.34384942 -2.3965199  -1.05471873]\n",
      " [ 0.70532614 -3.62605548  0.57420778  2.18082142]]\n",
      "Minibatch train loss at step 37: [ 0.0013689   0.35923815]\n",
      "Minibatch train loss mean at step 37: 0.18030352890491486\n",
      "Minibatch train prediction at step 37: [0 3]\n",
      "Ground truth at step 37: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 38: [[ 5.87827492 -1.8786279  -2.06221056  0.12191454]\n",
      " [ 0.69152534 -3.60403252  0.59661549  2.34957361]]\n",
      "Minibatch train loss at step 38: [ 0.00393866  0.31215581]\n",
      "Minibatch train loss mean at step 38: 0.15804722905158997\n",
      "Minibatch train prediction at step 38: [0 3]\n",
      "Ground truth at step 38: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 39: [[ 6.27570963 -1.34837365 -2.40199947 -1.06530356]\n",
      " [-2.04781079 -2.65725541  5.02556086 -1.00384128]]\n",
      "Minibatch train loss at step 39: [ 0.0013064   0.00370815]\n",
      "Minibatch train loss mean at step 39: 0.0025072749704122543\n",
      "Minibatch train prediction at step 39: [0 2]\n",
      "Ground truth at step 39: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 40: [[ 1.74268973  4.95342731 -0.93353659 -0.69686937]\n",
      " [ 4.21853924  1.31407499 -1.10267401 -0.19727577]]\n",
      "Minibatch train loss at step 40: [ 0.04556468  0.06929254]\n",
      "Minibatch train loss mean at step 40: 0.05742860957980156\n",
      "Minibatch train prediction at step 40: [1 0]\n",
      "Ground truth at step 40: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 41: [[ 0.27123061 -0.49123266 -0.19753419  3.53334069]\n",
      " [ 1.79461038  4.94815111 -0.95005429 -0.70239389]]\n",
      "Minibatch train loss at step 41: [ 0.07710026  0.04779974]\n",
      "Minibatch train loss mean at step 41: 0.062449999153614044\n",
      "Minibatch train prediction at step 41: [3 1]\n",
      "Ground truth at step 41: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 42: [[-1.23319483 -1.34825504  3.73353148 -1.50798595]\n",
      " [ 0.27884296 -0.51545316 -0.21222681  3.58944225]]\n",
      "Minibatch train loss at step 42: [ 0.01829851  0.07261785]\n",
      "Minibatch train loss mean at step 42: 0.04545817896723747\n",
      "Minibatch train prediction at step 42: [2 3]\n",
      "Ground truth at step 42: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 43: [[-0.81397724 -2.74268413  3.07043314  0.42768773]\n",
      " [ 4.48352146  1.17117119 -1.15689981 -0.24189217]]\n",
      "Minibatch train loss at step 43: [ 0.09049286  0.04769325]\n",
      "Minibatch train loss mean at step 43: 0.06909304857254028\n",
      "Minibatch train prediction at step 43: [2 0]\n",
      "Ground truth at step 43: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 44: [[-2.09519744 -2.68757725  5.09178066 -1.02191329]\n",
      " [ 4.53370571  1.15765345 -1.09250343 -0.17164165]]\n",
      "Minibatch train loss at step 44: [ 0.00338137  0.04576752]\n",
      "Minibatch train loss mean at step 44: 0.024574443697929382\n",
      "Minibatch train prediction at step 44: [2 0]\n",
      "Ground truth at step 44: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 45: [[-1.2358222  -2.68818831  3.68337941 -0.15403925]\n",
      " [ 4.3562727  -1.95024288 -1.55090809 -0.39062008]]\n",
      "Minibatch train loss at step 45: [ 0.03010583  0.01313627]\n",
      "Minibatch train loss mean at step 45: 0.02162105217576027\n",
      "Minibatch train prediction at step 45: [2 0]\n",
      "Ground truth at step 45: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 46: [[ 0.31790185 -3.48261833  0.5352574   3.37918043]\n",
      " [-1.30866146 -1.5037756   4.03371572 -1.52012277]]\n",
      "Minibatch train loss at step 46: [ 0.10081474  0.01251475]\n",
      "Minibatch train loss mean at step 46: 0.05666474252939224\n",
      "Minibatch train prediction at step 46: [3 2]\n",
      "Ground truth at step 46: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 47: [[ 1.76341534  3.89574218 -0.80898619 -0.44851443]\n",
      " [-2.1348443  -2.70103621  5.14557934 -1.04380512]]\n",
      "Minibatch train loss at step 47: [ 0.13154981  0.00312614]\n",
      "Minibatch train loss mean at step 47: 0.06733797490596771\n",
      "Minibatch train prediction at step 47: [1 2]\n",
      "Ground truth at step 47: [1 2]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 48: [[ 5.97639275 -1.85417283 -2.1632812   0.10147496]\n",
      " [ 1.99570847  4.79168844 -1.09401584 -0.67337799]]\n",
      "Minibatch train loss at step 48: [ 0.00349209  0.06584954]\n",
      "Minibatch train loss mean at step 48: 0.034670814871788025\n",
      "Minibatch train prediction at step 48: [0 1]\n",
      "Ground truth at step 48: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 49: [[-1.43187749 -2.78044915  3.77378249 -0.11442173]\n",
      " [ 6.40065718 -1.3691237  -2.41020894 -1.09990919]]\n",
      "Minibatch train loss at step 49: [ 0.02702312  0.00112363]\n",
      "Minibatch train loss mean at step 49: 0.014073374681174755\n",
      "Minibatch train prediction at step 49: [2 0]\n",
      "Ground truth at step 49: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 50: [[ 4.8664341   1.02256095 -1.18570554 -0.25966394]\n",
      " [ 4.90246105  0.96159917 -1.26351297 -0.34996083]]\n",
      "Minibatch train loss at step 50: [ 0.02927043  0.026414  ]\n",
      "Minibatch train loss mean at step 50: 0.027842216193675995\n",
      "Minibatch train prediction at step 50: [0 0]\n",
      "Ground truth at step 50: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 51: [[ 1.65405548  4.0810976  -0.81652164 -0.46373516]\n",
      " [ 4.58682251 -1.96640086 -1.68168437 -0.42219922]]\n",
      "Minibatch train loss at step 51: [ 0.10109686  0.00994832]\n",
      "Minibatch train loss mean at step 51: 0.05552258715033531\n",
      "Minibatch train prediction at step 51: [1 0]\n",
      "Ground truth at step 51: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 52: [[-2.19070482 -2.71440315  5.23387432 -1.07989275]\n",
      " [ 1.6004349   4.14886045 -0.81458271 -0.47147217]]\n",
      "Minibatch train loss at step 52: [ 0.00275708  0.09079358]\n",
      "Minibatch train loss mean at step 52: 0.04677532985806465\n",
      "Minibatch train prediction at step 52: [2 1]\n",
      "Ground truth at step 52: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 53: [[ 1.52844059  4.23365593 -0.81367338 -0.48760384]\n",
      " [ 6.42831182 -1.3728236  -2.41146636 -1.1115576 ]]\n",
      "Minibatch train loss at step 53: [ 0.07898338  0.00108493]\n",
      "Minibatch train loss mean at step 53: 0.040034156292676926\n",
      "Minibatch train prediction at step 53: [1 0]\n",
      "Ground truth at step 53: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 54: [[ 0.05135988 -3.56718755  0.49155977  3.9087925 ]\n",
      " [ 6.02838802 -1.84432185 -2.19514394  0.07869298]]\n",
      "Minibatch train loss at step 54: [ 0.0530589   0.00325056]\n",
      "Minibatch train loss mean at step 54: 0.028154728934168816\n",
      "Minibatch train prediction at step 54: [3 0]\n",
      "Ground truth at step 54: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 55: [[ 1.38085103  4.40479088 -0.82062876 -0.5442574 ]\n",
      " [ 1.8687346   5.22577333 -1.18721437 -0.73100066]]\n",
      "Minibatch train loss at step 55: [ 0.0592851   0.03832288]\n",
      "Minibatch train loss mean at step 55: 0.048803992569446564\n",
      "Minibatch train prediction at step 55: [1 1]\n",
      "Ground truth at step 55: [1 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 56: [[ 0.16963641 -0.69655138 -0.20122303  4.1707077 ]\n",
      " [ 4.72037172 -1.97480786 -1.750651   -0.45450976]]\n",
      "Minibatch train loss at step 56: [ 0.03789026  0.00840602]\n",
      "Minibatch train loss mean at step 56: 0.02314814180135727\n",
      "Minibatch train prediction at step 56: [3 0]\n",
      "Ground truth at step 56: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 57: [[-1.47111881 -2.75346303  3.41007876  0.72088033]\n",
      " [ 4.74523973 -1.97628605 -1.76246154 -0.46237984]]\n",
      "Minibatch train loss at step 57: [ 0.07476234  0.00813812]\n",
      "Minibatch train loss mean at step 57: 0.04145022854208946\n",
      "Minibatch train prediction at step 57: [2 0]\n",
      "Ground truth at step 57: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 58: [[-1.5651505  -2.71929979  4.0703845  -0.23124214]\n",
      " [ 6.06649494 -1.83785129 -2.21011305  0.05880912]]\n",
      "Minibatch train loss at step 58: [ 0.01807621  0.0030786 ]\n",
      "Minibatch train loss mean at step 58: 0.01057740580290556\n",
      "Minibatch train prediction at step 58: [2 0]\n",
      "Ground truth at step 58: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 59: [[ 6.0777564  -1.83780277 -2.20902801  0.04934693]\n",
      " [-1.48878181 -2.73169374  3.75666595  0.46401981]]\n",
      "Minibatch train loss at step 59: [ 0.00302156  0.04300962]\n",
      "Minibatch train loss mean at step 59: 0.023015586659312248\n",
      "Minibatch train prediction at step 59: [0 2]\n",
      "Ground truth at step 59: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 60: [[-1.40430593 -1.59822595  4.40322113 -1.51158321]\n",
      " [-1.55553424 -2.70804858  3.83580661  0.43201864]]\n",
      "Minibatch train loss at step 60: [ 0.00814592  0.03849151]\n",
      "Minibatch train loss mean at step 60: 0.023318715393543243\n",
      "Minibatch train prediction at step 60: [2 2]\n",
      "Ground truth at step 60: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 61: [[ 5.15969992  1.01880682 -1.43544328 -0.52141738]\n",
      " [ 6.45303106 -1.36816823 -2.41278362 -1.13217902]]\n",
      "Minibatch train loss at step 61: [ 0.02047428  0.00104968]\n",
      "Minibatch train loss mean at step 61: 0.010761980898678303\n",
      "Minibatch train prediction at step 61: [0 0]\n",
      "Ground truth at step 61: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 62: [[ 0.08020264 -0.70686543 -0.15427287  4.31370592]\n",
      " [ 5.10988903  1.15165758 -1.35855961 -0.4233599 ]]\n",
      "Minibatch train loss at step 62: [ 0.0320535   0.02430383]\n",
      "Minibatch train loss mean at step 62: 0.028178665786981583\n",
      "Minibatch train prediction at step 62: [3 0]\n",
      "Ground truth at step 62: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 63: [[-1.42629695 -1.62957799  4.51321793 -1.54037309]\n",
      " [ 4.89427757 -1.98414826 -1.78923619 -0.54946715]]\n",
      "Minibatch train loss at step 63: [ 0.00710627  0.0065826 ]\n",
      "Minibatch train loss mean at step 63: 0.006844433955848217\n",
      "Minibatch train prediction at step 63: [2 0]\n",
      "Ground truth at step 63: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 64: [[-0.19036227 -3.61261249  0.48264307  4.19859552]\n",
      " [ 1.6123246   4.79534006 -1.10532498 -0.6661821 ]]\n",
      "Minibatch train loss at step 64: [ 0.03647763  0.04730821]\n",
      "Minibatch train loss mean at step 64: 0.041892919689416885\n",
      "Minibatch train prediction at step 64: [3 1]\n",
      "Ground truth at step 64: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 65: [[-1.70260429 -2.68876386  4.08319712  0.26253808]\n",
      " [-1.71139908 -2.72134924  4.34934711 -0.42229891]]\n",
      "Minibatch train loss at step 65: [ 0.02579417  0.01158145]\n",
      "Minibatch train loss mean at step 65: 0.018687812611460686\n",
      "Minibatch train prediction at step 65: [2 2]\n",
      "Ground truth at step 65: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 66: [[ 6.4676199  -1.36997104 -2.41237879 -1.14465308]\n",
      " [-1.72510004 -2.68931413  4.12554169  0.23249018]]\n",
      "Minibatch train loss at step 66: [ 0.00102753  0.02406646]\n",
      "Minibatch train loss mean at step 66: 0.012546995654702187\n",
      "Minibatch train prediction at step 66: [0 2]\n",
      "Ground truth at step 66: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 67: [[-0.29440263 -3.57143688  0.4590885   4.23558998]\n",
      " [-1.75282907 -2.72308731  4.4280405  -0.47585273]]\n",
      "Minibatch train loss at step 67: [ 0.03352208  0.01021786]\n",
      "Minibatch train loss mean at step 67: 0.021869970485568047\n",
      "Minibatch train prediction at step 67: [3 2]\n",
      "Ground truth at step 67: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 68: [[ 5.18393803  1.185063   -1.43126559 -0.48104733]\n",
      " [-1.46015763 -1.7062676   4.70294476 -1.5903064 ]]\n",
      "Minibatch train loss at step 68: [ 0.02287757  0.00558507]\n",
      "Minibatch train loss mean at step 68: 0.014231315813958645\n",
      "Minibatch train prediction at step 68: [0 2]\n",
      "Ground truth at step 68: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 69: [[-0.38339537 -3.53931046  0.49315882  4.26826715]\n",
      " [ 1.4623692   4.95998287 -1.09651566 -0.73274082]]\n",
      "Minibatch train loss at step 69: [ 0.03235777  0.03535024]\n",
      "Minibatch train loss mean at step 69: 0.033854007720947266\n",
      "Minibatch train prediction at step 69: [3 1]\n",
      "Ground truth at step 69: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 70: [[-0.43741626 -3.51413989  0.48903453  4.33127832]\n",
      " [ 5.02683735 -1.9953078  -1.8026222  -0.62980342]]\n",
      "Minibatch train loss at step 70: [ 0.02987758  0.00545276]\n",
      "Minibatch train loss mean at step 70: 0.01766517013311386\n",
      "Minibatch train prediction at step 70: [3 0]\n",
      "Ground truth at step 70: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 71: [[ 5.29223394  1.0357672  -1.547297   -0.62386835]\n",
      " [-0.48927855 -3.48825979  0.47003955  4.41082668]]\n",
      "Minibatch train loss at step 71: [ 0.01777961  0.02688515]\n",
      "Minibatch train loss mean at step 71: 0.022332381457090378\n",
      "Minibatch train prediction at step 71: [0 3]\n",
      "Ground truth at step 71: [0 3]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 72: [[-0.02230977 -0.74664724 -0.08839084  4.50178576]\n",
      " [ 4.73838091  1.65760767 -1.56216538 -0.53836924]]\n",
      "Minibatch train loss at step 72: [ 0.02591266  0.05151788]\n",
      "Minibatch train loss mean at step 72: 0.03871527314186096\n",
      "Minibatch train prediction at step 72: [3 0]\n",
      "Ground truth at step 72: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 73: [[ 6.21920395 -1.85685682 -2.15769815 -0.07197788]\n",
      " [ 5.07935524 -2.00388026 -1.81122231 -0.6540485 ]]\n",
      "Minibatch train loss at step 73: [ 0.00239062  0.00507946]\n",
      "Minibatch train loss mean at step 73: 0.0037350421771407127\n",
      "Minibatch train prediction at step 73: [0 0]\n",
      "Ground truth at step 73: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 74: [[ 5.29229403  1.14442444 -1.50369895 -0.5598489 ]\n",
      " [ 1.33629525  5.07916307 -1.07863963 -0.80263489]]\n",
      "Minibatch train loss at step 74: [ 0.01959689  0.02819156]\n",
      "Minibatch train loss mean at step 74: 0.023894228041172028\n",
      "Minibatch train prediction at step 74: [0 1]\n",
      "Ground truth at step 74: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 75: [[-1.89695382 -2.74503565  4.64292717 -0.57574642]\n",
      " [ 5.32223701  1.1249783  -1.51708245 -0.57927364]]\n",
      "Minibatch train loss at step 75: [ 0.00744994  0.01866749]\n",
      "Minibatch train loss mean at step 75: 0.01305871456861496\n",
      "Minibatch train prediction at step 75: [2 0]\n",
      "Ground truth at step 75: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 76: [[-1.55906999 -1.79125869  4.91234159 -1.65080154]\n",
      " [ 0.7434743   5.25166464 -0.87763977 -0.89093137]]\n",
      "Minibatch train loss at step 76: [ 0.00417623  0.01522926]\n",
      "Minibatch train loss mean at step 76: 0.009702742099761963\n",
      "Minibatch train prediction at step 76: [2 1]\n",
      "Ground truth at step 76: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 77: [[-0.04272934 -0.78731686 -0.08004748  4.59907293]\n",
      " [ 1.26919019  5.13789797 -1.06489289 -0.85377008]]\n",
      "Minibatch train loss at step 77: [ 0.02323392  0.02509119]\n",
      "Minibatch train loss mean at step 77: 0.024162553250789642\n",
      "Minibatch train prediction at step 77: [3 1]\n",
      "Ground truth at step 77: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 78: [[ 6.48495817 -1.38504565 -2.40686083 -1.17475998]\n",
      " [-2.37402105 -2.76281667  5.63304996 -1.35875857]]\n",
      "Minibatch train loss at step 78: [ 0.00099062  0.0014771 ]\n",
      "Minibatch train loss mean at step 78: 0.0012338594533503056\n",
      "Minibatch train prediction at step 78: [0 2]\n",
      "Ground truth at step 78: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 79: [[-1.94702554 -2.7549417   4.70818424 -0.59837419]\n",
      " [ 0.69532681  5.31956291 -0.87922037 -0.92487133]]\n",
      "Minibatch train loss at step 79: [ 0.00679705  0.01369015]\n",
      "Minibatch train loss mean at step 79: 0.010243600234389305\n",
      "Minibatch train prediction at step 79: [2 1]\n",
      "Ground truth at step 79: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 80: [[-0.6349299  -3.45078301  0.30970129  4.80253792]\n",
      " [ 6.27945232 -1.86274087 -2.15436721 -0.10466431]]\n",
      "Minibatch train loss at step 80: [ 0.01567624  0.00219414]\n",
      "Minibatch train loss mean at step 80: 0.008935192599892616\n",
      "Minibatch train prediction at step 80: [3 0]\n",
      "Ground truth at step 80: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 81: [[-1.65539789 -1.83617377  5.03748941 -1.69866133]\n",
      " [ 5.4680686   1.0486623  -1.58067334 -0.67116147]]\n",
      "Minibatch train loss at step 81: [ 0.00345562  0.01495413]\n",
      "Minibatch train loss mean at step 81: 0.009204876609146595\n",
      "Minibatch train prediction at step 81: [2 0]\n",
      "Ground truth at step 81: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 82: [[-0.64751637 -3.45888138  0.27707335  4.86499453]\n",
      " [ 0.64952606  5.38627958 -0.88314354 -0.95647597]]\n",
      "Minibatch train loss at step 82: [ 0.01434913  0.01234321]\n",
      "Minibatch train loss mean at step 82: 0.013346171006560326\n",
      "Minibatch train prediction at step 82: [3 1]\n",
      "Ground truth at step 82: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 83: [[ 6.30289555 -1.86359632 -2.15334535 -0.11827763]\n",
      " [-0.65575403 -3.46587324  0.25511736  4.90206671]]\n",
      "Minibatch train loss at step 83: [ 0.00212099  0.01358749]\n",
      "Minibatch train loss mean at step 83: 0.007854238152503967\n",
      "Minibatch train prediction at step 83: [0 3]\n",
      "Ground truth at step 83: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 84: [[-1.74755478 -1.87480211  5.14381886 -1.75396931]\n",
      " [ 1.09924138  5.30458403 -1.04658234 -0.96860027]]\n",
      "Minibatch train loss at step 84: [ 0.00291732  0.01837681]\n",
      "Minibatch train loss mean at step 84: 0.010647065006196499\n",
      "Minibatch train prediction at step 84: [2 1]\n",
      "Ground truth at step 84: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 85: [[-1.78943622 -1.89141011  5.19022179 -1.77979422]\n",
      " [-2.02687287 -2.71415305  4.49492836  0.05694533]]\n",
      "Minibatch train loss at step 85: [ 0.00270691  0.01393306]\n",
      "Minibatch train loss mean at step 85: 0.008319986052811146\n",
      "Minibatch train prediction at step 85: [2 2]\n",
      "Ground truth at step 85: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 86: [[ 5.24254465 -2.04263592 -1.83841312 -0.73588985]\n",
      " [-0.67796588 -3.48665237  0.18967189  5.01239157]]\n",
      "Minibatch train loss at step 86: [ 0.00405122  0.01155988]\n",
      "Minibatch train loss mean at step 86: 0.007805550936609507\n",
      "Minibatch train prediction at step 86: [0 3]\n",
      "Ground truth at step 86: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 87: [[-2.02448463 -2.78151131  4.80195284 -0.61736989]\n",
      " [-1.8714788  -1.92405939  5.28094053 -1.82749069]]\n",
      "Minibatch train loss at step 87: [ 0.00600558  0.00234127]\n",
      "Minibatch train loss mean at step 87: 0.004173422697931528\n",
      "Minibatch train prediction at step 87: [2 2]\n",
      "Ground truth at step 87: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 88: [[-2.4146297  -2.77522206  5.70291281 -1.39150023]\n",
      " [-2.0557766  -2.72263861  4.52396917  0.04585254]]\n",
      "Minibatch train loss at step 88: [ 0.00133509  0.01336591]\n",
      "Minibatch train loss mean at step 88: 0.00735049694776535\n",
      "Minibatch train prediction at step 88: [2 2]\n",
      "Ground truth at step 88: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 89: [[ 5.27349949 -2.05149674 -1.84090459 -0.75372773]\n",
      " [-2.41848564 -2.77662468  5.71257544 -1.3975991 ]]\n",
      "Minibatch train loss at step 89: [ 0.00387679  0.0013158 ]\n",
      "Minibatch train loss mean at step 89: 0.002596295904368162\n",
      "Minibatch train prediction at step 89: [0 2]\n",
      "Ground truth at step 89: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 90: [[ 5.28528595 -2.05497837 -1.84117663 -0.76159465]\n",
      " [-2.04870534 -2.7958715   4.84627342 -0.63619709]]\n",
      "Minibatch train loss at step 90: [ 0.00381041  0.0056358 ]\n",
      "Minibatch train loss mean at step 90: 0.004723105113953352\n",
      "Minibatch train prediction at step 90: [0 2]\n",
      "Ground truth at step 90: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 91: [[-0.08296966 -0.8673411  -0.08280487  4.822474  ]\n",
      " [-0.70444173 -3.51085138  0.12000554  5.14712906]]\n",
      "Minibatch train loss at step 91: [ 0.0180302   0.00956087]\n",
      "Minibatch train loss mean at step 91: 0.013795532286167145\n",
      "Minibatch train prediction at step 91: [3 3]\n",
      "Ground truth at step 91: [3 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 92: [[ 5.31201792 -2.06309056 -1.84173536 -0.77988601]\n",
      " [-1.97796321 -1.96014857  5.40543127 -1.8681407 ]]\n",
      "Minibatch train loss at step 92: [ 0.0036629   0.00194599]\n",
      "Minibatch train loss mean at step 92: 0.0028044439386576414\n",
      "Minibatch train prediction at step 92: [0 2]\n",
      "Ground truth at step 92: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 93: [[ 6.37358332 -1.86423945 -2.14660859 -0.16295154]\n",
      " [ 5.606287    1.00974083 -1.64843845 -0.75562459]]\n",
      "Minibatch train loss at step 93: [ 0.00191148  0.01244176]\n",
      "Minibatch train loss mean at step 93: 0.007176619488745928\n",
      "Minibatch train prediction at step 93: [0 0]\n",
      "Ground truth at step 93: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 94: [[ 0.93566388  5.47425508 -1.04175556 -1.05703628]\n",
      " [ 5.34053993 -2.07213759 -1.84260356 -0.79917514]]\n",
      "Minibatch train loss at step 94: [ 0.01353304  0.00351217]\n",
      "Minibatch train loss mean at step 94: 0.008522603660821915\n",
      "Minibatch train prediction at step 94: [1 0]\n",
      "Ground truth at step 94: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 95: [[-2.44064307 -2.78380704  5.76682329 -1.43173373]\n",
      " [-2.08174491 -2.82018423  4.91370583 -0.66535968]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train loss at step 95: [ 0.00121293  0.0051167 ]\n",
      "Minibatch train loss mean at step 95: 0.003164817811921239\n",
      "Minibatch train prediction at step 95: [2 2]\n",
      "Ground truth at step 95: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 96: [[ 1.32523119  6.03927803 -1.305691   -0.955836  ]\n",
      " [ 6.39482832 -1.86427283 -2.14310741 -0.178055  ]]\n",
      "Minibatch train loss at step 96: [ 0.01047553  0.0018508 ]\n",
      "Minibatch train loss mean at step 96: 0.006163164507597685\n",
      "Minibatch train prediction at step 96: [1 0]\n",
      "Ground truth at step 96: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 97: [[-2.04771996 -1.97255063  5.49524307 -1.8767904 ]\n",
      " [-0.09340362 -0.8987813  -0.09504595  4.90903568]]\n",
      "Minibatch train loss at step 97: [ 0.00172811  0.01630237]\n",
      "Minibatch train loss mean at step 97: 0.00901524256914854\n",
      "Minibatch train prediction at step 97: [2 3]\n",
      "Ground truth at step 97: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 98: [[-0.09600584 -0.90509123 -0.09905425  4.9279213 ]\n",
      " [ 5.64410925  1.00105214 -1.66726136 -0.77531046]]\n",
      "Minibatch train loss at step 98: [ 0.01593885  0.01185515]\n",
      "Minibatch train loss mean at step 98: 0.013896998018026352\n",
      "Minibatch train prediction at step 98: [3 0]\n",
      "Ground truth at step 98: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 99: [[ 5.40598249 -2.09289885 -1.84477937 -0.84460616]\n",
      " [-0.09848204 -0.91406113 -0.1059396   4.95352411]]\n",
      "Minibatch train loss at step 99: [ 0.00318758  0.01545512]\n",
      "Minibatch train loss mean at step 99: 0.009321349672973156\n",
      "Minibatch train prediction at step 99: [0 3]\n",
      "Ground truth at step 99: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 100: [[ 5.64186239  0.89476144 -1.72723484 -0.82781059]\n",
      " [-0.10153313 -0.92447954 -0.11485869  4.98440409]]\n",
      "Minibatch train loss at step 100: [ 0.01079835  0.01488813]\n",
      "Minibatch train loss mean at step 100: 0.012843241915106773\n",
      "Minibatch train prediction at step 100: [0 3]\n",
      "Ground truth at step 100: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 101: [[ 0.86267924  5.55792189 -1.0495832  -1.08348405]\n",
      " [ 5.67404652  0.98869979 -1.68087125 -0.78533202]]\n",
      "Minibatch train loss at step 101: [ 0.01172508  0.01136991]\n",
      "Minibatch train loss mean at step 101: 0.011547496542334557\n",
      "Minibatch train prediction at step 101: [1 0]\n",
      "Ground truth at step 101: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 102: [[-0.10680553 -0.94634902 -0.13435519  5.05021143]\n",
      " [-2.15079546 -2.77192831  4.66769981 -0.04046672]]\n",
      "Minibatch train loss at step 102: [ 0.01375353  0.01064527]\n",
      "Minibatch train loss mean at step 102: 0.0121993999928236\n",
      "Minibatch train prediction at step 102: [3 2]\n",
      "Ground truth at step 102: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 103: [[-2.15673566 -2.77569175  4.6781249  -0.04798361]\n",
      " [ 5.22100925  1.44264019 -1.85029674 -0.83047903]]\n",
      "Minibatch train loss at step 103: [ 0.01046078  0.02572958]\n",
      "Minibatch train loss mean at step 103: 0.01809518225491047\n",
      "Minibatch train prediction at step 103: [2 0]\n",
      "Ground truth at step 103: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 104: [[ 6.4783349  -1.40815306 -2.39516449 -1.2185663 ]\n",
      " [-0.74827892 -3.5530262   0.03699861  5.34596825]]\n",
      "Minibatch train loss at step 104: [ 0.00096954  0.00731256]\n",
      "Minibatch train loss mean at step 104: 0.004141046199947596\n",
      "Minibatch train prediction at step 104: [0 3]\n",
      "Ground truth at step 104: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 105: [[ 5.73679638  0.9480871  -1.69850719 -0.80400759]\n",
      " [ 0.440588    5.6794281  -0.91747046 -1.0817492 ]]\n",
      "Minibatch train loss at step 105: [ 0.0103034   0.00779835]\n",
      "Minibatch train loss mean at step 105: 0.00905087310820818\n",
      "Minibatch train prediction at step 105: [0 1]\n",
      "Ground truth at step 105: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 106: [[-2.46892023 -2.79495573  5.84514523 -1.47798836]\n",
      " [ 1.2756387   6.1056962  -1.31740832 -1.02003992]]\n",
      "Minibatch train loss at step 106: [ 0.00108136  0.0093437 ]\n",
      "Minibatch train loss mean at step 106: 0.005212531425058842\n",
      "Minibatch train prediction at step 106: [2 1]\n",
      "Ground truth at step 106: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 107: [[ 5.77238035  0.92246217 -1.70565498 -0.81538206]\n",
      " [ 5.30399656  1.3833766  -1.86983573 -0.85374314]]\n",
      "Minibatch train loss at step 107: [ 0.00972415  0.02245815]\n",
      "Minibatch train loss mean at step 107: 0.016091149300336838\n",
      "Minibatch train prediction at step 107: [0 0]\n",
      "Ground truth at step 107: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 108: [[-2.20816875 -1.98283625  5.69211054 -1.85952806]\n",
      " [ 0.4299081   5.70052671 -0.92198455 -1.09298229]]\n",
      "Minibatch train loss at step 108: [ 0.00135925  0.00756293]\n",
      "Minibatch train loss mean at step 108: 0.004461093805730343\n",
      "Minibatch train prediction at step 108: [2 1]\n",
      "Ground truth at step 108: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 109: [[ 5.82001829  0.88287818 -1.71088088 -0.83076233]\n",
      " [-2.1454823  -2.87116337  5.05852413 -0.7348209 ]]\n",
      "Minibatch train loss at step 109: [ 0.00896406  0.00414275]\n",
      "Minibatch train loss mean at step 109: 0.0065534040331840515\n",
      "Minibatch train prediction at step 109: [0 2]\n",
      "Ground truth at step 109: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 110: [[-2.19207692 -2.79676771  4.75343513 -0.11134827]\n",
      " [ 5.79523373  0.7816329  -1.76503086 -0.88159335]]\n",
      "Minibatch train loss at step 110: [ 0.0091604   0.00839207]\n",
      "Minibatch train loss mean at step 110: 0.008776232600212097\n",
      "Minibatch train prediction at step 110: [2 0]\n",
      "Ground truth at step 110: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 111: [[ 6.47832203 -1.87434375 -2.12726569 -0.23823468]\n",
      " [-0.76591361 -3.56848812  0.02767669  5.39844894]]\n",
      "Minibatch train loss at step 111: [ 0.00162826  0.00685755]\n",
      "Minibatch train loss mean at step 111: 0.004242907743901014\n",
      "Minibatch train prediction at step 111: [0 3]\n",
      "Ground truth at step 111: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 112: [[ 6.48369217 -1.8751936  -2.1259892  -0.24265367]\n",
      " [-2.20095253 -2.80207038  4.77751255 -0.13271669]]\n",
      "Minibatch train loss at step 112: [ 0.00161422  0.00877477]\n",
      "Minibatch train loss mean at step 112: 0.005194495432078838\n",
      "Minibatch train prediction at step 112: [0 2]\n",
      "Ground truth at step 112: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 113: [[-2.16082382 -2.88193083  5.10442114 -0.76575637]\n",
      " [ 6.49008703 -1.8756057  -2.12386131 -0.24896103]]\n",
      "Minibatch train loss at step 113: [ 0.00385447  0.00159673]\n",
      "Minibatch train loss mean at step 113: 0.002725595608353615\n",
      "Minibatch train prediction at step 113: [2 0]\n",
      "Ground truth at step 113: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 114: [[ 0.82524514  5.62329245 -1.06847394 -1.13482821]\n",
      " [ 5.86509085  0.72037822 -1.77419627 -0.91025209]]\n",
      "Minibatch train loss at step 114: [ 0.01059208  0.00742521]\n",
      "Minibatch train loss mean at step 114: 0.00900864228606224\n",
      "Minibatch train prediction at step 114: [1 0]\n",
      "Ground truth at step 114: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 115: [[ 6.50568962 -1.43013728 -2.38782263 -1.24016321]\n",
      " [ 0.42298958  5.76353216 -0.92729986 -1.12792873]]\n",
      "Minibatch train loss at step 115: [ 0.00092714  0.0070272 ]\n",
      "Minibatch train loss mean at step 115: 0.0039771669544279575\n",
      "Minibatch train prediction at step 115: [0 1]\n",
      "Ground truth at step 115: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 116: [[-0.0877744  -1.04482353 -0.20499161  5.30082273]\n",
      " [ 6.50930786 -1.43209434 -2.38749671 -1.24253905]]\n",
      "Minibatch train loss at step 116: [ 0.01033231  0.00092214]\n",
      "Minibatch train loss mean at step 116: 0.005627221893519163\n",
      "Minibatch train prediction at step 116: [3 0]\n",
      "Ground truth at step 116: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 117: [[ 6.51622915 -1.87550771 -2.11588001 -0.2757853 ]\n",
      " [-2.17635989 -2.89226747  5.15278816 -0.79972684]]\n",
      "Minibatch train loss at step 117: [ 0.00152662  0.00356966]\n",
      "Minibatch train loss mean at step 117: 0.0025481409393250942\n",
      "Minibatch train prediction at step 117: [0 2]\n",
      "Ground truth at step 117: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 118: [[ 6.51847172 -1.43623006 -2.3864994  -1.24813175]\n",
      " [-0.08787849 -1.05154848 -0.20888747  5.32420444]]\n",
      "Minibatch train loss at step 118: [ 0.00090987  0.01006752]\n",
      "Minibatch train loss mean at step 118: 0.005488692317157984\n",
      "Minibatch train prediction at step 118: [0 3]\n",
      "Ground truth at step 118: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 119: [[-2.230721   -2.81987476  4.86495161 -0.20801175]\n",
      " [-0.08919097 -1.05447733 -0.2107867   5.33878899]]\n",
      "Minibatch train loss at step 119: [ 0.00752389  0.00990464]\n",
      "Minibatch train loss mean at step 119: 0.008714267052710056\n",
      "Minibatch train prediction at step 119: [2 3]\n",
      "Ground truth at step 119: [2 3]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 120: [[ 1.22969365  6.14677763 -1.32584071 -1.1465646 ]\n",
      " [ 6.53473234 -1.87405443 -2.11072755 -0.29546681]]\n",
      "Minibatch train loss at step 120: [ 0.00853238  0.00147841]\n",
      "Minibatch train loss mean at step 120: 0.005005395971238613\n",
      "Minibatch train prediction at step 120: [1 0]\n",
      "Ground truth at step 120: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 121: [[-2.23965287 -2.82472062  4.89021826 -0.22872967]\n",
      " [ 5.9468112   0.66289818 -1.79432738 -0.94550252]]\n",
      "Minibatch train loss at step 121: [ 0.0072032   0.00650147]\n",
      "Minibatch train loss mean at step 121: 0.006852339021861553\n",
      "Minibatch train prediction at step 121: [2 0]\n",
      "Ground truth at step 121: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 122: [[ 6.54742908 -1.87254786 -2.10689902 -0.30978024]\n",
      " [-2.4908824  -2.802176    5.94840384 -1.54650092]]\n",
      "Minibatch train loss at step 122: [ 0.00144568  0.00093   ]\n",
      "Minibatch train loss mean at step 122: 0.001187837217003107\n",
      "Minibatch train prediction at step 122: [0 2]\n",
      "Ground truth at step 122: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 123: [[ 6.53953981 -1.44554508 -2.38304567 -1.26194227]\n",
      " [ 0.79744375  5.73212385 -1.08165407 -1.16437018]]\n",
      "Minibatch train loss at step 123: [ 0.00088259  0.00925961]\n",
      "Minibatch train loss mean at step 123: 0.005071103572845459\n",
      "Minibatch train prediction at step 123: [0 1]\n",
      "Ground truth at step 123: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 124: [[ 5.97234011  0.64792258 -1.8034333  -0.95665425]\n",
      " [ 0.79585642  5.78317499 -1.08002257 -1.17528379]]\n",
      "Minibatch train loss at step 124: [ 0.00625024  0.00878139]\n",
      "Minibatch train loss mean at step 124: 0.00751581322401762\n",
      "Minibatch train prediction at step 124: [0 1]\n",
      "Ground truth at step 124: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 125: [[ 0.79696536  5.87452269 -1.0745213  -1.19658959]\n",
      " [-0.10399953 -1.06520116 -0.2146361   5.42247248]]\n",
      "Minibatch train loss at step 125: [ 0.00801171  0.00902443]\n",
      "Minibatch train loss mean at step 125: 0.008518066257238388\n",
      "Minibatch train prediction at step 125: [1 3]\n",
      "Ground truth at step 125: [1 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 126: [[ 5.58641291 -2.1582706  -1.84023225 -0.98207319]\n",
      " [ 5.98827124  0.64257348 -1.81025398 -0.96476674]]\n",
      "Minibatch train loss at step 126: [ 0.00242927  0.00611589]\n",
      "Minibatch train loss mean at step 126: 0.004272581543773413\n",
      "Minibatch train prediction at step 126: [0 0]\n",
      "Ground truth at step 126: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 127: [[ 6.06041241  0.71456742 -1.74983156 -0.92564327]\n",
      " [ 0.77700174  5.95906019 -1.07326508 -1.21010363]]\n",
      "Minibatch train loss at step 127: [ 0.00607963  0.00724297]\n",
      "Minibatch train loss mean at step 127: 0.006661301478743553\n",
      "Minibatch train prediction at step 127: [0 1]\n",
      "Ground truth at step 127: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 128: [[ 6.07001543  0.71137542 -1.75264478 -0.93002725]\n",
      " [ 6.00688505  0.63524354 -1.81716871 -0.97433043]]\n",
      "Minibatch train loss at step 128: [ 0.00600167  0.00595782]\n",
      "Minibatch train loss mean at step 128: 0.005979742854833603\n",
      "Minibatch train prediction at step 128: [0 0]\n",
      "Ground truth at step 128: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 129: [[ 0.38406765  6.0767312  -0.92917341 -1.21846879]\n",
      " [ 6.08366585  0.70433295 -1.75516236 -0.93461007]]\n",
      "Minibatch train loss at step 129: [ 0.00494365  0.00588304]\n",
      "Minibatch train loss mean at step 129: 0.005413345992565155\n",
      "Minibatch train prediction at step 129: [1 0]\n",
      "Ground truth at step 129: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 130: [[-2.27132559 -2.84020257  4.98260689 -0.30542105]\n",
      " [ 6.59228659 -1.86930704 -2.09612489 -0.35738274]]\n",
      "Minibatch train loss at step 130: [ 0.00614077  0.00133806]\n",
      "Minibatch train loss mean at step 130: 0.0037394165992736816\n",
      "Minibatch train prediction at step 130: [2 0]\n",
      "Ground truth at step 130: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 131: [[ 6.5764122  -1.45994854 -2.38040686 -1.28176832]\n",
      " [ 0.37212598  6.09426069 -0.93158948 -1.22402191]]\n",
      "Minibatch train loss at step 131: [ 0.00083864  0.00481304]\n",
      "Minibatch train loss mean at step 131: 0.0028258401434868574\n",
      "Minibatch train prediction at step 131: [0 1]\n",
      "Ground truth at step 131: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 132: [[-2.27714872 -2.84270334  5.00042582 -0.32085985]\n",
      " [ 0.3661471   6.1038537  -0.93303812 -1.22684622]]\n",
      "Minibatch train loss at step 132: [ 0.00595201  0.00474482]\n",
      "Minibatch train loss mean at step 132: 0.0053484165109694\n",
      "Minibatch train prediction at step 132: [2 1]\n",
      "Ground truth at step 132: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 133: [[ 6.13495016  0.67942858 -1.76506197 -0.95222586]\n",
      " [-0.12254824 -1.07057691 -0.21267228  5.50695658]]\n",
      "Minibatch train loss at step 133: [ 0.00546426  0.00822857]\n",
      "Minibatch train loss mean at step 133: 0.0068464139476418495\n",
      "Minibatch train prediction at step 133: [0 3]\n",
      "Ground truth at step 133: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 134: [[ 6.14694977  0.67366552 -1.76767159 -0.95653653]\n",
      " [ 0.69744581  6.03091145 -1.08582079 -1.21234107]]\n",
      "Minibatch train loss at step 134: [ 0.00537059  0.00633364]\n",
      "Minibatch train loss mean at step 134: 0.005852114874869585\n",
      "Minibatch train prediction at step 134: [0 1]\n",
      "Ground truth at step 134: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 135: [[-0.12646908 -1.07198346 -0.21233657  5.5285697 ]\n",
      " [-0.81453401 -3.60101771  0.06933013  5.44403362]]\n",
      "Minibatch train loss at step 135: [ 0.0080389   0.00664217]\n",
      "Minibatch train loss mean at step 135: 0.007340536452829838\n",
      "Minibatch train prediction at step 135: [3 3]\n",
      "Ground truth at step 135: [3 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 136: [[ 5.72979784  1.10150182 -1.96582675 -1.01945198]\n",
      " [-0.12911236 -1.07261002 -0.21263696  5.54250145]]\n",
      "Minibatch train loss at step 136: [ 0.01133337  0.00791733]\n",
      "Minibatch train loss mean at step 136: 0.00962535198777914\n",
      "Minibatch train prediction at step 136: [0 3]\n",
      "Ground truth at step 136: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 137: [[ 0.67176336  6.05698442 -1.09204721 -1.2148689 ]\n",
      " [ 6.60150671 -1.47063684 -2.37932038 -1.29530823]]\n",
      "Minibatch train loss at step 137: [ 0.00604598  0.00080946]\n",
      "Minibatch train loss mean at step 137: 0.0034277213271707296\n",
      "Minibatch train prediction at step 137: [1 0]\n",
      "Ground truth at step 137: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 138: [[-2.41946006 -1.96913075  5.98169708 -1.87706137]\n",
      " [ 5.75877953  1.0837028  -1.97232187 -1.02971423]]\n",
      "Minibatch train loss at step 138: [ 0.00096287  0.01083149]\n",
      "Minibatch train loss mean at step 138: 0.0058971792459487915\n",
      "Minibatch train prediction at step 138: [2 0]\n",
      "Ground truth at step 138: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 139: [[-2.24324036 -2.9277873   5.34745932 -0.93231291]\n",
      " [-2.42450881 -1.96878564  5.98854113 -1.87706494]]\n",
      "Minibatch train loss at step 139: [ 0.00263023  0.00095536]\n",
      "Minibatch train loss mean at step 139: 0.001792796072550118\n",
      "Minibatch train prediction at step 139: [2 2]\n",
      "Ground truth at step 139: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 140: [[-0.82018471 -3.60624456  0.06494652  5.46604633]\n",
      " [ 5.7934165   1.05875587 -1.97719288 -1.04115796]]\n",
      "Minibatch train loss at step 140: [ 0.00646725  0.01023095]\n",
      "Minibatch train loss mean at step 140: 0.0083491001278162\n",
      "Minibatch train prediction at step 140: [3 0]\n",
      "Ground truth at step 140: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 141: [[ 5.64620066 -2.18197203 -1.83907509 -1.02883959]\n",
      " [ 6.62030554 -1.47923541 -2.37789583 -1.30491149]]\n",
      "Minibatch train loss at step 141: [ 0.00221924  0.0007885 ]\n",
      "Minibatch train loss mean at step 141: 0.0015038680285215378\n",
      "Minibatch train prediction at step 141: [0 0]\n",
      "Ground truth at step 141: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 142: [[-0.13711172 -1.08190215 -0.2169053   5.62497425]\n",
      " [-2.30805469 -2.85638356  5.07287312 -0.37241852]]\n",
      "Minibatch train loss at step 142: [ 0.00724415  0.00528569]\n",
      "Minibatch train loss mean at step 142: 0.006264923140406609\n",
      "Minibatch train prediction at step 142: [3 2]\n",
      "Ground truth at step 142: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 143: [[-2.31151891 -2.85802269  5.08022356 -0.37690055]\n",
      " [-2.50738931 -2.80739212  6.04139996 -1.60534763]]\n",
      "Minibatch train loss at step 143: [ 0.00522522  0.00081458]\n",
      "Minibatch train loss mean at step 143: 0.0030198991298675537\n",
      "Minibatch train prediction at step 143: [2 2]\n",
      "Ground truth at step 143: [2 2]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 144: [[ 6.20125961  0.5163756  -1.86540794 -1.05225158]\n",
      " [-2.31542945 -2.8598423   5.08995485 -0.3839249 ]]\n",
      "Minibatch train loss at step 144: [ 0.00440864  0.00514244]\n",
      "Minibatch train loss mean at step 144: 0.004775539040565491\n",
      "Minibatch train prediction at step 144: [0 2]\n",
      "Ground truth at step 144: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 145: [[ 0.31865615  6.19320774 -0.94896048 -1.25864065]\n",
      " [ 6.29738331  0.57223028 -1.79209268 -1.00448608]]\n",
      "Minibatch train loss at step 145: [ 0.00417267  0.00423487]\n",
      "Minibatch train loss mean at step 145: 0.004203769378364086\n",
      "Minibatch train prediction at step 145: [1 0]\n",
      "Ground truth at step 145: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 146: [[-0.13974807 -1.08919108 -0.22084755  5.67207289]\n",
      " [ 0.63792926  6.09414196 -1.10406911 -1.23047245]]\n",
      "Minibatch train loss at step 146: [ 0.0068849   0.00566069]\n",
      "Minibatch train loss mean at step 146: 0.0062727974727749825\n",
      "Minibatch train prediction at step 146: [3 1]\n",
      "Ground truth at step 146: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 147: [[-0.8299188  -3.61237741  0.05553484  5.50336599]\n",
      " [-2.46162128 -1.97008991  6.04720974 -1.88287878]]\n",
      "Minibatch train loss at step 147: [ 0.00617264  0.00089081]\n",
      "Minibatch train loss mean at step 147: 0.003531726310029626\n",
      "Minibatch train prediction at step 147: [3 2]\n",
      "Ground truth at step 147: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 148: [[-2.46593595 -1.97046971  6.05459356 -1.88377833]\n",
      " [-0.83131051 -3.613235    0.05209725  5.51165152]]\n",
      "Minibatch train loss at step 148: [ 0.00088295  0.00610475]\n",
      "Minibatch train loss mean at step 148: 0.0034938512835651636\n",
      "Minibatch train prediction at step 148: [2 3]\n",
      "Ground truth at step 148: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 149: [[ 1.11598587  6.29775858 -1.35670376 -1.26146948]\n",
      " [-2.33512878 -2.86975908  5.13727427 -0.41681948]]\n",
      "Minibatch train loss at step 149: [ 0.0065916  0.0047619]\n",
      "Minibatch train loss mean at step 149: 0.005676751956343651\n",
      "Minibatch train prediction at step 149: [1 2]\n",
      "Ground truth at step 149: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 150: [[ 6.34935904  0.53764051 -1.80401194 -1.02151835]\n",
      " [ 1.10385573  6.30562353 -1.3553108  -1.27008474]]\n",
      "Minibatch train loss at step 150: [ 0.00390173  0.0064695 ]\n",
      "Minibatch train loss mean at step 150: 0.005185612011700869\n",
      "Minibatch train prediction at step 150: [0 1]\n",
      "Ground truth at step 150: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 151: [[-2.47861409 -1.97223926  6.07801533 -1.88715041]\n",
      " [ 5.94990063  0.94248915 -1.99973869 -1.08920979]]\n",
      "Minibatch train loss at step 151: [ 0.0008583  0.0078867]\n",
      "Minibatch train loss mean at step 151: 0.00437249755486846\n",
      "Minibatch train prediction at step 151: [2 0]\n",
      "Ground truth at step 151: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 152: [[ 0.60785586  6.12802172 -1.11185074 -1.23564506]\n",
      " [-2.34669948 -2.87664294  5.16257048 -0.43184114]]\n",
      "Minibatch train loss at step 152: [ 0.00534214  0.0045787 ]\n",
      "Minibatch train loss mean at step 152: 0.004960418678820133\n",
      "Minibatch train prediction at step 152: [1 2]\n",
      "Ground truth at step 152: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 153: [[-2.51960087 -2.80986619  6.09094858 -1.63833702]\n",
      " [-2.48701715 -1.97365415  6.09413004 -1.89006591]]\n",
      "Minibatch train loss at step 153: [ 0.00075788  0.0008415 ]\n",
      "Minibatch train loss mean at step 153: 0.0007996928179636598\n",
      "Minibatch train prediction at step 153: [2 2]\n",
      "Ground truth at step 153: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 154: [[-2.35421968 -2.88116312  5.18159199 -0.44507846]\n",
      " [ 0.59402156  6.142519   -1.11400807 -1.23615754]]\n",
      "Minibatch train loss at step 154: [ 0.00443938  0.00520968]\n",
      "Minibatch train loss mean at step 154: 0.004824529867619276\n",
      "Minibatch train prediction at step 154: [2 1]\n",
      "Ground truth at step 154: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 155: [[-2.35819888 -2.8835988   5.19270468 -0.45349056]\n",
      " [ 6.30076265  0.45200723 -1.89663577 -1.08397591]]\n",
      "Minibatch train loss at step 155: [ 0.00435796  0.00377229]\n",
      "Minibatch train loss mean at step 155: 0.0040651243180036545\n",
      "Minibatch train prediction at step 155: [2 0]\n",
      "Ground truth at step 155: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 156: [[-0.84244859 -3.61830068  0.02512636  5.57533789]\n",
      " [ 0.2807714   6.26198292 -0.95895481 -1.27943337]]\n",
      "Minibatch train loss at step 156: [ 0.00560486  0.00378036]\n",
      "Minibatch train loss mean at step 156: 0.004692612681537867\n",
      "Minibatch train prediction at step 156: [3 1]\n",
      "Ground truth at step 156: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 157: [[ 0.98661506  6.37487602 -1.33571506 -1.34737659]\n",
      " [ 6.31490993  0.44454435 -1.90234625 -1.08921957]]\n",
      "Minibatch train loss at step 157: [ 0.005446    0.00369366]\n",
      "Minibatch train loss mean at step 157: 0.004569833166897297\n",
      "Minibatch train prediction at step 157: [1 0]\n",
      "Ground truth at step 157: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 158: [[-0.15609072 -1.10266244 -0.22407915  5.77274132]\n",
      " [ 6.66030073 -1.50487423 -2.37282395 -1.33373511]]\n",
      "Minibatch train loss at step 158: [ 0.0061621   0.00074109]\n",
      "Minibatch train loss mean at step 158: 0.003451592056080699\n",
      "Minibatch train prediction at step 158: [3 0]\n",
      "Ground truth at step 158: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 159: [[-2.52764487 -2.81173658  6.12583733 -1.66276205]\n",
      " [ 6.02023745  0.90367049 -2.0196104  -1.1181469 ]]\n",
      "Minibatch train loss at step 159: [ 0.00072012  0.0070878 ]\n",
      "Minibatch train loss mean at step 159: 0.003903962206095457\n",
      "Minibatch train prediction at step 159: [2 0]\n",
      "Ground truth at step 159: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 160: [[ 6.42448807  0.49655735 -1.83087373 -1.05879223]\n",
      " [ 6.66460896 -1.50772965 -2.37212515 -1.33720481]]\n",
      "Minibatch train loss at step 160: [ 0.00348021  0.00073597]\n",
      "Minibatch train loss mean at step 160: 0.0021080891601741314\n",
      "Minibatch train prediction at step 160: [0 0]\n",
      "Ground truth at step 160: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 161: [[-2.51521945 -1.9776516   6.15193605 -1.90543723]\n",
      " [-2.38079524 -2.89755034  5.25136423 -0.49562216]]\n",
      "Minibatch train loss at step 161: [ 0.00078326  0.00395825]\n",
      "Minibatch train loss mean at step 161: 0.0023707523941993713\n",
      "Minibatch train prediction at step 161: [2 2]\n",
      "Ground truth at step 161: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 162: [[-0.16045408 -1.10714817 -0.22604999  5.80034781]\n",
      " [ 0.92036712  6.41381359 -1.32328188 -1.38693869]]\n",
      "Minibatch train loss at step 162: [ 0.00597441  0.00494721]\n",
      "Minibatch train loss mean at step 162: 0.005460808984935284\n",
      "Minibatch train prediction at step 162: [3 1]\n",
      "Ground truth at step 162: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 163: [[-0.8504346  -3.62174463  0.0076688   5.61942196]\n",
      " [ 6.05861473  0.87894946 -2.02901697 -1.1345768 ]]\n",
      "Minibatch train loss at step 163: [ 0.00528712  0.00666668]\n",
      "Minibatch train loss mean at step 163: 0.005976898595690727\n",
      "Minibatch train prediction at step 163: [3 0]\n",
      "Ground truth at step 163: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 164: [[ 6.6786294  -1.51470649 -2.37103558 -1.34524858]\n",
      " [ 6.45730734  0.4750146  -1.84188986 -1.07595885]]\n",
      "Minibatch train loss at step 164: [ 0.00072119  0.00330129]\n",
      "Minibatch train loss mean at step 164: 0.002011242788285017\n",
      "Minibatch train prediction at step 164: [0 0]\n",
      "Ground truth at step 164: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 165: [[ 6.68363571 -1.51683366 -2.37087035 -1.34758282]\n",
      " [-2.52698588 -1.97977948  6.17536783 -1.91120207]]\n",
      "Minibatch train loss at step 165: [ 0.00071631  0.00076086]\n",
      "Minibatch train loss mean at step 165: 0.0007385860662907362\n",
      "Minibatch train prediction at step 165: [0 2]\n",
      "Ground truth at step 165: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 166: [[ 5.73148441 -2.2209444  -1.84188962 -1.08057606]\n",
      " [ 6.68995762 -1.51917708 -2.37076092 -1.35035563]]\n",
      "Minibatch train loss at step 166: [ 0.00196419  0.00071024]\n",
      "Minibatch train loss mean at step 166: 0.00133721181191504\n",
      "Minibatch train prediction at step 166: [0 0]\n",
      "Ground truth at step 166: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 167: [[-2.39940262 -2.90910339  5.29533863 -0.52427751]\n",
      " [ 6.48722792  0.4543252  -1.85041225 -1.08602691]]\n",
      "Minibatch train loss at step 167: [ 0.00369058  0.00314694]\n",
      "Minibatch train loss mean at step 167: 0.0034187566488981247\n",
      "Minibatch train prediction at step 167: [2 0]\n",
      "Ground truth at step 167: [2 0]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 168: [[-0.85406351 -3.62368941 -0.00978522  5.65545368]\n",
      " [-0.16447604 -1.11561024 -0.23252474  5.84347248]]\n",
      "Minibatch train loss at step 168: [ 0.00503415  0.00569021]\n",
      "Minibatch train loss mean at step 168: 0.00536218099296093\n",
      "Minibatch train prediction at step 168: [3 3]\n",
      "Ground truth at step 168: [3 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 169: [[ 6.12432241  0.83164507 -2.04200196 -1.15548372]\n",
      " [ 6.41365719  0.38024947 -1.93557847 -1.12319493]]\n",
      "Minibatch train loss at step 169: [ 0.00598365  0.00316203]\n",
      "Minibatch train loss mean at step 169: 0.004572841338813305\n",
      "Minibatch train prediction at step 169: [0 0]\n",
      "Ground truth at step 169: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 170: [[-0.16473846 -1.11901033 -0.23681951  5.86043978]\n",
      " [-2.40836692 -2.91485596  5.31504726 -0.53588057]]\n",
      "Minibatch train loss at step 170: [ 0.00558139  0.00357976]\n",
      "Minibatch train loss mean at step 170: 0.004580574110150337\n",
      "Minibatch train prediction at step 170: [3 2]\n",
      "Ground truth at step 170: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 171: [[ 6.71348381 -1.89322984 -2.08397651 -0.44720149]\n",
      " [ 6.43397188  0.36396754 -1.94070017 -1.12942493]]\n",
      "Minibatch train loss at step 171: [ 0.00110994  0.00305626]\n",
      "Minibatch train loss mean at step 171: 0.0020830996800214052\n",
      "Minibatch train prediction at step 171: [0 0]\n",
      "Ground truth at step 171: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 172: [[-0.1646753  -1.12292135 -0.24285206  5.88083363]\n",
      " [-2.41484952 -2.9189744   5.32989931 -0.54516822]]\n",
      "Minibatch train loss at step 172: [ 0.0054524   0.00349696]\n",
      "Minibatch train loss mean at step 172: 0.004474683664739132\n",
      "Minibatch train prediction at step 172: [3 2]\n",
      "Ground truth at step 172: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 173: [[-2.54655552 -1.98449409  6.2143178  -1.92002857]\n",
      " [ 0.24786101  6.33774376 -0.96927762 -1.30874348]]\n",
      "Minibatch train loss at step 173: [ 0.00072477  0.00340834]\n",
      "Minibatch train loss mean at step 173: 0.002066553570330143\n",
      "Minibatch train prediction at step 173: [2 1]\n",
      "Ground truth at step 173: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 174: [[-0.85944754 -3.62641406 -0.0300324   5.70049858]\n",
      " [ 5.76343441 -2.236202   -1.84592259 -1.09556222]]\n",
      "Minibatch train loss at step 174: [ 0.00473901  0.00187959]\n",
      "Minibatch train loss mean at step 174: 0.003309299238026142\n",
      "Minibatch train prediction at step 174: [3 0]\n",
      "Ground truth at step 174: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 175: [[ 6.73885107 -1.53813314 -2.36984444 -1.37208438]\n",
      " [-2.54646754 -2.81296301  6.19291735 -1.70774806]]\n",
      "Minibatch train loss at step 175: [ 0.00066509  0.00065317]\n",
      "Minibatch train loss mean at step 175: 0.0006591292330995202\n",
      "Minibatch train prediction at step 175: [0 2]\n",
      "Ground truth at step 175: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 176: [[ 0.52610981  6.24212122 -1.13141167 -1.25376463]\n",
      " [-0.86093533 -3.62732482 -0.03907048  5.71869183]]\n",
      "Minibatch train loss at step 176: [ 0.00446585  0.0046232 ]\n",
      "Minibatch train loss mean at step 176: 0.004544522613286972\n",
      "Minibatch train prediction at step 176: [1 3]\n",
      "Ground truth at step 176: [1 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 177: [[-2.54939723 -2.81335497  6.2011981  -1.71313691]\n",
      " [-2.43093967 -2.93050146  5.36641884 -0.56607419]]\n",
      "Minibatch train loss at step 177: [ 0.00064531  0.0033064 ]\n",
      "Minibatch train loss mean at step 177: 0.001975855091586709\n",
      "Minibatch train prediction at step 177: [2 2]\n",
      "Ground truth at step 177: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 178: [[ 6.48666096  0.32512677 -1.95733595 -1.14438248]\n",
      " [-0.16695088 -1.13190067 -0.26132163  5.94083786]]\n",
      "Minibatch train loss at step 178: [ 0.00280547  0.00508551]\n",
      "Minibatch train loss mean at step 178: 0.003945487551391125\n",
      "Minibatch train prediction at step 178: [0 3]\n",
      "Ground truth at step 178: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 179: [[ 6.22351456  0.75896853 -2.06469655 -1.18494117]\n",
      " [ 0.85277534  6.45810366 -1.30403209 -1.45165646]]\n",
      "Minibatch train loss at step 179: [ 0.00507887  0.00446098]\n",
      "Minibatch train loss mean at step 179: 0.004769923165440559\n",
      "Minibatch train prediction at step 179: [0 1]\n",
      "Ground truth at step 179: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 180: [[ 0.84755999  6.46122503 -1.30278647 -1.45470333]\n",
      " [ 6.23222923  0.75309932 -2.06712294 -1.18767715]]\n",
      "Minibatch train loss at step 180: [ 0.00442751  0.00500818]\n",
      "Minibatch train loss mean at step 180: 0.004717843607068062\n",
      "Minibatch train prediction at step 180: [1 0]\n",
      "Ground truth at step 180: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 181: [[ 6.76014614 -1.54743803 -2.3688972  -1.3829999 ]\n",
      " [-2.35065413 -3.01255631  5.6586051  -1.10738409]]\n",
      "Minibatch train loss at step 181: [ 0.00064567  0.00165469]\n",
      "Minibatch train loss mean at step 181: 0.00115017662756145\n",
      "Minibatch train prediction at step 181: [0 2]\n",
      "Ground truth at step 181: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 182: [[-2.56614256 -1.98801816  6.25341034 -1.92916429]\n",
      " [-0.86421514 -3.62919188 -0.06815398  5.77464342]]\n",
      "Minibatch train loss at step 182: [ 0.00069046  0.00428247]\n",
      "Minibatch train loss mean at step 182: 0.002486465498805046\n",
      "Minibatch train prediction at step 182: [2 3]\n",
      "Ground truth at step 182: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 183: [[ 0.23439217  6.3738842  -0.97612631 -1.32228518]\n",
      " [ 5.80331278 -2.25554991 -1.85467279 -1.10959566]]\n",
      "Minibatch train loss at step 183: [ 0.00324782  0.00178178]\n",
      "Minibatch train loss mean at step 183: 0.002514802385121584\n",
      "Minibatch train prediction at step 183: [1 0]\n",
      "Ground truth at step 183: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 184: [[ 6.26743841  0.72914416 -2.07606459 -1.19880259]\n",
      " [-0.86484647 -3.62947059 -0.07915306  5.79456568]]\n",
      "Minibatch train loss at step 184: [ 0.004732    0.00416638]\n",
      "Minibatch train loss mean at step 184: 0.004449190106242895\n",
      "Minibatch train prediction at step 184: [0 3]\n",
      "Ground truth at step 184: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 185: [[-2.5615871  -2.81469417  6.23640156 -1.73595393]\n",
      " [-0.8648994  -3.62952757 -0.08653393  5.80714178]]\n",
      "Minibatch train loss at step 185: [ 0.00061302  0.00409396]\n",
      "Minibatch train loss mean at step 185: 0.0023534910287708044\n",
      "Minibatch train prediction at step 185: [2 3]\n",
      "Ground truth at step 185: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 186: [[-0.17228249 -1.14096677 -0.27801165  6.00504303]\n",
      " [ 5.81957626 -2.26388884 -1.85917914 -1.11345613]]\n",
      "Minibatch train loss at step 186: [ 0.0047205   0.00174465]\n",
      "Minibatch train loss mean at step 186: 0.003232575487345457\n",
      "Minibatch train prediction at step 186: [3 0]\n",
      "Ground truth at step 186: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 187: [[ 6.75875282 -1.89510918 -2.08760834 -0.47465116]\n",
      " [ 6.54875898  0.2816107  -1.97740507 -1.16008544]]\n",
      "Minibatch train loss at step 187: [ 0.0010398   0.00254141]\n",
      "Minibatch train loss mean at step 187: 0.0017906040884554386\n",
      "Minibatch train prediction at step 187: [0 0]\n",
      "Ground truth at step 187: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 188: [[-2.36367965 -3.027107    5.69085693 -1.11479306]\n",
      " [ 6.78292942 -1.55835867 -2.36797667 -1.39512169]]\n",
      "Minibatch train loss at step 188: [ 0.00158744  0.00062506]\n",
      "Minibatch train loss mean at step 188: 0.001106249401345849\n",
      "Minibatch train prediction at step 188: [2 0]\n",
      "Ground truth at step 188: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 189: [[-0.17386922 -1.1439656  -0.2853336   6.02980185]\n",
      " [ 0.49793771  6.28973961 -1.14058542 -1.25888741]]\n",
      "Minibatch train loss at step 189: [ 0.00458665  0.00416353]\n",
      "Minibatch train loss mean at step 189: 0.004375089425593615\n",
      "Minibatch train prediction at step 189: [3 1]\n",
      "Ground truth at step 189: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 190: [[-0.86529195 -3.62982917 -0.1239727   5.87141371]\n",
      " [-2.36712456 -3.03121018  5.69926786 -1.11607754]]\n",
      "Minibatch train loss at step 190: [ 0.0037445   0.00157114]\n",
      "Minibatch train loss mean at step 190: 0.002657816279679537\n",
      "Minibatch train prediction at step 190: [3 2]\n",
      "Ground truth at step 190: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 191: [[ 0.22185655  6.40459156 -0.9811061  -1.33272183]\n",
      " [-2.57128096 -2.81623888  6.25815535 -1.74858642]]\n",
      "Minibatch train loss at step 191: [ 0.00311616  0.00059396]\n",
      "Minibatch train loss mean at step 191: 0.0018550603417679667\n",
      "Minibatch train prediction at step 191: [1 2]\n",
      "Ground truth at step 191: [1 2]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 192: [[-2.37104678 -3.03591013  5.70859766 -1.11718524]\n",
      " [-0.1768878  -1.14592481 -0.29410079  6.05788088]]\n",
      "Minibatch train loss at step 192: [ 0.00155328  0.00443724]\n",
      "Minibatch train loss mean at step 192: 0.0029952628538012505\n",
      "Minibatch train prediction at step 192: [2 3]\n",
      "Ground truth at step 192: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 193: [[-0.86639518 -3.62976217 -0.14362431  5.90700579]\n",
      " [ 6.57996225  0.26165307 -1.98895741 -1.16643548]]\n",
      "Minibatch train loss at step 193: [ 0.00356598  0.00242226]\n",
      "Minibatch train loss mean at step 193: 0.0029941184911876917\n",
      "Minibatch train prediction at step 193: [3 0]\n",
      "Ground truth at step 193: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 194: [[ 0.21425039  6.42014456 -0.98412073 -1.33556807]\n",
      " [-2.57671547 -2.81738424  6.27087975 -1.75626755]]\n",
      "Minibatch train loss at step 194: [ 0.00304984  0.000583  ]\n",
      "Minibatch train loss mean at step 194: 0.0018164230277761817\n",
      "Minibatch train prediction at step 194: [1 2]\n",
      "Ground truth at step 194: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 195: [[ 5.86853647 -2.28787923 -1.87384105 -1.1245997 ]\n",
      " [ 6.58864307  0.25688905 -1.99278188 -1.1679368 ]]\n",
      "Minibatch train loss at step 195: [ 0.00163779  0.00239158]\n",
      "Minibatch train loss mean at step 195: 0.00201468076556921\n",
      "Minibatch train prediction at step 195: [0 0]\n",
      "Ground truth at step 195: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 196: [[ 0.4740496   6.32428932 -1.14908528 -1.26006424]\n",
      " [ 6.69690943  0.31146622 -1.92790043 -1.16942918]]\n",
      "Minibatch train loss at step 196: [ 0.0039478   0.00224648]\n",
      "Minibatch train loss mean at step 196: 0.003097138600423932\n",
      "Minibatch train prediction at step 196: [1 0]\n",
      "Ground truth at step 196: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 197: [[ 6.3521471   0.67666149 -2.10092831 -1.22201777]\n",
      " [ 6.59869862  0.25153711 -1.99656785 -1.17043602]]\n",
      "Minibatch train loss at step 197: [ 0.00414726  0.00235649]\n",
      "Minibatch train loss mean at step 197: 0.0032518773805350065\n",
      "Minibatch train prediction at step 197: [0 0]\n",
      "Ground truth at step 197: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 198: [[ 6.60635042  0.24641243 -1.99834609 -1.17234135]\n",
      " [-2.59494448 -1.98909163  6.30622911 -1.93761075]]\n",
      "Minibatch train loss at step 198: [ 0.00232854  0.00064853]\n",
      "Minibatch train loss mean at step 198: 0.0014885351993143559\n",
      "Minibatch train prediction at step 198: [0 2]\n",
      "Ground truth at step 198: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 199: [[ 5.88998985 -2.29792714 -1.8794657  -1.13114095]\n",
      " [-2.59674025 -1.98913836  6.30949545 -1.93810701]]\n",
      "Minibatch train loss at step 199: [ 0.00159196  0.00064614]\n",
      "Minibatch train loss mean at step 199: 0.0011190541554242373\n",
      "Minibatch train prediction at step 199: [0 2]\n",
      "Ground truth at step 199: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 200: [[ 0.46166366  6.34455109 -1.15380287 -1.26404238]\n",
      " [ 6.62173939  0.23579635 -2.00145292 -1.17627013]]\n",
      "Minibatch train loss at step 200: [ 0.00382953  0.00227288]\n",
      "Minibatch train loss mean at step 200: 0.0030512050725519657\n",
      "Minibatch train prediction at step 200: [1 0]\n",
      "Ground truth at step 200: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 201: [[ 6.73193932  0.29027829 -1.939659   -1.18719721]\n",
      " [ 5.90243626 -2.30319142 -1.88173485 -1.13652289]]\n",
      "Minibatch train loss at step 201: [ 0.00212658  0.00156518]\n",
      "Minibatch train loss mean at step 201: 0.0018458806443959475\n",
      "Minibatch train prediction at step 201: [0 0]\n",
      "Ground truth at step 201: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 202: [[ 6.39425468  0.6500594  -2.10926175 -1.23317266]\n",
      " [-2.58891749 -2.81912589  6.30016899 -1.7736659 ]]\n",
      "Minibatch train loss at step 202: [ 0.00388356  0.00055882]\n",
      "Minibatch train loss mean at step 202: 0.0022211880423128605\n",
      "Minibatch train prediction at step 202: [0 2]\n",
      "Ground truth at step 202: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 203: [[-0.86736327 -3.63040042 -0.1908811   5.99339628]\n",
      " [-2.60389113 -1.99011445  6.32361221 -1.94078708]]\n",
      "Minibatch train loss at step 203: [ 0.00317082  0.00063506]\n",
      "Minibatch train loss mean at step 203: 0.0019029434770345688\n",
      "Minibatch train prediction at step 203: [3 2]\n",
      "Ground truth at step 203: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 204: [[-2.59187865 -2.81929064  6.30732632 -1.77795422]\n",
      " [ 5.92361784 -2.31168365 -1.88496184 -1.14647388]]\n",
      "Minibatch train loss at step 204: [ 0.0005531   0.00152043]\n",
      "Minibatch train loss mean at step 204: 0.0010367641225457191\n",
      "Minibatch train prediction at step 204: [2 0]\n",
      "Ground truth at step 204: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 205: [[-0.86721396 -3.6305635  -0.19903703  6.00828981]\n",
      " [ 5.93164301 -2.31475353 -1.88613045 -1.15026426]]\n",
      "Minibatch train loss at step 205: [ 0.00310772  0.00150389]\n",
      "Minibatch train loss mean at step 205: 0.002305803121998906\n",
      "Minibatch train prediction at step 205: [3 0]\n",
      "Ground truth at step 205: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 206: [[-0.18202506 -1.15494835 -0.32420281  6.15565825]\n",
      " [ 6.42927265  0.6250633  -2.1131649  -1.23967528]]\n",
      "Minibatch train loss at step 206: [ 0.003963    0.00367015]\n",
      "Minibatch train loss mean at step 206: 0.0038165731821209192\n",
      "Minibatch train prediction at step 206: [3 0]\n",
      "Ground truth at step 206: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 207: [[-2.49852967 -2.98383975  5.47565031 -0.57577121]\n",
      " [ 0.1915292   6.47910357 -0.99423385 -1.35403204]]\n",
      "Minibatch train loss at step 207: [ 0.00290639  0.00281973]\n",
      "Minibatch train loss mean at step 207: 0.0028630588203668594\n",
      "Minibatch train prediction at step 207: [2 1]\n",
      "Ground truth at step 207: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 208: [[-0.18185331 -1.15648139 -0.32850629  6.16948509]\n",
      " [-0.8673414  -3.63069773 -0.2127251   6.03395748]]\n",
      "Minibatch train loss at step 208: [ 0.00390149  0.0030023 ]\n",
      "Minibatch train loss mean at step 208: 0.003451897297054529\n",
      "Minibatch train prediction at step 208: [3 3]\n",
      "Ground truth at step 208: [3 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 209: [[-2.5020988  -2.98740149  5.48312283 -0.57704324]\n",
      " [ 6.45483875  0.60642731 -2.11536574 -1.24073958]]\n",
      "Minibatch train loss at step 209: [ 0.00287988  0.00352274]\n",
      "Minibatch train loss mean at step 209: 0.003201310057193041\n",
      "Minibatch train prediction at step 209: [2 0]\n",
      "Ground truth at step 209: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 210: [[ 5.97289371 -2.3304801  -1.8926307  -1.16871881]\n",
      " [-0.18224171 -1.15791047 -0.33433419  6.18752813]]\n",
      "Minibatch train loss at step 210: [ 0.00142187  0.00382169]\n",
      "Minibatch train loss mean at step 210: 0.00262178061529994\n",
      "Minibatch train prediction at step 210: [0 3]\n",
      "Ground truth at step 210: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 211: [[ 6.70142031  0.17769954 -2.0151484  -1.19198644]\n",
      " [ 6.82761335 -1.89272296 -2.10274363 -0.51065701]]\n",
      "Minibatch train loss at step 211: [ 0.00200321  0.00094524]\n",
      "Minibatch train loss mean at step 211: 0.00147422612644732\n",
      "Minibatch train prediction at step 211: [0 0]\n",
      "Ground truth at step 211: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 212: [[-2.60455847 -2.82098055  6.33939743 -1.79845166]\n",
      " [ 6.83153248 -1.89255011 -2.10258603 -0.51367283]]\n",
      "Minibatch train loss at step 212: [ 0.00052784  0.00093964]\n",
      "Minibatch train loss mean at step 212: 0.0007337407441809773\n",
      "Minibatch train prediction at step 212: [2 0]\n",
      "Ground truth at step 212: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 213: [[ 0.18725933  6.49864864 -0.99841052 -1.36163282]\n",
      " [ 0.44199786  6.38655376 -1.16480756 -1.28169298]]\n",
      "Minibatch train loss at step 213: [ 0.00275221  0.00360637]\n",
      "Minibatch train loss mean at step 213: 0.0031792870722711086\n",
      "Minibatch train prediction at step 213: [1 1]\n",
      "Ground truth at step 213: [1 1]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 214: [[-2.5104003  -2.99737883  5.50946379 -0.58888829]\n",
      " [ 6.82456923 -1.58532524 -2.36452508 -1.42456281]]\n",
      "Minibatch train loss at step 214: [ 0.00277373  0.0005861 ]\n",
      "Minibatch train loss mean at step 214: 0.0016799127915874124\n",
      "Minibatch train prediction at step 214: [2 0]\n",
      "Ground truth at step 214: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 215: [[ 6.84499454 -1.89122069 -2.10068846 -0.52622592]\n",
      " [-2.60898566 -2.82177138  6.35276127 -1.807706  ]]\n",
      "Minibatch train loss at step 215: [ 0.00091963  0.00051747]\n",
      "Minibatch train loss mean at step 215: 0.0007185535505414009\n",
      "Minibatch train prediction at step 215: [0 2]\n",
      "Ground truth at step 215: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 216: [[-0.87125081 -3.63184857 -0.23552474  6.08820629]\n",
      " [ 0.17971371  6.51486301 -1.00152707 -1.3637712 ]]\n",
      "Minibatch train loss at step 216: [ 0.00279905  0.00269205]\n",
      "Minibatch train loss mean at step 216: 0.0027455498930066824\n",
      "Minibatch train prediction at step 216: [3 1]\n",
      "Ground truth at step 216: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 217: [[ 6.83680916  0.22009489 -1.96816623 -1.23284876]\n",
      " [ 0.42904672  6.40676498 -1.17011821 -1.28558528]]\n",
      "Minibatch train loss at step 217: [ 0.00179904  0.00349696]\n",
      "Minibatch train loss mean at step 217: 0.002647999906912446\n",
      "Minibatch train prediction at step 217: [0 1]\n",
      "Ground truth at step 217: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 218: [[ 0.78795737  6.55945873 -1.29716539 -1.52428377]\n",
      " [-2.40004563 -3.08346963  5.81375265 -1.14555478]]\n",
      "Minibatch train loss at step 218: [ 0.00380352  0.00135652]\n",
      "Minibatch train loss mean at step 218: 0.002580018248409033\n",
      "Minibatch train prediction at step 218: [1 2]\n",
      "Ground truth at step 218: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 219: [[ 0.16781959  6.53673267 -1.00510561 -1.36433005]\n",
      " [ 6.8632617  -1.88879848 -2.09747386 -0.54477322]]\n",
      "Minibatch train loss at step 219: [ 0.00261144  0.0008926 ]\n",
      "Minibatch train loss mean at step 219: 0.0017520205583423376\n",
      "Minibatch train prediction at step 219: [1 0]\n",
      "Ground truth at step 219: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 220: [[ 6.52528906  0.56427193 -2.12477684 -1.23468733]\n",
      " [-2.40114832 -3.08732176  5.82498169 -1.15170097]]\n",
      "Minibatch train loss at step 220: [ 0.00317379  0.00133485]\n",
      "Minibatch train loss mean at step 220: 0.0022543210070580244\n",
      "Minibatch train prediction at step 220: [0 2]\n",
      "Ground truth at step 220: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 221: [[-0.87321281 -3.6322155  -0.24612522  6.11477661]\n",
      " [ 0.77062994  6.57796097 -1.29643571 -1.5257746 ]]\n",
      "Minibatch train loss at step 221: [ 0.00270549  0.00368143]\n",
      "Minibatch train loss mean at step 221: 0.0031934594735503197\n",
      "Minibatch train prediction at step 221: [3 1]\n",
      "Ground truth at step 221: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 222: [[ 6.87733173 -1.88662374 -2.09493256 -0.55975813]\n",
      " [ 6.53461933  0.56109506 -2.12658    -1.23214746]]\n",
      "Minibatch train loss at step 222: [ 0.00087176  0.00313707]\n",
      "Minibatch train loss mean at step 222: 0.0020044143311679363\n",
      "Minibatch train prediction at step 222: [0 0]\n",
      "Ground truth at step 222: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 223: [[-0.19148627 -1.16124022 -0.35957539  6.27707815]\n",
      " [ 6.88265324 -1.88562083 -2.09407425 -0.56553113]]\n",
      "Minibatch train loss at step 223: [ 0.00344517  0.00086401]\n",
      "Minibatch train loss mean at step 223: 0.002154590329155326\n",
      "Minibatch train prediction at step 223: [3 0]\n",
      "Ground truth at step 223: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 224: [[ 6.88872147 -1.88421381 -2.0930109  -0.57241994]\n",
      " [ 0.14540276  6.57690287 -1.01042867 -1.36239636]]\n",
      "Minibatch train loss at step 224: [ 0.00085508  0.0024703 ]\n",
      "Minibatch train loss mean at step 224: 0.0016626904252916574\n",
      "Minibatch train prediction at step 224: [0 1]\n",
      "Ground truth at step 224: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 225: [[ 6.89544296 -1.88251317 -2.09174323 -0.58029354]\n",
      " [ 6.86785173  0.21031259 -1.98280931 -1.24665868]]\n",
      "Minibatch train loss at step 225: [ 0.00084555  0.00172538]\n",
      "Minibatch train loss mean at step 225: 0.0012854640372097492\n",
      "Minibatch train prediction at step 225: [0 0]\n",
      "Ground truth at step 225: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 226: [[ 6.83790541 -1.59432805 -2.36297107 -1.43594694]\n",
      " [ 6.76921415  0.14264035 -2.03488398 -1.19815135]]\n",
      "Minibatch train loss at step 226: [ 0.00057359  0.00181962]\n",
      "Minibatch train loss mean at step 226: 0.0011966064339503646\n",
      "Minibatch train prediction at step 226: [0 0]\n",
      "Ground truth at step 226: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 227: [[ 6.84062672 -1.59547317 -2.36298037 -1.43738508]\n",
      " [ 6.7741251   0.13997848 -2.03595591 -1.1987493 ]]\n",
      "Minibatch train loss at step 227: [ 0.00057145  0.00180701]\n",
      "Minibatch train loss mean at step 227: 0.0011892274487763643\n",
      "Minibatch train prediction at step 227: [0 0]\n",
      "Ground truth at step 227: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 228: [[-2.62431526 -2.82601094  6.40021753 -1.83953059]\n",
      " [ 6.91570807 -1.87746406 -2.08783531 -0.60425442]]\n",
      "Minibatch train loss at step 228: [ 0.00048268  0.00081661]\n",
      "Minibatch train loss mean at step 228: 0.0006496444111689925\n",
      "Minibatch train prediction at step 228: [2 0]\n",
      "Ground truth at step 228: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 229: [[ 0.72491592  6.61791801 -1.29111898 -1.53184068]\n",
      " [ 6.78448725  0.13385557 -2.03776932 -1.20038998]]\n",
      "Minibatch train loss at step 229: [ 0.00340917  0.00177964]\n",
      "Minibatch train loss mean at step 229: 0.00259440578520298\n",
      "Minibatch train prediction at step 229: [1 0]\n",
      "Ground truth at step 229: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 230: [[-2.62629533 -2.82641459  6.40608454 -1.84348261]\n",
      " [ 6.92873001 -1.87424529 -2.08530021 -0.61973995]]\n",
      "Minibatch train loss at step 230: [ 0.00047863  0.00079862]\n",
      "Minibatch train loss mean at step 230: 0.000638625817373395\n",
      "Minibatch train prediction at step 230: [2 0]\n",
      "Ground truth at step 230: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 231: [[-2.40574503 -3.10446048  5.87226725 -1.17500508]\n",
      " [ 6.93553972 -1.87248266 -2.08394051 -0.62793636]]\n",
      "Minibatch train loss at step 231: [ 0.00124937  0.00078921]\n",
      "Minibatch train loss mean at step 231: 0.0010192892514169216\n",
      "Minibatch train prediction at step 231: [2 0]\n",
      "Ground truth at step 231: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 232: [[ 6.85923195 -1.6022532  -2.3635416  -1.44609702]\n",
      " [-0.19927019 -1.16235971 -0.36972606  6.32060289]]\n",
      "Minibatch train loss at step 232: [ 0.00055727  0.00327396]\n",
      "Minibatch train loss mean at step 232: 0.0019156151684001088\n",
      "Minibatch train prediction at step 232: [0 3]\n",
      "Ground truth at step 232: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 233: [[-2.52744651 -3.02900243  5.60003233 -0.63792956]\n",
      " [ 0.70197546  6.63480139 -1.2885561  -1.53273654]]\n",
      "Minibatch train loss at step 233: [ 0.00242499  0.00329143]\n",
      "Minibatch train loss mean at step 233: 0.002858211286365986\n",
      "Minibatch train prediction at step 233: [2 1]\n",
      "Ground truth at step 233: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 234: [[ 6.59522581  0.53170466 -2.13383007 -1.22135878]\n",
      " [ 0.35694155  6.50842619 -1.18970859 -1.29413342]]\n",
      "Minibatch train loss at step 234: [ 0.00288689  0.00298816]\n",
      "Minibatch train loss mean at step 234: 0.002937526907771826\n",
      "Minibatch train prediction at step 234: [0 1]\n",
      "Ground truth at step 234: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 235: [[ 6.08744621 -2.36930323 -1.90631104 -1.22240102]\n",
      " [-0.20267601 -1.16235042 -0.3738358   6.3367238 ]]\n",
      "Minibatch train loss at step 235: [ 0.00121817  0.00321182]\n",
      "Minibatch train loss mean at step 235: 0.002214995678514242\n",
      "Minibatch train prediction at step 235: [0 3]\n",
      "Ground truth at step 235: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 236: [[-2.63290763 -2.8280189   6.4280448  -1.85887611]\n",
      " [ 6.87395763 -1.60743654 -2.36383724 -1.45284343]]\n",
      "Minibatch train loss at step 236: [ 0.00046326  0.00054643]\n",
      "Minibatch train loss mean at step 236: 0.0005048422608524561\n",
      "Minibatch train prediction at step 236: [2 0]\n",
      "Ground truth at step 236: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 237: [[ 6.87851381 -1.60892665 -2.36394882 -1.45475805]\n",
      " [-2.6341424  -2.82829928  6.43224716 -1.8618784 ]]\n",
      "Minibatch train loss at step 237: [ 0.00054297  0.00046052]\n",
      "Minibatch train loss mean at step 237: 0.0005017443327233195\n",
      "Minibatch train prediction at step 237: [0 2]\n",
      "Ground truth at step 237: [0 2]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 238: [[-2.65163636 -1.98888445  6.42256975 -1.96503496]\n",
      " [ 6.8843236  -1.61069465 -2.36415267 -1.4570502 ]]\n",
      "Minibatch train loss at step 238: [ 0.00056442  0.00053892]\n",
      "Minibatch train loss mean at step 238: 0.0005516675300896168\n",
      "Minibatch train prediction at step 238: [2 0]\n",
      "Ground truth at step 238: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 239: [[ 6.61441708  0.52469683 -2.13815403 -1.21851599]\n",
      " [ 6.89127636 -1.61270177 -2.36447358 -1.4596858 ]]\n",
      "Minibatch train loss at step 239: [ 0.00281664  0.00053427]\n",
      "Minibatch train loss mean at step 239: 0.0016754565294831991\n",
      "Minibatch train prediction at step 239: [0 0]\n",
      "Ground truth at step 239: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 240: [[ 0.33648694  6.53686285 -1.1936121  -1.29543352]\n",
      " [ 6.82739449  0.1135366  -2.04923129 -1.20563793]]\n",
      "Minibatch train loss at step 240: [ 0.00286062  0.00167682]\n",
      "Minibatch train loss mean at step 240: 0.002268722280859947\n",
      "Minibatch train prediction at step 240: [1 0]\n",
      "Ground truth at step 240: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 241: [[ 0.09601485  6.6660428  -1.02336776 -1.36258006]\n",
      " [ 0.65819913  6.67038918 -1.28785944 -1.53012514]]\n",
      "Minibatch train loss at step 241: [ 0.00218308  0.00306827]\n",
      "Minibatch train loss mean at step 241: 0.002625672146677971\n",
      "Minibatch train prediction at step 241: [1 1]\n",
      "Ground truth at step 241: [1 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 242: [[-2.64028859 -2.82934284  6.45263767 -1.8764168 ]\n",
      " [-2.52895117 -3.04082227  5.64305115 -0.66903776]]\n",
      "Minibatch train loss at step 242: [ 0.00044682  0.00226349]\n",
      "Minibatch train loss mean at step 242: 0.0013551507145166397\n",
      "Minibatch train prediction at step 242: [2 2]\n",
      "Ground truth at step 242: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 243: [[-2.64161086 -2.82961392  6.45759153 -1.88002002]\n",
      " [ 6.63164854  0.51754904 -2.14116621 -1.21820498]]\n",
      "Minibatch train loss at step 243: [ 0.0004436   0.00275233]\n",
      "Minibatch train loss mean at step 243: 0.0015979630406945944\n",
      "Minibatch train prediction at step 243: [2 0]\n",
      "Ground truth at step 243: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 244: [[ 7.00099611 -1.86186433 -2.06785679 -0.70415807]\n",
      " [ 6.11595297 -2.37766695 -1.90748227 -1.2395997 ]]\n",
      "Minibatch train loss at step 244: [ 0.0007069  0.0011709]\n",
      "Minibatch train loss mean at step 244: 0.0009389013284817338\n",
      "Minibatch train prediction at step 244: [0 0]\n",
      "Ground truth at step 244: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 245: [[-2.64470673 -2.83013868  6.46839428 -1.88787556]\n",
      " [ 6.9305768  -1.62468565 -2.36630917 -1.47441888]]\n",
      "Minibatch train loss at step 245: [ 0.00043669  0.00050782]\n",
      "Minibatch train loss mean at step 245: 0.00047225464368239045\n",
      "Minibatch train prediction at step 245: [2 0]\n",
      "Ground truth at step 245: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 246: [[ 0.63298631  6.6947031  -1.28915739 -1.52901673]\n",
      " [ 6.12397814 -2.37990427 -1.90756881 -1.24476671]]\n",
      "Minibatch train loss at step 246: [ 0.00293527  0.00115769]\n",
      "Minibatch train loss mean at step 246: 0.0020464779809117317\n",
      "Minibatch train prediction at step 246: [1 0]\n",
      "Ground truth at step 246: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 247: [[-0.21227291 -1.16266668 -0.38900626  6.39441776]\n",
      " [-2.40388608 -3.1232152   5.94293642 -1.22495353]]\n",
      "Minibatch train loss at step 247: [ 0.00300159  0.00112292]\n",
      "Minibatch train loss mean at step 247: 0.0020622541196644306\n",
      "Minibatch train prediction at step 247: [3 2]\n",
      "Ground truth at step 247: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 248: [[-2.66169715 -1.98888707  6.44650507 -1.97388804]\n",
      " [-0.88007379 -3.63728285 -0.26081935  6.17523289]]\n",
      "Minibatch train loss at step 248: [ 0.00054797  0.00251715]\n",
      "Minibatch train loss mean at step 248: 0.0015325631247833371\n",
      "Minibatch train prediction at step 248: [2 3]\n",
      "Ground truth at step 248: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 249: [[ 6.85963154  0.09945922 -2.05840039 -1.21013331]\n",
      " [ 6.95650434  0.16903035 -2.0130527  -1.27288961]]\n",
      "Minibatch train loss at step 249: [ 0.00160458  0.00152055]\n",
      "Minibatch train loss mean at step 249: 0.0015625650994479656\n",
      "Minibatch train prediction at step 249: [0 0]\n",
      "Ground truth at step 249: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 250: [[ 6.65872383  0.50567573 -2.14647293 -1.21905458]\n",
      " [-2.66390324 -1.98962629  6.45213175 -1.97567201]]\n",
      "Minibatch train loss at step 250: [ 0.00265246  0.00054416]\n",
      "Minibatch train loss mean at step 250: 0.0015983118209987879\n",
      "Minibatch train prediction at step 250: [0 2]\n",
      "Ground truth at step 250: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 251: [[ 6.96542168  0.16356304 -2.01456141 -1.27492702]\n",
      " [ 6.66435051  0.50189018 -2.14687037 -1.21974063]]\n",
      "Minibatch train loss at step 251: [ 0.00150031  0.00262928]\n",
      "Minibatch train loss mean at step 251: 0.0020647961646318436\n",
      "Minibatch train prediction at step 251: [0 0]\n",
      "Ground truth at step 251: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 252: [[ 6.97250557  0.15858124 -2.01485324 -1.27660036]\n",
      " [-0.88110518 -3.63747096 -0.26409876  6.18578243]]\n",
      "Minibatch train loss at step 252: [ 0.00148377  0.00248469]\n",
      "Minibatch train loss mean at step 252: 0.0019842293113470078\n",
      "Minibatch train prediction at step 252: [0 3]\n",
      "Ground truth at step 252: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 253: [[ 6.15516567 -2.38971996 -1.90974593 -1.26120389]\n",
      " [ 6.97984886  0.15333851 -2.01498151 -1.27835619]]\n",
      "Minibatch train loss at step 253: [ 0.00110958  0.00146687]\n",
      "Minibatch train loss mean at step 253: 0.0012882235459983349\n",
      "Minibatch train prediction at step 253: [0 0]\n",
      "Ground truth at step 253: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 254: [[ 6.68780184  0.48369518 -2.14686537 -1.22373092]\n",
      " [-0.88141632 -3.63705516 -0.26864606  6.19515705]]\n",
      "Minibatch train loss at step 254: [ 0.00252988  0.00245425]\n",
      "Minibatch train loss mean at step 254: 0.0024920611176639795\n",
      "Minibatch train prediction at step 254: [0 3]\n",
      "Ground truth at step 254: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 255: [[-2.532763   -3.0563848   5.69796324 -0.70537794]\n",
      " [ 7.0393877  -1.86083114 -2.0617702  -0.74019712]]\n",
      "Minibatch train loss at step 255: [ 0.00207804  0.00066592]\n",
      "Minibatch train loss mean at step 255: 0.001371980644762516\n",
      "Minibatch train prediction at step 255: [2 0]\n",
      "Ground truth at step 255: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 256: [[ 7.00298119  0.13581227 -2.01483464 -1.28357828]\n",
      " [ 0.068672    6.72030497 -1.03224754 -1.37469876]]\n",
      "Minibatch train loss at step 256: [ 0.00141354  0.00202451]\n",
      "Minibatch train loss mean at step 256: 0.001719022518955171\n",
      "Minibatch train prediction at step 256: [0 1]\n",
      "Ground truth at step 256: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 257: [[-2.40709186 -3.13486671  5.97910357 -1.24347758]\n",
      " [ 0.06843709  6.72274303 -1.03288853 -1.37636578]]\n",
      "Minibatch train loss at step 257: [ 0.00106743  0.00201856]\n",
      "Minibatch train loss mean at step 257: 0.0015429924242198467\n",
      "Minibatch train prediction at step 257: [2 1]\n",
      "Ground truth at step 257: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 258: [[-2.53493166 -3.06055069  5.7112236  -0.71256894]\n",
      " [-2.67299199 -1.99279261  6.47488356 -1.98208451]]\n",
      "Minibatch train loss at step 258: [ 0.00203771  0.00052891]\n",
      "Minibatch train loss mean at step 258: 0.0012833117507398129\n",
      "Minibatch train prediction at step 258: [2 2]\n",
      "Ground truth at step 258: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 259: [[ 6.98400402 -1.64503658 -2.36874056 -1.49705338]\n",
      " [-2.66249394 -2.8332243   6.5217948  -1.9241749 ]]\n",
      "Minibatch train loss at step 259: [ 0.00047279  0.00040392]\n",
      "Minibatch train loss mean at step 259: 0.00043835508404299617\n",
      "Minibatch train prediction at step 259: [0 2]\n",
      "Ground truth at step 259: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 260: [[-0.21156517 -1.16899061 -0.40369266  6.44730806]\n",
      " [ 0.3077352   6.60559988 -1.2004782  -1.31331754]]\n",
      "Minibatch train loss at step 260: [ 0.00282936  0.00260788]\n",
      "Minibatch train loss mean at step 260: 0.0027186181396245956\n",
      "Minibatch train prediction at step 260: [3 1]\n",
      "Ground truth at step 260: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 261: [[ 0.30633149  6.60979843 -1.20087111 -1.31449461]\n",
      " [ 7.06005001 -1.8615135  -2.05788112 -0.75910872]]\n",
      "Minibatch train loss at step 261: [ 0.00259385  0.00064495]\n",
      "Minibatch train loss mean at step 261: 0.0016193992923945189\n",
      "Minibatch train prediction at step 261: [1 0]\n",
      "Ground truth at step 261: [1 0]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 262: [[-0.88382393 -3.63555384 -0.28225401  6.2294631 ]\n",
      " [ 6.99226141 -1.64843166 -2.36918449 -1.50092292]]\n",
      "Minibatch train loss at step 262: [ 0.00234936  0.00046743]\n",
      "Minibatch train loss mean at step 262: 0.0014083931455388665\n",
      "Minibatch train prediction at step 262: [3 0]\n",
      "Ground truth at step 262: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 263: [[ 0.30099845  6.62070465 -1.20138216 -1.31651676]\n",
      " [ 7.04131699  0.11126721 -2.01799893 -1.29149473]]\n",
      "Minibatch train loss at step 263: [ 0.00255508  0.0013339 ]\n",
      "Minibatch train loss mean at step 263: 0.001944490009918809\n",
      "Minibatch train prediction at step 263: [1 0]\n",
      "Ground truth at step 263: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 264: [[-2.66795564 -2.83454537  6.53933716 -1.93587327]\n",
      " [ 6.74758005  0.44243515 -2.15125608 -1.23723412]]\n",
      "Minibatch train loss at step 264: [ 0.00039379  0.00230131]\n",
      "Minibatch train loss mean at step 264: 0.0013475489104166627\n",
      "Minibatch train prediction at step 264: [2 0]\n",
      "Ground truth at step 264: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 265: [[ 7.05011606  0.10681096 -2.01940727 -1.29337025]\n",
      " [ 0.61495185  6.75040388 -1.29185462 -1.55353177]]\n",
      "Minibatch train loss at step 265: [ 0.00131735  0.00273022]\n",
      "Minibatch train loss mean at step 265: 0.002023781882598996\n",
      "Minibatch train prediction at step 265: [0 1]\n",
      "Ground truth at step 265: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 266: [[-2.4107852  -3.14755297  6.02186966 -1.26780331]\n",
      " [ 7.05485535  0.10424612 -2.02010512 -1.29422092]]\n",
      "Minibatch train loss at step 266: [ 0.00100395  0.0013083 ]\n",
      "Minibatch train loss mean at step 266: 0.0011561268474906683\n",
      "Minibatch train prediction at step 266: [2 0]\n",
      "Ground truth at step 266: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 267: [[ 0.04543065  6.76805019 -1.04251814 -1.39032292]\n",
      " [ 7.05991507  0.10122806 -2.02068067 -1.29501271]]\n",
      "Minibatch train loss at step 267: [ 0.0018934   0.00129854]\n",
      "Minibatch train loss mean at step 267: 0.001595966750755906\n",
      "Minibatch train prediction at step 267: [1 0]\n",
      "Ground truth at step 267: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 268: [[-2.67242885 -2.83557367  6.55290174 -1.94455397]\n",
      " [ 0.04228327  6.77366209 -1.0436635  -1.39166653]]\n",
      "Minibatch train loss at step 268: [ 0.00038604  0.00187817]\n",
      "Minibatch train loss mean at step 268: 0.0011321050114929676\n",
      "Minibatch train prediction at step 268: [2 1]\n",
      "Ground truth at step 268: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 269: [[ 7.01272631 -1.65692878 -2.3703053  -1.51041615]\n",
      " [-2.54266357 -3.07600427  5.76338148 -0.74361831]]\n",
      "Minibatch train loss at step 269: [ 0.00045456  0.00188316]\n",
      "Minibatch train loss mean at step 269: 0.001168861985206604\n",
      "Minibatch train prediction at step 269: [0 2]\n",
      "Ground truth at step 269: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 270: [[ 7.07434464  0.09364516 -2.02274466 -1.2973758 ]\n",
      " [ 6.9622798   0.02718484 -2.07274866 -1.23165727]]\n",
      "Minibatch train loss at step 270: [ 0.00127199  0.00136759]\n",
      "Minibatch train loss mean at step 270: 0.001319787697866559\n",
      "Minibatch train prediction at step 270: [0 0]\n",
      "Ground truth at step 270: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 271: [[-0.21634425 -1.17051673 -0.41566145  6.49028969]\n",
      " [ 6.21636152 -2.41413522 -1.91794634 -1.28536153]]\n",
      "Minibatch train loss at step 271: [ 0.00269193  0.00102348]\n",
      "Minibatch train loss mean at step 271: 0.0018577089067548513\n",
      "Minibatch train prediction at step 271: [3 0]\n",
      "Ground truth at step 271: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 272: [[ 0.60111797  6.78157473 -1.29558933 -1.56011891]\n",
      " [-2.68791485 -1.99332213  6.51602793 -1.99685156]]\n",
      "Minibatch train loss at step 272: [ 0.00261501  0.00050294]\n",
      "Minibatch train loss mean at step 272: 0.0015589733375236392\n",
      "Minibatch train prediction at step 272: [1 2]\n",
      "Ground truth at step 272: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 273: [[ 0.27362752  6.67679024 -1.20432162 -1.32600653]\n",
      " [ 0.59728539  6.78659725 -1.2962029  -1.55997884]]\n",
      "Minibatch train loss at step 273: [ 0.00236577  0.00259385]\n",
      "Minibatch train loss mean at step 273: 0.0024798072408884764\n",
      "Minibatch train prediction at step 273: [1 1]\n",
      "Ground truth at step 273: [1 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 274: [[ 6.79065561  0.42307875 -2.16109395 -1.25021327]\n",
      " [-2.41485047 -3.15610409  6.05411768 -1.28637946]]\n",
      "Minibatch train loss at step 274: [ 0.00216536  0.00095822]\n",
      "Minibatch train loss mean at step 274: 0.0015617888420820236\n",
      "Minibatch train prediction at step 274: [0 2]\n",
      "Ground truth at step 274: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 275: [[-2.415416   -3.1571734   6.05832958 -1.28888452]\n",
      " [ 0.01780955  6.81462765 -1.05232406 -1.39943266]]\n",
      "Minibatch train loss at step 275: [ 0.00095227  0.00176988]\n",
      "Minibatch train loss mean at step 275: 0.0013610749738290906\n",
      "Minibatch train prediction at step 275: [2 1]\n",
      "Ground truth at step 275: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 276: [[-2.41610789 -3.15838504  6.06312227 -1.29176378]\n",
      " [-2.68041062 -2.83780622  6.5806303  -1.96283388]]\n",
      "Minibatch train loss at step 276: [ 0.0009456   0.00037103]\n",
      "Minibatch train loss mean at step 276: 0.0006583136855624616\n",
      "Minibatch train prediction at step 276: [2 2]\n",
      "Ground truth at step 276: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 277: [[-0.88816392 -3.63396525 -0.29480848  6.26846123]\n",
      " [ 7.03237391 -1.66530573 -2.37181973 -1.51987743]]\n",
      "Minibatch train loss at step 277: [ 0.00223851  0.00044241]\n",
      "Minibatch train loss mean at step 277: 0.0013404575875028968\n",
      "Minibatch train prediction at step 277: [3 0]\n",
      "Ground truth at step 277: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 278: [[ 0.25216952  6.71069384 -1.20675266 -1.32782316]\n",
      " [-2.6947763  -1.99383938  6.53434134 -2.00341702]]\n",
      "Minibatch train loss at step 278: [ 0.00225171  0.00049186]\n",
      "Minibatch train loss mean at step 278: 0.0013717832043766975\n",
      "Minibatch train prediction at step 278: [1 2]\n",
      "Ground truth at step 278: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 279: [[ 7.1054225  -1.86648273 -2.04926586 -0.79973334]\n",
      " [-2.68388128 -2.83897042  6.59199905 -1.97001803]]\n",
      "Minibatch train loss at step 279: [ 0.00060135  0.00036495]\n",
      "Minibatch train loss mean at step 279: 0.0004831507394555956\n",
      "Minibatch train prediction at step 279: [0 2]\n",
      "Ground truth at step 279: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 280: [[ 0.24367328  6.7240181  -1.20736825 -1.32861185]\n",
      " [-2.69710851 -1.9943105   6.5408659  -2.00578475]]\n",
      "Minibatch train loss at step 280: [ 0.00220842  0.0004878 ]\n",
      "Minibatch train loss mean at step 280: 0.0013481099158525467\n",
      "Minibatch train prediction at step 280: [1 2]\n",
      "Ground truth at step 280: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 281: [[ 6.99666977  0.01485379 -2.08473492 -1.24029696]\n",
      " [-2.6983583  -1.99477148  6.54452467 -2.00710225]]\n",
      "Minibatch train loss at step 281: [ 0.00130616  0.00048554]\n",
      "Minibatch train loss mean at step 281: 0.0008958489634096622\n",
      "Minibatch train prediction at step 281: [0 2]\n",
      "Ground truth at step 281: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 282: [[ 6.81070375  0.42230061 -2.17097592 -1.25751674]\n",
      " [-2.42159939 -3.16616154  6.08929157 -1.30446446]]\n",
      "Minibatch train loss at step 282: [ 0.00211766  0.00091154]\n",
      "Minibatch train loss mean at step 282: 0.0015145952347666025\n",
      "Minibatch train prediction at step 282: [0 2]\n",
      "Ground truth at step 282: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 283: [[ 0.54722202  6.84425688 -1.30407691 -1.55557454]\n",
      " [-2.42253137 -3.16743445  6.09351254 -1.30647755]]\n",
      "Minibatch train loss at step 283: [ 0.00235316  0.00090618]\n",
      "Minibatch train loss mean at step 283: 0.0016296691028401256\n",
      "Minibatch train prediction at step 283: [1 2]\n",
      "Ground truth at step 283: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 284: [[-2.70227504 -1.99699891  6.55641413 -2.01144695]\n",
      " [-0.01661463  6.86835432 -1.0609355  -1.40730095]]\n",
      "Minibatch train loss at step 284: [ 0.00047815  0.00163636]\n",
      "Minibatch train loss mean at step 284: 0.0010572555474936962\n",
      "Minibatch train prediction at step 284: [2 1]\n",
      "Ground truth at step 284: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 285: [[ 7.04728222 -1.67164886 -2.37321925 -1.52804399]\n",
      " [ 6.81887436  0.42062509 -2.17405748 -1.26049292]]\n",
      "Minibatch train loss at step 285: [ 0.00043311  0.00209648]\n",
      "Minibatch train loss mean at step 285: 0.0012647965922951698\n",
      "Minibatch train prediction at step 285: [0 0]\n",
      "Ground truth at step 285: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 286: [[-0.89190418 -3.63256168 -0.30158377  6.29002905]\n",
      " [ 6.82250357  0.41909152 -2.17487764 -1.26166213]]\n",
      "Minibatch train loss at step 286: [ 0.00217868  0.00208589]\n",
      "Minibatch train loss mean at step 286: 0.002132285386323929\n",
      "Minibatch train prediction at step 286: [3 0]\n",
      "Ground truth at step 286: [3 0]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 287: [[ 7.05284119 -1.67385256 -2.37371659 -1.53059959]\n",
      " [ 0.21794353  6.76624107 -1.20841479 -1.33242798]]\n",
      "Minibatch train loss at step 287: [ 0.0004299  0.0020784]\n",
      "Minibatch train loss mean at step 287: 0.0012541469186544418\n",
      "Minibatch train prediction at step 287: [0 1]\n",
      "Ground truth at step 287: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 288: [[-2.70747805 -1.99987888  6.57160378 -2.01678443]\n",
      " [ 6.83137512  0.41400585 -2.17606044 -1.26433372]]\n",
      "Minibatch train loss at step 288: [ 0.00046886  0.00205817]\n",
      "Minibatch train loss mean at step 288: 0.00126351707149297\n",
      "Minibatch train prediction at step 288: [2 0]\n",
      "Ground truth at step 288: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 289: [[ 0.52444726  6.87134361 -1.308038   -1.55671322]\n",
      " [ 7.12661362 -1.86872983 -2.04647374 -0.81789631]]\n",
      "Minibatch train loss at step 289: [ 0.00224862  0.00058229]\n",
      "Minibatch train loss mean at step 289: 0.0014154526870697737\n",
      "Minibatch train prediction at step 289: [1 0]\n",
      "Ground truth at step 289: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 290: [[ 7.12929487 -1.86881173 -2.04622221 -0.82026517]\n",
      " [-2.42922211 -3.1755271   6.11911297 -1.31692815]]\n",
      "Minibatch train loss at step 290: [ 0.0005799   0.00087497]\n",
      "Minibatch train loss mean at step 290: 0.0007274375529959798\n",
      "Minibatch train prediction at step 290: [0 2]\n",
      "Ground truth at step 290: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 291: [[ 7.14640236  0.06344686 -2.03976846 -1.31209457]\n",
      " [ 7.06697273 -1.67907679 -2.37496758 -1.53660595]]\n",
      "Minibatch train loss at step 291: [ 0.00115304  0.00042179]\n",
      "Minibatch train loss mean at step 291: 0.0007874176371842623\n",
      "Minibatch train prediction at step 291: [0 0]\n",
      "Ground truth at step 291: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 292: [[-0.22988096 -1.17176044 -0.4262765   6.55132961]\n",
      " [ 0.20410505  6.79120064 -1.20826888 -1.33577728]]\n",
      "Minibatch train loss at step 292: [ 0.00250681  0.00200714]\n",
      "Minibatch train loss mean at step 292: 0.002256972249597311\n",
      "Minibatch train prediction at step 292: [3 1]\n",
      "Ground truth at step 292: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 293: [[  7.03629255e+00  -5.08127548e-03  -2.09449410e+00  -1.25137520e+00]\n",
      " [  7.13924789e+00  -1.86860800e+00  -2.04461074e+00  -8.29952300e-01]]\n",
      "Minibatch train loss at step 293: [ 0.00123401  0.00057085]\n",
      "Minibatch train loss mean at step 293: 0.0009024288156069815\n",
      "Minibatch train prediction at step 293: [0 0]\n",
      "Ground truth at step 293: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 294: [[ 0.505566    6.89259386 -1.31098938 -1.55770683]\n",
      " [ 7.14315128 -1.86840141 -2.04391813 -0.83389801]]\n",
      "Minibatch train loss at step 294: [ 0.00216845  0.00056739]\n",
      "Minibatch train loss mean at step 294: 0.0013679214753210545\n",
      "Minibatch train prediction at step 294: [1 0]\n",
      "Ground truth at step 294: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 295: [[ 7.0430727  -0.00843472 -2.09615779 -1.25324094]\n",
      " [ 6.26908112 -2.43648243 -1.92396581 -1.30842125]]\n",
      "Minibatch train loss at step 295: [ 0.0012221  0.0009537]\n",
      "Minibatch train loss mean at step 295: 0.001087899086996913\n",
      "Minibatch train prediction at step 295: [0 0]\n",
      "Ground truth at step 295: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 296: [[-2.43385673 -3.18118834  6.13618422 -1.32312632]\n",
      " [ 6.27245998 -2.43748999 -1.92419398 -1.31009054]]\n",
      "Minibatch train loss at step 296: [ 0.0008552   0.00094941]\n",
      "Minibatch train loss mean at step 296: 0.000902304076589644\n",
      "Minibatch train prediction at step 296: [2 0]\n",
      "Ground truth at step 296: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 297: [[ 0.18887092  6.81625223 -1.20837879 -1.33792198]\n",
      " [ 7.15674353 -1.86748123 -2.04143333 -0.84749353]]\n",
      "Minibatch train loss at step 297: [ 0.00193659  0.00055548]\n",
      "Minibatch train loss mean at step 297: 0.0012460330035537481\n",
      "Minibatch train prediction at step 297: [1 0]\n",
      "Ground truth at step 297: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 298: [[ 6.28169966 -2.4398756  -1.92447853 -1.31527185]\n",
      " [-0.89515185 -3.63028693 -0.31487229  6.32133627]]\n",
      "Minibatch train loss at step 298: [ 0.00093762  0.00209184]\n",
      "Minibatch train loss mean at step 298: 0.0015147295780479908\n",
      "Minibatch train prediction at step 298: [0 3]\n",
      "Ground truth at step 298: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 299: [[-0.05748257  6.93814421 -1.06825829 -1.42042756]\n",
      " [ 7.16675138 -1.86655343 -2.03998446 -0.85714841]]\n",
      "Minibatch train loss at step 299: [ 0.00148246  0.00054714]\n",
      "Minibatch train loss mean at step 299: 0.0010148001601919532\n",
      "Minibatch train prediction at step 299: [1 0]\n",
      "Ground truth at step 299: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 300: [[ 7.17225361 -1.8658936  -2.0392983  -0.86245495]\n",
      " [ 0.47902563  6.91957378 -1.3147819  -1.55694151]]\n",
      "Minibatch train loss at step 300: [ 0.00054249  0.00206698]\n",
      "Minibatch train loss mean at step 300: 0.0013047357788309455\n",
      "Minibatch train prediction at step 300: [0 1]\n",
      "Ground truth at step 300: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 301: [[-2.5700357  -3.1102016   5.87482166 -0.79669899]\n",
      " [-2.70124531 -2.8453021   6.64489889 -1.99950874]]\n",
      "Minibatch train loss at step 301: [ 0.00160541  0.00033885]\n",
      "Minibatch train loss mean at step 301: 0.0009721340029500425\n",
      "Minibatch train prediction at step 301: [2 2]\n",
      "Ground truth at step 301: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 302: [[-0.23706692 -1.17217052 -0.43416867  6.5828743 ]\n",
      " [ 7.06129503 -0.01472432 -2.10211062 -1.25936425]]\n",
      "Minibatch train loss at step 302: [ 0.00241393  0.00119257]\n",
      "Minibatch train loss mean at step 302: 0.001803253311663866\n",
      "Minibatch train prediction at step 302: [3 0]\n",
      "Ground truth at step 302: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 303: [[ 6.87855434  0.39558774 -2.18843651 -1.28022885]\n",
      " [ 6.30763054 -2.44654703 -1.92659068 -1.32782853]]\n",
      "Minibatch train loss at step 303: [ 0.00192897  0.00090582]\n",
      "Minibatch train loss mean at step 303: 0.0014173949602991343\n",
      "Minibatch train prediction at step 303: [0 0]\n",
      "Ground truth at step 303: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 304: [[ 7.10106468 -1.69178545 -2.37856412 -1.55300999]\n",
      " [ 6.88191032  0.3939341  -2.18924165 -1.28118396]]\n",
      "Minibatch train loss at step 304: [ 0.00040261  0.00191969]\n",
      "Minibatch train loss mean at step 304: 0.0011611493537202477\n",
      "Minibatch train prediction at step 304: [0 0]\n",
      "Ground truth at step 304: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 305: [[-0.0730397   6.96485043 -1.07128441 -1.42435801]\n",
      " [ 6.88650513  0.39088622 -2.18974161 -1.2823745 ]]\n",
      "Minibatch train loss at step 305: [ 0.00142782  0.00190589]\n",
      "Minibatch train loss mean at step 305: 0.0016668557655066252\n",
      "Minibatch train prediction at step 305: [1 0]\n",
      "Ground truth at step 305: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 306: [[ 6.89208221  0.38699245 -2.19013381 -1.28379571]\n",
      " [ 7.07543707 -0.02319398 -2.10507059 -1.2631737 ]]\n",
      "Minibatch train loss at step 306: [ 0.00188911  0.00116769]\n",
      "Minibatch train loss mean at step 306: 0.0015284002292901278\n",
      "Minibatch train prediction at step 306: [0 0]\n",
      "Ground truth at step 306: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 307: [[-0.07526024  6.97154474 -1.07217991 -1.42673004]\n",
      " [ 6.89926147  0.38136065 -2.19020748 -1.28557539]]\n",
      "Minibatch train loss at step 307: [ 0.00141568  0.00186674]\n",
      "Minibatch train loss mean at step 307: 0.001641211798414588\n",
      "Minibatch train prediction at step 307: [1 0]\n",
      "Ground truth at step 307: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 308: [[ 7.11335516 -1.69675922 -2.37965941 -1.55848444]\n",
      " [-0.23847656 -1.17455065 -0.44263175  6.60818768]]\n",
      "Minibatch train loss at step 308: [ 0.00039582  0.00234377]\n",
      "Minibatch train loss mean at step 308: 0.0013697913382202387\n",
      "Minibatch train prediction at step 308: [0 3]\n",
      "Ground truth at step 308: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 309: [[-0.89724576 -3.62936997 -0.3274796   6.35082054]\n",
      " [ 0.16386116  6.86722946 -1.20982718 -1.34499645]]\n",
      "Minibatch train loss at step 309: [ 0.0020138   0.00180701]\n",
      "Minibatch train loss mean at step 309: 0.0019104047678411007\n",
      "Minibatch train prediction at step 309: [3 1]\n",
      "Ground truth at step 309: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 310: [[ 6.34080219 -2.45607543 -1.92900074 -1.34411001]\n",
      " [ 7.12085247 -1.699705   -2.38021398 -1.56158423]]\n",
      "Minibatch train loss at step 310: [ 0.00086663  0.00039188]\n",
      "Minibatch train loss mean at step 310: 0.0006292584585025907\n",
      "Minibatch train prediction at step 310: [0 0]\n",
      "Ground truth at step 310: [0 0]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 311: [[ 7.21906137  0.02136426 -2.05149293 -1.33074057]\n",
      " [ 6.92626476  0.36110294 -2.19121599 -1.29204381]]\n",
      "Minibatch train loss at step 311: [ 0.00103551  0.0017863 ]\n",
      "Minibatch train loss mean at step 311: 0.0014109080657362938\n",
      "Minibatch train prediction at step 311: [0 0]\n",
      "Ground truth at step 311: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 312: [[-0.08042802  6.98937368 -1.07444882 -1.43342853]\n",
      " [ 7.10908651 -0.04961182 -2.10767746 -1.27161765]]\n",
      "Minibatch train loss at step 312: [ 0.00138402  0.00110601]\n",
      "Minibatch train loss mean at step 312: 0.0012450115755200386\n",
      "Minibatch train prediction at step 312: [1 0]\n",
      "Ground truth at step 312: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 313: [[ 0.16146201  6.88059998 -1.20997822 -1.34907544]\n",
      " [-0.89781791 -3.62768459 -0.33624199  6.36675692]]\n",
      "Minibatch train loss at step 313: [ 0.00177905  0.00197085]\n",
      "Minibatch train loss mean at step 313: 0.0018749483861029148\n",
      "Minibatch train prediction at step 313: [1 3]\n",
      "Ground truth at step 313: [1 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 314: [[ 7.12000036 -0.05792874 -2.10852122 -1.27450562]\n",
      " [-2.58020401 -3.12038779  5.9034152  -0.80456096]]\n",
      "Minibatch train loss at step 314: [ 0.00108707  0.00154733]\n",
      "Minibatch train loss mean at step 314: 0.0013172028120607138\n",
      "Minibatch train prediction at step 314: [0 2]\n",
      "Ground truth at step 314: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 315: [[ 7.12523317 -0.06190676 -2.10894251 -1.27592301]\n",
      " [ 0.15922657  6.8888588  -1.20993972 -1.35125399]]\n",
      "Minibatch train loss at step 315: [ 0.00107791  0.00176108]\n",
      "Minibatch train loss mean at step 315: 0.00141949113458395\n",
      "Minibatch train prediction at step 315: [0 1]\n",
      "Ground truth at step 315: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 316: [[ 7.1444788  -1.70891595 -2.38215256 -1.57140851]\n",
      " [-2.73487735 -2.00335789  6.6366539  -2.03568792]]\n",
      "Minibatch train loss at step 316: [ 0.00037961  0.00043323]\n",
      "Minibatch train loss mean at step 316: 0.00040642073145136237\n",
      "Minibatch train prediction at step 316: [0 2]\n",
      "Ground truth at step 316: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 317: [[ 7.14871025 -1.71046555 -2.3825655  -1.57310843]\n",
      " [-0.23990846 -1.17880857 -0.45779064  6.64997482]]\n",
      "Minibatch train loss at step 317: [ 0.00037758  0.00223232]\n",
      "Minibatch train loss mean at step 317: 0.0013049534754827619\n",
      "Minibatch train prediction at step 317: [0 3]\n",
      "Ground truth at step 317: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 318: [[ -2.58413839e+00  -3.12362289e+00   5.91271496e+00  -8.06926250e-01]\n",
      " [  7.25546265e+00  -4.54152422e-03  -2.05322480e+00  -1.34032452e+00]]\n",
      "Minibatch train loss at step 318: [ 0.00152888  0.00097811]\n",
      "Minibatch train loss mean at step 318: 0.0012534961570054293\n",
      "Minibatch train prediction at step 318: [2 0]\n",
      "Ground truth at step 318: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 319: [[ 7.15847254 -1.71383286 -2.3835094  -1.57676136]\n",
      " [ 7.14426851 -0.07537527 -2.11100912 -1.28115237]]\n",
      "Minibatch train loss at step 319: [ 0.0003727   0.00104635]\n",
      "Minibatch train loss mean at step 319: 0.0007095231558196247\n",
      "Minibatch train prediction at step 319: [0 0]\n",
      "Ground truth at step 319: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 320: [[-0.24156065 -1.18019772 -0.46372536  6.66543293]\n",
      " [ 7.14903498 -0.07890874 -2.11144304 -1.28243113]]\n",
      "Minibatch train loss at step 320: [ 0.00219117  0.00103849]\n",
      "Minibatch train loss mean at step 320: 0.0016148282447829843\n",
      "Minibatch train prediction at step 320: [3 0]\n",
      "Ground truth at step 320: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 321: [[-0.9003948  -3.62337661 -0.35237077  6.39831591]\n",
      " [ 6.38206911 -2.47041416 -1.93534458 -1.35972738]]\n",
      "Minibatch train loss at step 321: [ 0.00188911  0.00082125]\n",
      "Minibatch train loss mean at step 321: 0.0013551827287301421\n",
      "Minibatch train prediction at step 321: [3 0]\n",
      "Ground truth at step 321: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 322: [[ 7.17412567 -1.71916687 -2.38504028 -1.58249998]\n",
      " [ 7.27414227 -0.01794175 -2.05431128 -1.34427667]]\n",
      "Minibatch train loss at step 322: [ 0.00036519  0.00095   ]\n",
      "Minibatch train loss mean at step 322: 0.0006575974402949214\n",
      "Minibatch train prediction at step 322: [0 0]\n",
      "Ground truth at step 322: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 323: [[ 7.17987728 -1.72109175 -2.3856051  -1.58457327]\n",
      " [ 7.27880621 -0.02150645 -2.05442262 -1.34514892]]\n",
      "Minibatch train loss at step 323: [ 0.00036245  0.0009431 ]\n",
      "Minibatch train loss mean at step 323: 0.0006527731893584132\n",
      "Minibatch train prediction at step 323: [0 0]\n",
      "Ground truth at step 323: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 324: [[-2.45301414 -3.20313811  6.20152855 -1.34303188]\n",
      " [-2.74198341 -2.00358558  6.65294266 -2.03980637]]\n",
      "Minibatch train loss at step 324: [ 0.00078528  0.00042489]\n",
      "Minibatch train loss mean at step 324: 0.0006050858064554632\n",
      "Minibatch train prediction at step 324: [2 2]\n",
      "Ground truth at step 324: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 325: [[-0.24495545 -1.18294549 -0.475649    6.69476271]\n",
      " [ 0.45924774  6.98223019 -1.32625222 -1.58719051]]\n",
      "Minibatch train loss at step 325: [ 0.00211444  0.00190363]\n",
      "Minibatch train loss mean at step 325: 0.0020090355537831783\n",
      "Minibatch train prediction at step 325: [3 1]\n",
      "Ground truth at step 325: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 326: [[ 7.2539382  -1.86688435 -2.03397942 -0.93072575]\n",
      " [ 7.29185915 -0.03136078 -2.05480933 -1.34715974]]\n",
      "Minibatch train loss at step 326: [ 0.00048066  0.00092392]\n",
      "Minibatch train loss mean at step 326: 0.0007022887002676725\n",
      "Minibatch train prediction at step 326: [0 0]\n",
      "Ground truth at step 326: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 327: [[-0.2472973  -1.18395233 -0.48092714  6.70798969]\n",
      " [ 0.1494502   6.92453957 -1.20985901 -1.36076748]]\n",
      "Minibatch train loss at step 327: [ 0.00208006  0.00168599]\n",
      "Minibatch train loss mean at step 327: 0.0018830245826393366\n",
      "Minibatch train prediction at step 327: [3 1]\n",
      "Ground truth at step 327: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 328: [[ 7.01459169  0.29906678 -2.19912481 -1.31555104]\n",
      " [-2.45644951 -3.20613647  6.2112174  -1.34591126]]\n",
      "Minibatch train loss at step 328: [ 0.00155162  0.00077539]\n",
      "Minibatch train loss mean at step 328: 0.0011635050177574158\n",
      "Minibatch train prediction at step 328: [0 2]\n",
      "Ground truth at step 328: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 329: [[-0.25037003 -1.18501794 -0.48734066  6.72381115]\n",
      " [ 7.18684816 -0.10699443 -2.11595821 -1.29133725]]\n",
      "Minibatch train loss at step 329: [ 0.0020395   0.00097835]\n",
      "Minibatch train loss mean at step 329: 0.0015089228982105851\n",
      "Minibatch train prediction at step 329: [3 0]\n",
      "Ground truth at step 329: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 330: [[ 6.41114044 -2.48042822 -1.9400456  -1.37015772]\n",
      " [ 0.14550684  6.93353891 -1.21011114 -1.36193252]]\n",
      "Minibatch train loss at step 330: [ 0.00079088  0.00166611]\n",
      "Minibatch train loss mean at step 330: 0.0012284950353205204\n",
      "Minibatch train prediction at step 330: [0 1]\n",
      "Ground truth at step 330: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 331: [[-2.4592545  -3.20856524  6.21927118 -1.34847474]\n",
      " [-2.71655297 -2.84770107  6.68840647 -2.02322674]]\n",
      "Minibatch train loss at step 331: [ 0.00076729  0.00031907]\n",
      "Minibatch train loss mean at step 331: 0.0005431832396425307\n",
      "Minibatch train prediction at step 331: [2 2]\n",
      "Ground truth at step 331: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 332: [[-2.46028948 -3.20948124  6.22247744 -1.34972763]\n",
      " [ 7.26736259 -1.86765528 -2.03361177 -0.94124717]]\n",
      "Minibatch train loss at step 332: [ 0.00076384  0.00047124]\n",
      "Minibatch train loss mean at step 332: 0.0006175411399453878\n",
      "Minibatch train prediction at step 332: [2 0]\n",
      "Ground truth at step 332: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 333: [[ 7.03392124  0.28703514 -2.2032001  -1.31971931]\n",
      " [ 6.42180777 -2.48346734 -1.94103456 -1.37534058]]\n",
      "Minibatch train loss at step 333: [ 0.00150639  0.00077968]\n",
      "Minibatch train loss mean at step 333: 0.0011430339654907584\n",
      "Minibatch train prediction at step 333: [0 0]\n",
      "Ground truth at step 333: [0 0]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 334: [[-0.25800106 -1.1879257  -0.50300902  6.76262379]\n",
      " [ 6.42643309 -2.48467064 -1.94127142 -1.37776363]]\n",
      "Minibatch train loss at step 334: [ 0.00194301  0.0007748 ]\n",
      "Minibatch train loss mean at step 334: 0.001358904642984271\n",
      "Minibatch train prediction at step 334: [3 0]\n",
      "Ground truth at step 334: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 335: [[-2.71969318 -2.84853935  6.69784403 -2.02892876]\n",
      " [ 7.27615166 -1.86780155 -2.03275084 -0.94854987]]\n",
      "Minibatch train loss at step 335: [ 0.0003149   0.00046528]\n",
      "Minibatch train loss mean at step 335: 0.00039009307511150837\n",
      "Minibatch train prediction at step 335: [2 0]\n",
      "Ground truth at step 335: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 336: [[ 7.32872629 -0.05642453 -2.0591495  -1.35168374]\n",
      " [-2.75341153 -2.00713491  6.68000221 -2.04546094]]\n",
      "Minibatch train loss at step 336: [ 0.00087354  0.00041107]\n",
      "Minibatch train loss mean at step 336: 0.0006423050072044134\n",
      "Minibatch train prediction at step 336: [0 2]\n",
      "Ground truth at step 336: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 337: [[ 7.2302928  -1.73890078 -2.39091563 -1.6042136 ]\n",
      " [ 0.13714959  6.95266104 -1.21047509 -1.36418235]]\n",
      "Minibatch train loss at step 337: [ 0.00033921  0.00162458]\n",
      "Minibatch train loss mean at step 337: 0.0009818936232477427\n",
      "Minibatch train prediction at step 337: [0 1]\n",
      "Ground truth at step 337: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 338: [[ 7.21724939 -0.12779114 -2.12218714 -1.29721844]\n",
      " [-2.60499072 -3.14057803  5.968328   -0.82705724]]\n",
      "Minibatch train loss at step 338: [ 0.00093369  0.0014177 ]\n",
      "Minibatch train loss mean at step 338: 0.0011756961466744542\n",
      "Minibatch train prediction at step 338: [0 2]\n",
      "Ground truth at step 338: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 339: [[ 7.33850479 -0.06303689 -2.06033826 -1.35304916]\n",
      " [ 7.05562592  0.27261016 -2.20720387 -1.32433319]]\n",
      "Minibatch train loss at step 339: [ 0.0008608   0.00145615]\n",
      "Minibatch train loss mean at step 339: 0.0011584755266085267\n",
      "Minibatch train prediction at step 339: [0 0]\n",
      "Ground truth at step 339: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 340: [[ 6.45506239 -2.49184918 -1.94274855 -1.39289117]\n",
      " [ 7.34287214 -0.06649213 -2.06048226 -1.35371661]]\n",
      "Minibatch train loss at step 340: [ 0.00074573  0.00085484]\n",
      "Minibatch train loss mean at step 340: 0.0008002878166735172\n",
      "Minibatch train prediction at step 340: [0 0]\n",
      "Ground truth at step 340: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 341: [[ 7.34742498 -0.07017051 -2.06053019 -1.35438287]\n",
      " [ 7.2412591  -1.74287283 -2.39235783 -1.60871935]]\n",
      "Minibatch train loss at step 341: [ 0.00084853  0.00033421]\n",
      "Minibatch train loss mean at step 341: 0.0005913681234233081\n",
      "Minibatch train prediction at step 341: [0 0]\n",
      "Ground truth at step 341: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 342: [[-2.75840092 -2.00983739  6.69460487 -2.04993725]\n",
      " [ 7.07018614  0.26124254 -2.20811844 -1.32783282]]\n",
      "Minibatch train loss at step 342: [ 0.00040356  0.00142163]\n",
      "Minibatch train loss mean at step 342: 0.0009125969372689724\n",
      "Minibatch train prediction at step 342: [2 0]\n",
      "Ground truth at step 342: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 343: [[-0.26668543 -1.19369733 -0.52295834  6.81404209]\n",
      " [-2.72615671 -2.84997559  6.71813154 -2.04182291]]\n",
      "Minibatch train loss at step 343: [ 0.00182343  0.00030584]\n",
      "Minibatch train loss mean at step 343: 0.001064637559466064\n",
      "Minibatch train prediction at step 343: [3 2]\n",
      "Ground truth at step 343: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 344: [[ 7.25182438 -1.74661648 -2.39371681 -1.61266136]\n",
      " [-2.76009536 -2.01125884  6.6997838  -2.05145359]]\n",
      "Minibatch train loss at step 344: [ 0.00032956  0.00040094]\n",
      "Minibatch train loss mean at step 344: 0.00036524952156469226\n",
      "Minibatch train prediction at step 344: [0 2]\n",
      "Ground truth at step 344: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 345: [[-2.61027503 -3.14638638  5.99109125 -0.84034085]\n",
      " [-0.26818305 -1.19532049 -0.5271067   6.82487106]]\n",
      "Minibatch train loss at step 345: [ 0.00136985  0.00179928]\n",
      "Minibatch train loss mean at step 345: 0.0015845620073378086\n",
      "Minibatch train prediction at step 345: [2 3]\n",
      "Ground truth at step 345: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 346: [[-0.26939642 -1.19624877 -0.52972651  6.83164358]\n",
      " [-2.47180438 -3.2209549   6.26591396 -1.37129843]]\n",
      "Minibatch train loss at step 346: [ 0.00178416  0.00071822]\n",
      "Minibatch train loss mean at step 346: 0.0012511895038187504\n",
      "Minibatch train prediction at step 346: [3 2]\n",
      "Ground truth at step 346: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 347: [[ 7.37349463 -0.09223007 -2.06085682 -1.35751462]\n",
      " [ 0.13512281  6.96819782 -1.20990491 -1.36913323]]\n",
      "Minibatch train loss at step 347: [ 0.00081351  0.00159637]\n",
      "Minibatch train loss mean at step 347: 0.0012049394426867366\n",
      "Minibatch train prediction at step 347: [0 1]\n",
      "Ground truth at step 347: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 348: [[ 0.45637178  7.01783514 -1.33514321 -1.60772216]\n",
      " [ 7.31029129 -1.87043679 -2.02805591 -0.97650641]]\n",
      "Minibatch train loss at step 348: [ 0.00182724  0.00044265]\n",
      "Minibatch train loss mean at step 348: 0.0011349419364705682\n",
      "Minibatch train prediction at step 348: [1 0]\n",
      "Ground truth at step 348: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 349: [[ 7.26719952 -1.75189626 -2.395679   -1.61834705]\n",
      " [-0.11036117  7.08601332 -1.07909131 -1.46394885]]\n",
      "Minibatch train loss at step 349: [ 0.00032312  0.00122663]\n",
      "Minibatch train loss mean at step 349: 0.0007748751668259501\n",
      "Minibatch train prediction at step 349: [0 1]\n",
      "Ground truth at step 349: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 350: [[ 0.45147839  7.02391863 -1.33629    -1.60855734]\n",
      " [ 7.2706213  -1.75299668 -2.39613438 -1.61959183]]\n",
      "Minibatch train loss at step 350: [ 0.00180891  0.00032146]\n",
      "Minibatch train loss mean at step 350: 0.0010651847114786506\n",
      "Minibatch train prediction at step 350: [1 0]\n",
      "Ground truth at step 350: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 351: [[ 7.10521889  0.23581377 -2.21318293 -1.33472443]\n",
      " [ 7.31753778 -1.87105012 -2.02681899 -0.9827767 ]]\n",
      "Minibatch train loss at step 351: [ 0.00134402  0.000438  ]\n",
      "Minibatch train loss mean at step 351: 0.0008910069009289145\n",
      "Minibatch train prediction at step 351: [0 0]\n",
      "Ground truth at step 351: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 352: [[ 0.12577942  6.98439741 -1.21077764 -1.36965597]\n",
      " [-2.61660957 -3.15290093  6.01730633 -0.85619467]]\n",
      "Minibatch train loss at step 352: [ 0.00156054  0.00131604]\n",
      "Minibatch train loss mean at step 352: 0.0014382905792444944\n",
      "Minibatch train prediction at step 352: [1 2]\n",
      "Ground truth at step 352: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 353: [[-0.91796839 -3.62268257 -0.36359149  6.4552598 ]\n",
      " [ 0.43814117  7.03689337 -1.3381722  -1.60927606]]\n",
      "Minibatch train loss at step 353: [ 0.00176131  0.00176691]\n",
      "Minibatch train loss mean at step 353: 0.001764111453667283\n",
      "Minibatch train prediction at step 353: [3 1]\n",
      "Ground truth at step 353: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 354: [[ 0.11920445  6.99405193 -1.21108401 -1.36958539]\n",
      " [ 7.11226988  0.23352836 -2.21611786 -1.33649158]]\n",
      "Minibatch train loss at step 354: [ 0.00153864  0.00133163]\n",
      "Minibatch train loss mean at step 354: 0.0014351382851600647\n",
      "Minibatch train prediction at step 354: [1 0]\n",
      "Ground truth at step 354: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 355: [[-2.47972631 -3.22775865  6.29588223 -1.38762939]\n",
      " [ 0.42538998  7.04826641 -1.33961725 -1.609496  ]]\n",
      "Minibatch train loss at step 355: [ 0.00068772  0.00172942]\n",
      "Minibatch train loss mean at step 355: 0.0012085714843124151\n",
      "Minibatch train prediction at step 355: [2 1]\n",
      "Ground truth at step 355: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 356: [[-2.48080444 -3.2285459   6.29940081 -1.38936377]\n",
      " [ 7.11639881  0.23307618 -2.21835971 -1.33776534]]\n",
      "Minibatch train loss at step 356: [ 0.00068427  0.00132521]\n",
      "Minibatch train loss mean at step 356: 0.0010047355899587274\n",
      "Minibatch train prediction at step 356: [2 0]\n",
      "Ground truth at step 356: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 357: [[ 6.5076232  -2.50747085 -1.94501543 -1.42124343]\n",
      " [-2.77219105 -2.01751828  6.73454046 -2.06184888]]\n",
      "Minibatch train loss at step 357: [ 0.00069499  0.00038366]\n",
      "Minibatch train loss mean at step 357: 0.0005393240717239678\n",
      "Minibatch train prediction at step 357: [0 2]\n",
      "Ground truth at step 357: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 358: [[ 0.1040045   7.01573324 -1.21125114 -1.36953843]\n",
      " [-0.13416588  7.12382078 -1.0815984  -1.46607769]]\n",
      "Minibatch train loss at step 358: [ 0.00149044  0.00116304]\n",
      "Minibatch train loss mean at step 358: 0.0013267399044707417\n",
      "Minibatch train prediction at step 358: [1 1]\n",
      "Ground truth at step 358: [1 1]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 359: [[ 6.51371813 -2.50940585 -1.94549477 -1.42424881]\n",
      " [-0.92102426 -3.62061238 -0.36765715  6.46806002]]\n",
      "Minibatch train loss at step 359: [ 0.00068927  0.00173275]\n",
      "Minibatch train loss mean at step 359: 0.0012110117822885513\n",
      "Minibatch train prediction at step 359: [0 3]\n",
      "Ground truth at step 359: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 360: [[-2.775213   -2.01833606  6.74277782 -2.06461406]\n",
      " [-2.62521744 -3.16043806  6.04692793 -0.87203449]]\n",
      "Minibatch train loss at step 360: [ 0.00037973  0.00125961]\n",
      "Minibatch train loss mean at step 360: 0.0008196674752980471\n",
      "Minibatch train prediction at step 360: [2 2]\n",
      "Ground truth at step 360: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 361: [[ 7.341609   -1.87305164 -2.02451587 -1.00209892]\n",
      " [-0.92195827 -3.61909795 -0.37152478  6.47611332]]\n",
      "Minibatch train loss at step 361: [ 0.00042298  0.00171419]\n",
      "Minibatch train loss mean at step 361: 0.0010685871820896864\n",
      "Minibatch train prediction at step 361: [0 3]\n",
      "Ground truth at step 361: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 362: [[ 0.38217089  7.0872674  -1.34483516 -1.61051106]\n",
      " [ 7.30169725 -1.76428425 -2.40042591 -1.63294363]]\n",
      "Minibatch train loss at step 362: [ 0.00160815  0.00030835]\n",
      "Minibatch train loss mean at step 362: 0.0009582488564774394\n",
      "Minibatch train prediction at step 362: [1 0]\n",
      "Ground truth at step 362: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 363: [[-0.14954059  7.1483469  -1.0831151  -1.46786833]\n",
      " [ 0.08559928  7.04325199 -1.2116183  -1.37030303]]\n",
      "Minibatch train loss at step 363: [ 0.00112375  0.00143211]\n",
      "Minibatch train loss mean at step 363: 0.0012779291719198227\n",
      "Minibatch train prediction at step 363: [1 1]\n",
      "Ground truth at step 363: [1 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 364: [[ 7.34945154 -1.87293458 -2.0249052  -1.00775659]\n",
      " [ 6.53261995 -2.51536751 -1.94878423 -1.43049848]]\n",
      "Minibatch train loss at step 364: [ 0.00041834  0.00067283]\n",
      "Minibatch train loss mean at step 364: 0.0005455831415019929\n",
      "Minibatch train prediction at step 364: [0 0]\n",
      "Ground truth at step 364: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 365: [[-2.49092913 -3.23771429  6.33113003 -1.4021672 ]\n",
      " [-0.15687135  7.15995502 -1.08397877 -1.46860969]]\n",
      "Minibatch train loss at step 365: [ 0.00065508  0.00110541]\n",
      "Minibatch train loss mean at step 365: 0.0008802457014098763\n",
      "Minibatch train prediction at step 365: [2 1]\n",
      "Ground truth at step 365: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 366: [[ 7.3557806  -1.87274611 -2.02488518 -1.01262832]\n",
      " [ 0.07280339  7.0625782  -1.21222889 -1.37084508]]\n",
      "Minibatch train loss at step 366: [ 0.00041452  0.00139259]\n",
      "Minibatch train loss mean at step 366: 0.0009035552502609789\n",
      "Minibatch train prediction at step 366: [0 1]\n",
      "Ground truth at step 366: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 367: [[-2.63385487 -3.16853762  6.07149553 -0.88105935]\n",
      " [ 7.41089964 -0.09936456 -2.07752728 -1.37358749]]\n",
      "Minibatch train loss at step 367: [ 0.00121817  0.00077587]\n",
      "Minibatch train loss mean at step 367: 0.000997021677903831\n",
      "Minibatch train prediction at step 367: [2 0]\n",
      "Ground truth at step 367: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 368: [[ 6.54859877 -2.52000618 -1.9511745  -1.43670774]\n",
      " [ 7.29054403 -0.16330242 -2.14558411 -1.31865036]]\n",
      "Minibatch train loss at step 368: [ 0.00065901  0.00084103]\n",
      "Minibatch train loss mean at step 368: 0.0007500178180634975\n",
      "Minibatch train prediction at step 368: [0 0]\n",
      "Ground truth at step 368: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 369: [[ 7.36618519 -1.87261844 -2.02413797 -1.02118278]\n",
      " [ 6.55322886 -2.52131009 -1.95167506 -1.43879652]]\n",
      "Minibatch train loss at step 369: [ 0.00040833  0.00065496]\n",
      "Minibatch train loss mean at step 369: 0.0005316436872817576\n",
      "Minibatch train prediction at step 369: [0 0]\n",
      "Ground truth at step 369: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 370: [[ 7.13376093  0.24077103 -2.23400736 -1.34756863]\n",
      " [ 0.33829474  7.13108921 -1.35167813 -1.6105907 ]]\n",
      "Minibatch train loss at step 370: [ 0.00130675  0.00148746]\n",
      "Minibatch train loss mean at step 370: 0.001397106098011136\n",
      "Minibatch train prediction at step 370: [0 1]\n",
      "Ground truth at step 370: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 371: [[ 0.05490419  7.09142637 -1.21306038 -1.37237203]\n",
      " [-2.63790488 -3.17293644  6.08737469 -0.88990951]]\n",
      "Minibatch train loss at step 371: [ 0.00133663  0.00118972]\n",
      "Minibatch train loss mean at step 371: 0.0012631751596927643\n",
      "Minibatch train prediction at step 371: [1 2]\n",
      "Ground truth at step 371: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 372: [[ 6.56889009 -2.52557349 -1.95318401 -1.44608498]\n",
      " [ 7.31692457 -1.77024603 -2.4033308  -1.64185584]]\n",
      "Minibatch train loss at step 372: [ 0.00064162  0.00030179]\n",
      "Minibatch train loss mean at step 372: 0.0004717046394944191\n",
      "Minibatch train prediction at step 372: [0 0]\n",
      "Ground truth at step 372: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 373: [[ 6.57454967 -2.52710724 -1.95368087 -1.44882536]\n",
      " [ 0.32413808  7.14585066 -1.35385001 -1.61084938]]\n",
      "Minibatch train loss at step 373: [ 0.00063673  0.00144961]\n",
      "Minibatch train loss mean at step 373: 0.0010431695263832808\n",
      "Minibatch train prediction at step 373: [0 1]\n",
      "Ground truth at step 373: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 374: [[ 7.32145739 -1.77172101 -2.40423417 -1.64396822]\n",
      " [-2.74449277 -2.85726786  6.78412485 -2.08133316]]\n",
      "Minibatch train loss at step 374: [ 0.00029989  0.00027879]\n",
      "Minibatch train loss mean at step 374: 0.0002893386408686638\n",
      "Minibatch train prediction at step 374: [0 2]\n",
      "Ground truth at step 374: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 375: [[-2.78777075 -2.02052236  6.78059149 -2.07969117]\n",
      " [-0.92670196 -3.61147547 -0.39148489  6.52105904]]\n",
      "Minibatch train loss at step 375: [ 0.00036221  0.00161648]\n",
      "Minibatch train loss mean at step 375: 0.0009893467649817467\n",
      "Minibatch train prediction at step 375: [2 3]\n",
      "Ground truth at step 375: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 376: [[-0.92690098 -3.61079097 -0.39332378  6.52495432]\n",
      " [-2.64224863 -3.17809033  6.10742569 -0.90246964]]\n",
      "Minibatch train loss at step 376: [ 0.00160827  0.00115352]\n",
      "Minibatch train loss mean at step 376: 0.0013808943331241608\n",
      "Minibatch train prediction at step 376: [3 2]\n",
      "Ground truth at step 376: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 377: [[-0.92727584 -3.60957098 -0.39616913  6.5308671 ]\n",
      " [ 7.30239248 -0.1616476  -2.15344357 -1.32557678]]\n",
      "Minibatch train loss at step 377: [ 0.00159577  0.00083019]\n",
      "Minibatch train loss mean at step 377: 0.0012129796668887138\n",
      "Minibatch train prediction at step 377: [3 0]\n",
      "Ground truth at step 377: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 378: [[ 0.29989284  7.16839314 -1.35702109 -1.60945034]\n",
      " [ 6.60261106 -2.53490686 -1.95762289 -1.45970893]]\n",
      "Minibatch train loss at step 378: [ 0.00139152  0.0006141 ]\n",
      "Minibatch train loss mean at step 378: 0.001002806005999446\n",
      "Minibatch train prediction at step 378: [1 0]\n",
      "Ground truth at step 378: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 379: [[ 0.03036142  7.12924862 -1.21377397 -1.37464571]\n",
      " [-2.64666653 -3.18173122  6.11854172 -0.90602475]]\n",
      "Minibatch train loss at step 379: [ 0.00126592  0.00113661]\n",
      "Minibatch train loss mean at step 379: 0.0012012633960694075\n",
      "Minibatch train prediction at step 379: [1 2]\n",
      "Ground truth at step 379: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 380: [[ 7.30544472 -0.16138312 -2.15561724 -1.32760513]\n",
      " [ 6.61369276 -2.53812432 -1.95969558 -1.46308112]]\n",
      "Minibatch train loss at step 380: [ 0.00082733  0.00060552]\n",
      "Minibatch train loss mean at step 380: 0.0007164233829826117\n",
      "Minibatch train prediction at step 380: [0 0]\n",
      "Ground truth at step 380: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 381: [[ 7.33652353 -1.77721643 -2.4067564  -1.65155602]\n",
      " [ 6.61976957 -2.53987741 -1.96069157 -1.46519887]]\n",
      "Minibatch train loss at step 381: [ 0.00029369  0.00060087]\n",
      "Minibatch train loss mean at step 381: 0.0004472806176636368\n",
      "Minibatch train prediction at step 381: [0 0]\n",
      "Ground truth at step 381: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 382: [[-0.92880297 -3.60301042 -0.41195044  6.56305981]\n",
      " [ 7.40625381 -1.87309837 -2.02353525 -1.05016685]]\n",
      "Minibatch train loss at step 382: [ 0.00152983  0.00038616]\n",
      "Minibatch train loss mean at step 382: 0.0009579987963661551\n",
      "Minibatch train prediction at step 382: [3 0]\n",
      "Ground truth at step 382: [3 0]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 383: [[-0.31247017 -1.200261   -0.55577981  6.93837118]\n",
      " [ 0.27727866  7.18872929 -1.35979843 -1.60756218]]\n",
      "Minibatch train loss at step 383: [ 0.00155673  0.00134056]\n",
      "Minibatch train loss mean at step 383: 0.0014486485160887241\n",
      "Minibatch train prediction at step 383: [3 1]\n",
      "Ground truth at step 383: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 384: [[ 7.41236591 -1.87283492 -2.0241344  -1.05414128]\n",
      " [-2.65343595 -3.18755531  6.13738108 -0.91338909]]\n",
      "Minibatch train loss at step 384: [ 0.00038283  0.00110744]\n",
      "Minibatch train loss mean at step 384: 0.0007451319834217429\n",
      "Minibatch train prediction at step 384: [0 2]\n",
      "Ground truth at step 384: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 385: [[-0.92980981 -3.59943032 -0.42098135  6.58260202]\n",
      " [-2.51168561 -3.25700641  6.40006685 -1.43304753]]\n",
      "Minibatch train loss at step 385: [ 0.00149163  0.00059492]\n",
      "Minibatch train loss mean at step 385: 0.0010432706912979484\n",
      "Minibatch train prediction at step 385: [3 2]\n",
      "Ground truth at step 385: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 386: [[-2.79619789 -2.02202868  6.80422068 -2.08908391]\n",
      " [-2.51325059 -3.25819063  6.40410089 -1.43441713]]\n",
      "Minibatch train loss at step 386: [ 0.00035184  0.0005917 ]\n",
      "Minibatch train loss mean at step 386: 0.00047177146188914776\n",
      "Minibatch train prediction at step 386: [2 2]\n",
      "Ground truth at step 386: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 387: [[-2.65858006 -3.19147325  6.15072107 -0.91865063]\n",
      " [ 7.34804773 -1.78154719 -2.40871    -1.65778601]]\n",
      "Minibatch train loss at step 387: [ 0.00108719  0.00028892]\n",
      "Minibatch train loss mean at step 387: 0.0006880576256662607\n",
      "Minibatch train prediction at step 387: [2 0]\n",
      "Ground truth at step 387: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 388: [[ 7.43045139 -0.091189   -2.09391999 -1.39763141]\n",
      " [ 7.35024071 -1.78227007 -2.40912962 -1.65884376]]\n",
      "Minibatch train loss at step 388: [ 0.0007605   0.00028809]\n",
      "Minibatch train loss mean at step 388: 0.000524295901414007\n",
      "Minibatch train prediction at step 388: [0 0]\n",
      "Ground truth at step 388: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 389: [[-0.21550068  7.25712156 -1.09106529 -1.47771585]\n",
      " [ 7.42788172 -1.87237668 -2.02420282 -1.06548524]]\n",
      "Minibatch train loss at step 389: [ 0.00096572  0.00037472]\n",
      "Minibatch train loss mean at step 389: 0.0006702243117615581\n",
      "Minibatch train prediction at step 389: [1 0]\n",
      "Ground truth at step 389: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 390: [[ 7.35665178 -1.78424871 -2.41023469 -1.6614219 ]\n",
      " [ 7.43114233 -1.87232721 -2.02404118 -1.06803405]]\n",
      "Minibatch train loss at step 390: [ 0.0002857   0.00037294]\n",
      "Minibatch train loss mean at step 390: 0.0003293200861662626\n",
      "Minibatch train prediction at step 390: [0 0]\n",
      "Ground truth at step 390: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 391: [[ 7.15076828  0.25052318 -2.25328302 -1.35985076]\n",
      " [ 7.31592894 -0.16154228 -2.16347623 -1.33231127]]\n",
      "Minibatch train loss at step 391: [ 0.00129044  0.0008172 ]\n",
      "Minibatch train loss mean at step 391: 0.0010538226924836636\n",
      "Minibatch train prediction at step 391: [0 0]\n",
      "Ground truth at step 391: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 392: [[-2.6666708  -3.1972692   6.17454147 -0.93116575]\n",
      " [-2.75761175 -2.86281323  6.82319498 -2.10479331]]\n",
      "Minibatch train loss at step 392: [ 0.00104956  0.00026378]\n",
      "Minibatch train loss mean at step 392: 0.0006566694937646389\n",
      "Minibatch train prediction at step 392: [2 2]\n",
      "Ground truth at step 392: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 393: [[ 7.32038021 -0.16510129 -2.16429329 -1.33234096]\n",
      " [-0.93495429 -3.59214973 -0.43553504  6.62295055]]\n",
      "Minibatch train loss at step 393: [ 0.00081149  0.0014177 ]\n",
      "Minibatch train loss mean at step 393: 0.001114595215767622\n",
      "Minibatch train prediction at step 393: [0 3]\n",
      "Ground truth at step 393: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 394: [[ 7.37125778 -1.78882289 -2.41258025 -1.66697252]\n",
      " [-2.75938225 -2.8632915   6.82881021 -2.10845327]]\n",
      "Minibatch train loss at step 394: [ 0.00028022  0.00026163]\n",
      "Minibatch train loss mean at step 394: 0.0002709259861148894\n",
      "Minibatch train prediction at step 394: [0 2]\n",
      "Ground truth at step 394: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 395: [[-0.22048557  7.26929808 -1.09223366 -1.48059928]\n",
      " [ 6.68281603 -2.55845833 -1.97068524 -1.48585093]]\n",
      "Minibatch train loss at step 395: [ 0.00095036  0.00055465]\n",
      "Minibatch train loss mean at step 395: 0.0007525038672611117\n",
      "Minibatch train prediction at step 395: [1 0]\n",
      "Ground truth at step 395: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 396: [[  1.15736667e-03   7.17819738e+00  -1.21588933e+00  -1.38006139e+00]\n",
      " [  7.45054817e+00  -1.87237549e+00  -2.02295470e+00  -1.08318222e+00]]\n",
      "Minibatch train loss at step 396: [ 0.00118138  0.00036293]\n",
      "Minibatch train loss mean at step 396: 0.0007721537840552628\n",
      "Minibatch train prediction at step 396: [1 0]\n",
      "Ground truth at step 396: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 397: [[-0.32281393 -1.2029357  -0.56225675  6.96684551]\n",
      " [ 6.69013262 -2.56071258 -1.97183609 -1.48829615]]\n",
      "Minibatch train loss at step 397: [ 0.00150174  0.00054964]\n",
      "Minibatch train loss mean at step 397: 0.001025692792609334\n",
      "Minibatch train prediction at step 397: [3 0]\n",
      "Ground truth at step 397: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 398: [[ 7.33046246 -0.17265129 -2.16639423 -1.33291221]\n",
      " [-2.7633245  -2.8645556   6.84054947 -2.11594248]]\n",
      "Minibatch train loss at step 398: [ 0.00079898  0.00025722]\n",
      "Minibatch train loss mean at step 398: 0.000528099772054702\n",
      "Minibatch train prediction at step 398: [0 2]\n",
      "Ground truth at step 398: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 399: [[-0.2243949   7.27925205 -1.09327626 -1.4827311 ]\n",
      " [-2.67616463 -3.20393109  6.20195389 -0.94500059]]\n",
      "Minibatch train loss at step 399: [ 0.00093833  0.00100836]\n",
      "Minibatch train loss mean at step 399: 0.0009733461774885654\n",
      "Minibatch train prediction at step 399: [1 2]\n",
      "Ground truth at step 399: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 400: [[-0.32523477 -1.2039274  -0.56530166  6.97548246]\n",
      " [ 7.39005613 -1.79487276 -2.41552019 -1.67438948]]\n",
      "Minibatch train loss at step 400: [ 0.00148532  0.00027343]\n",
      "Minibatch train loss mean at step 400: 0.0008793728775344789\n",
      "Minibatch train prediction at step 400: [3 0]\n",
      "Ground truth at step 400: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 401: [[-0.22663647  7.2850194  -1.09407854 -1.48380494]\n",
      " [-2.53331113 -3.27103376  6.46037912 -1.46136725]]\n",
      "Minibatch train loss at step 401: [ 0.00093131  0.00054619]\n",
      "Minibatch train loss mean at step 401: 0.0007387464866042137\n",
      "Minibatch train prediction at step 401: [1 2]\n",
      "Ground truth at step 401: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 402: [[-2.53456712 -3.27184248  6.46408272 -1.46330976]\n",
      " [ 6.70909786 -2.56645107 -1.97465467 -1.49550331]]\n",
      "Minibatch train loss at step 402: [ 0.00054333  0.00053618]\n",
      "Minibatch train loss mean at step 402: 0.0005397531786002219\n",
      "Minibatch train prediction at step 402: [2 0]\n",
      "Ground truth at step 402: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 403: [[-2.53592515 -3.2727387   6.46812439 -1.46551251]\n",
      " [ 6.71316099 -2.5676837  -1.97512281 -1.49729025]]\n",
      "Minibatch train loss at step 403: [ 0.00054011  0.00053344]\n",
      "Minibatch train loss mean at step 403: 0.000536774517968297\n",
      "Minibatch train prediction at step 403: [2 0]\n",
      "Ground truth at step 403: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 404: [[ 0.23660393  7.23862982 -1.36950278 -1.60959959]\n",
      " [-2.68285823 -3.20890403  6.22351122 -0.9574784 ]]\n",
      "Minibatch train loss at step 404: [ 0.00123556  0.00097609]\n",
      "Minibatch train loss mean at step 404: 0.001105820992961526\n",
      "Minibatch train prediction at step 404: [1 2]\n",
      "Ground truth at step 404: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 405: [[-0.33060959 -1.20591831 -0.5721935   6.99343824]\n",
      " [ 7.17775249  0.23620354 -2.26223898 -1.36359859]]\n",
      "Minibatch train loss at step 405: [ 0.00145127  0.00124079]\n",
      "Minibatch train loss mean at step 405: 0.0013460339978337288\n",
      "Minibatch train prediction at step 405: [3 0]\n",
      "Ground truth at step 405: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 406: [[ 7.40413618 -1.79913664 -2.41816187 -1.68027985]\n",
      " [-2.77110052 -2.86730433  6.86623192 -2.13273239]]\n",
      "Minibatch train loss at step 406: [ 0.0002683   0.00024792]\n",
      "Minibatch train loss mean at step 406: 0.0002581143635325134\n",
      "Minibatch train prediction at step 406: [0 2]\n",
      "Ground truth at step 406: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 407: [[-2.68739605 -3.21207094  6.23917866 -0.96771252]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ 7.47991467 -1.87358463 -2.02109671 -1.10541189]]\n",
      "Minibatch train loss at step 407: [ 0.00095239  0.00034815]\n",
      "Minibatch train loss mean at step 407: 0.0006502679316326976\n",
      "Minibatch train prediction at step 407: [2 0]\n",
      "Ground truth at step 407: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 408: [[-0.01212147  7.20915031 -1.22044468 -1.3840071 ]\n",
      " [-2.54341006 -3.27718163  6.49126053 -1.47963095]]\n",
      "Minibatch train loss at step 408: [ 0.00113387  0.00052164]\n",
      "Minibatch train loss mean at step 408: 0.0008277573506347835\n",
      "Minibatch train prediction at step 408: [1 2]\n",
      "Ground truth at step 408: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 409: [[-2.77422619 -2.86837029  6.87733889 -2.1401577 ]\n",
      " [-2.6905551  -3.21423221  6.25076056 -0.97581953]]\n",
      "Minibatch train loss at step 409: [ 0.00024411  0.000935  ]\n",
      "Minibatch train loss mean at step 409: 0.0005895544309169054\n",
      "Minibatch train prediction at step 409: [2 2]\n",
      "Ground truth at step 409: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 410: [[ 7.48759413 -1.87428892 -2.01983809 -1.11165512]\n",
      " [-0.01489273  7.21473598 -1.22122252 -1.38444221]]\n",
      "Minibatch train loss at step 410: [ 0.00034446  0.00112542]\n",
      "Minibatch train loss mean at step 410: 0.0007349364459514618\n",
      "Minibatch train prediction at step 410: [0 1]\n",
      "Ground truth at step 410: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 411: [[ 7.3496232  -0.18107469 -2.17639613 -1.33388042]\n",
      " [ 7.18753242  0.23263751 -2.26750326 -1.36441994]]\n",
      "Minibatch train loss at step 411: [ 0.00077837  0.0012246 ]\n",
      "Minibatch train loss mean at step 411: 0.0010014870204031467\n",
      "Minibatch train prediction at step 411: [0 0]\n",
      "Ground truth at step 411: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 412: [[-0.33750749 -1.20920515 -0.58131009  7.01798725]\n",
      " [ 6.74586391 -2.57804942 -1.97718906 -1.51422393]]\n",
      "Minibatch train loss at step 412: [ 0.00140616  0.00051068]\n",
      "Minibatch train loss mean at step 412: 0.0009584194049239159\n",
      "Minibatch train prediction at step 412: [3 0]\n",
      "Ground truth at step 412: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 413: [[ 6.74952126 -2.57926154 -1.97745121 -1.51609302]\n",
      " [ 7.49614143 -1.87498295 -2.01858115 -1.11861539]]\n",
      "Minibatch train loss at step 413: [ 0.00050806  0.00034028]\n",
      "Minibatch train loss mean at step 413: 0.000424172350903973\n",
      "Minibatch train prediction at step 413: [0 0]\n",
      "Ground truth at step 413: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 414: [[-2.81219673 -2.02787018  6.86483526 -2.11555433]\n",
      " [ 7.47642565 -0.11698102 -2.10899734 -1.39470434]]\n",
      "Minibatch train loss at step 414: [ 0.00032598  0.00071262]\n",
      "Minibatch train loss mean at step 414: 0.0005193009274080396\n",
      "Minibatch train prediction at step 414: [2 0]\n",
      "Ground truth at step 414: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 415: [[-0.94939137 -3.58885098 -0.43735394  6.66719007]\n",
      " [ 6.75825405 -2.58215714 -1.97820938 -1.52027166]]\n",
      "Minibatch train loss at step 415: [ 0.00134783  0.00050222]\n",
      "Minibatch train loss mean at step 415: 0.0009250234579667449\n",
      "Minibatch train prediction at step 415: [3 0]\n",
      "Ground truth at step 415: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 416: [[ 7.50602102 -1.87531078 -2.01790023 -1.12615383]\n",
      " [-2.70030737 -3.22149253  6.28725672 -1.00086558]]\n",
      "Minibatch train loss at step 416: [ 0.00033552  0.00088247]\n",
      "Minibatch train loss mean at step 416: 0.0006089961971156299\n",
      "Minibatch train prediction at step 416: [0 2]\n",
      "Ground truth at step 416: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 417: [[ 7.50972557 -1.87535048 -2.01768374 -1.12898493]\n",
      " [-0.34160024 -1.21202695 -0.58814669  7.03625679]]\n",
      "Minibatch train loss at step 417: [ 0.00033385  0.00137413]\n",
      "Minibatch train loss mean at step 417: 0.0008539920090697706\n",
      "Minibatch train prediction at step 417: [0 3]\n",
      "Ground truth at step 417: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 418: [[ 7.20496178  0.22135872 -2.27165127 -1.36681962]\n",
      " [-2.78327346 -2.87143826  6.90872526 -2.16085458]]\n",
      "Minibatch train loss at step 418: [ 0.00119234  0.0002335 ]\n",
      "Minibatch train loss mean at step 418: 0.0007129196310415864\n",
      "Minibatch train prediction at step 418: [0 2]\n",
      "Ground truth at step 418: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 419: [[-2.70422482 -3.22480631  6.30058193 -1.00865126]\n",
      " [-0.34340093 -1.21343791 -0.59160858  7.04510736]]\n",
      "Minibatch train loss at step 419: [ 0.00086473  0.0013589 ]\n",
      "Minibatch train loss mean at step 419: 0.0011118121910840273\n",
      "Minibatch train prediction at step 419: [2 3]\n",
      "Ground truth at step 419: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 420: [[ 7.42500734 -1.80616641 -2.42265296 -1.69044614]\n",
      " [-2.70567322 -3.22597337  6.30562687 -1.01180518]]\n",
      "Minibatch train loss at step 420: [ 0.0002608   0.00085806]\n",
      "Minibatch train loss mean at step 420: 0.0005594269023276865\n",
      "Minibatch train prediction at step 420: [0 2]\n",
      "Ground truth at step 420: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 421: [[-2.78643513 -2.87247896  6.91931772 -2.16794443]\n",
      " [-2.56143737 -3.28818893  6.54825449 -1.51446557]]\n",
      "Minibatch train loss at step 421: [ 0.00022993  0.00047899]\n",
      "Minibatch train loss mean at step 421: 0.00035445785033516586\n",
      "Minibatch train prediction at step 421: [2 2]\n",
      "Ground truth at step 421: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 422: [[-2.78771138 -2.87290263  6.92369986 -2.17089915]\n",
      " [-0.34647825 -1.21595764 -0.59769088  7.06019258]]\n",
      "Minibatch train loss at step 422: [ 0.0002285   0.00133306]\n",
      "Minibatch train loss mean at step 422: 0.0007807805668562651\n",
      "Minibatch train prediction at step 422: [2 3]\n",
      "Ground truth at step 422: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 423: [[ 6.79042244 -2.59290767 -1.98150814 -1.53418374]\n",
      " [-2.56457281 -3.29003072  6.55798006 -1.52039909]]\n",
      "Minibatch train loss at step 423: [ 0.00048149  0.00047208]\n",
      "Minibatch train loss mean at step 423: 0.0004767830832861364\n",
      "Minibatch train prediction at step 423: [0 2]\n",
      "Ground truth at step 423: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 424: [[ 7.42999125 -1.80785322 -2.42371202 -1.6929152 ]\n",
      " [-2.79057813 -2.87394333  6.93327665 -2.17729092]]\n",
      "Minibatch train loss at step 424: [ 0.00025901  0.0002254 ]\n",
      "Minibatch train loss mean at step 424: 0.00024220379418693483\n",
      "Minibatch train prediction at step 424: [0 2]\n",
      "Ground truth at step 424: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 425: [[-0.95682275 -3.58937621 -0.43441465  6.68501139]\n",
      " [ 7.22231579  0.20761763 -2.27536321 -1.36771774]]\n",
      "Minibatch train loss at step 425: [ 0.00132282  0.00115888]\n",
      "Minibatch train loss mean at step 425: 0.001240850891917944\n",
      "Minibatch train prediction at step 425: [3 0]\n",
      "Ground truth at step 425: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 426: [[ 7.22523355  0.20493585 -2.27562094 -1.36789382]\n",
      " [-0.95748496 -3.58907032 -0.43493807  6.68838167]]\n",
      "Minibatch train loss at step 426: [ 0.00115304  0.00131759]\n",
      "Minibatch train loss mean at step 426: 0.0012353144120424986\n",
      "Minibatch train prediction at step 426: [0 3]\n",
      "Ground truth at step 426: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 427: [[ 6.80454969 -2.59776568 -1.98326111 -1.53962433]\n",
      " [-2.57068658 -3.29383254  6.57495737 -1.52906668]]\n",
      "Minibatch train loss at step 427: [ 0.00047279  0.00046064]\n",
      "Minibatch train loss mean at step 427: 0.00046671461313962936\n",
      "Minibatch train prediction at step 427: [0 2]\n",
      "Ground truth at step 427: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 428: [[-0.24545814  7.33786249 -1.10186052 -1.49409449]\n",
      " [-2.81971312 -2.03472805  6.8972888  -2.12756896]]\n",
      "Minibatch train loss at step 428: [ 0.00087068  0.00031264]\n",
      "Minibatch train loss mean at step 428: 0.0005916600930504501\n",
      "Minibatch train prediction at step 428: [1 2]\n",
      "Ground truth at step 428: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 429: [[ 7.38527155 -0.21040978 -2.18606687 -1.33620334]\n",
      " [-0.95900035 -3.58729982 -0.43897262  6.70314741]]\n",
      "Minibatch train loss at step 429: [ 0.00073513  0.00129449]\n",
      "Minibatch train loss mean at step 429: 0.0010148108704015613\n",
      "Minibatch train prediction at step 429: [0 3]\n",
      "Ground truth at step 429: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 430: [[-2.79914093 -2.87681317  6.9576211  -2.19259691]\n",
      " [ 7.51275635 -0.14632718 -2.114079   -1.39517641]]\n",
      "Minibatch train loss at step 430: [ 0.00021765  0.00067271]\n",
      "Minibatch train loss mean at step 430: 0.0004451813001651317\n",
      "Minibatch train prediction at step 430: [2 0]\n",
      "Ground truth at step 430: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 431: [[ 7.44095469 -1.81226337 -2.42543292 -1.69843113]\n",
      " [-0.24530512  7.34079742 -1.10215318 -1.4956702 ]]\n",
      "Minibatch train loss at step 431: [ 0.00025508  0.00086782]\n",
      "Minibatch train loss mean at step 431: 0.000561449967790395\n",
      "Minibatch train prediction at step 431: [0 1]\n",
      "Ground truth at step 431: [0 1]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 432: [[ 7.51811504 -0.15110978 -2.11417723 -1.39587867]\n",
      " [ 6.82265091 -2.60421348 -1.98681235 -1.54403555]]\n",
      "Minibatch train loss at step 432: [ 0.00066675  0.00046231]\n",
      "Minibatch train loss mean at step 432: 0.0005645297933369875\n",
      "Minibatch train prediction at step 432: [0 0]\n",
      "Ground truth at step 432: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 433: [[ 7.52097607 -0.15353118 -2.11421323 -1.396294  ]\n",
      " [-0.24549915  7.34366608 -1.10248017 -1.49687862]]\n",
      "Minibatch train loss at step 433: [ 0.00066389  0.00086497]\n",
      "Minibatch train loss mean at step 433: 0.0007644302677363157\n",
      "Minibatch train prediction at step 433: [0 1]\n",
      "Ground truth at step 433: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 434: [[ 7.25219822  0.18025118 -2.27623272 -1.37151921]\n",
      " [ 6.83014965 -2.60706735 -1.98833323 -1.5459758 ]]\n",
      "Minibatch train loss at step 434: [ 0.00110041  0.0004579 ]\n",
      "Minibatch train loss mean at step 434: 0.0007791542448103428\n",
      "Minibatch train prediction at step 434: [0 0]\n",
      "Ground truth at step 434: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 435: [[ 0.23479342  7.27440977 -1.38171268 -1.62863863]\n",
      " [-0.01880032  7.24407768 -1.22650588 -1.39223135]]\n",
      "Minibatch train loss at step 435: [ 0.00118579  0.00108755]\n",
      "Minibatch train loss mean at step 435: 0.001136668724939227\n",
      "Minibatch train prediction at step 435: [1 1]\n",
      "Ground truth at step 435: [1 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 436: [[ 7.25977564  0.1740952  -2.27647805 -1.37304151]\n",
      " [-0.24670148  7.35003519 -1.10307598 -1.49903512]]\n",
      "Minibatch train loss at step 436: [ 0.00108684  0.00085853]\n",
      "Minibatch train loss mean at step 436: 0.0009726852877065539\n",
      "Minibatch train prediction at step 436: [0 1]\n",
      "Ground truth at step 436: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 437: [[ 7.26384258  0.17099619 -2.27668643 -1.3738699 ]\n",
      " [-0.24771444  7.35319424 -1.10332811 -1.49984705]]\n",
      "Minibatch train loss at step 437: [ 0.00107957  0.00085508]\n",
      "Minibatch train loss mean at step 437: 0.0009673263411968946\n",
      "Minibatch train prediction at step 437: [0 1]\n",
      "Ground truth at step 437: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 438: [[ 7.45454502 -1.81789351 -2.42751813 -1.70474362]\n",
      " [ 7.26858425  0.16724287 -2.27678967 -1.3747952 ]]\n",
      "Minibatch train loss at step 438: [ 0.00025031  0.00107124]\n",
      "Minibatch train loss mean at step 438: 0.0006607724935747683\n",
      "Minibatch train prediction at step 438: [0 0]\n",
      "Ground truth at step 438: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 439: [[ 7.41402006 -0.23524435 -2.18848705 -1.34221137]\n",
      " [ 6.84888697 -2.6147728  -1.99192822 -1.55150735]]\n",
      "Minibatch train loss at step 439: [ 0.00070118  0.00044705]\n",
      "Minibatch train loss mean at step 439: 0.0005741178756579757\n",
      "Minibatch train prediction at step 439: [0 0]\n",
      "Ground truth at step 439: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 440: [[-0.0210885   7.2559433  -1.22699749 -1.39601362]\n",
      " [-2.82638097 -2.03822398  6.9193058  -2.13270426]]\n",
      "Minibatch train loss at step 440: [ 0.00107231  0.00030441]\n",
      "Minibatch train loss mean at step 440: 0.0006883613532409072\n",
      "Minibatch train prediction at step 440: [1 2]\n",
      "Ground truth at step 440: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 441: [[ 7.54988909 -0.1761319  -2.11492157 -1.40238571]\n",
      " [-2.80959797 -2.87988734  6.98450327 -2.20772219]]\n",
      "Minibatch train loss at step 441: [ 0.00063387  0.00020955]\n",
      "Minibatch train loss mean at step 441: 0.0004217106325086206\n",
      "Minibatch train prediction at step 441: [0 2]\n",
      "Ground truth at step 441: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 442: [[-0.96047258 -3.58218169 -0.45489734  6.75116396]\n",
      " [ 6.85990191 -2.61961889 -1.99388039 -1.55508232]]\n",
      "Minibatch train loss at step 442: [ 0.00122139  0.00044062]\n",
      "Minibatch train loss mean at step 442: 0.0008310035336762667\n",
      "Minibatch train prediction at step 442: [3 0]\n",
      "Ground truth at step 442: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 443: [[ 7.55712843 -0.18177614 -2.1150322  -1.40416741]\n",
      " [-2.82808971 -2.03924346  6.92499971 -2.13387561]]\n",
      "Minibatch train loss at step 443: [ 0.00062673  0.00030227]\n",
      "Minibatch train loss mean at step 443: 0.0004644971340894699\n",
      "Minibatch train prediction at step 443: [0 2]\n",
      "Ground truth at step 443: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 444: [[ 7.46888018 -1.82373643 -2.42964745 -1.71080136]\n",
      " [ 7.56067848 -0.18464586 -2.11501408 -1.40501642]]\n",
      "Minibatch train loss at step 444: [ 0.00024554  0.00062303]\n",
      "Minibatch train loss mean at step 444: 0.0004342865140642971\n",
      "Minibatch train prediction at step 444: [0 0]\n",
      "Ground truth at step 444: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 445: [[-2.5877254  -3.30662704  6.61373568 -1.53741789]\n",
      " [-0.96037781 -3.58119059 -0.45892674  6.76179886]]\n",
      "Minibatch train loss at step 445: [ 0.00043836  0.00120567]\n",
      "Minibatch train loss mean at step 445: 0.0008220132440328598\n",
      "Minibatch train prediction at step 445: [2 3]\n",
      "Ground truth at step 445: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 446: [[ 0.23769927  7.29441214 -1.38451469 -1.64605427]\n",
      " [ 6.87510729 -2.62616324 -1.99705648 -1.55886972]]\n",
      "Minibatch train loss at step 446: [ 0.00116209  0.00043216]\n",
      "Minibatch train loss mean at step 446: 0.000797125743702054\n",
      "Minibatch train prediction at step 446: [1 0]\n",
      "Ground truth at step 446: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 447: [[ 6.87923288 -2.6279273  -1.99792576 -1.55988157]\n",
      " [-2.83065939 -2.04161048  6.93310356 -2.13552523]]\n",
      "Minibatch train loss at step 447: [ 0.00043001  0.00029929]\n",
      "Minibatch train loss mean at step 447: 0.00036465219454839826\n",
      "Minibatch train prediction at step 447: [0 2]\n",
      "Ground truth at step 447: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 448: [[ 7.44340563 -0.25907674 -2.18987393 -1.3503443 ]\n",
      " [-0.96073627 -3.5796721  -0.46407667  6.77524948]]\n",
      "Minibatch train loss at step 448: [ 0.00066866  0.00118579]\n",
      "Minibatch train loss mean at step 448: 0.0009272232418879867\n",
      "Minibatch train prediction at step 448: [0 3]\n",
      "Ground truth at step 448: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 449: [[-0.96090829 -3.5789938  -0.46636781  6.78098488]\n",
      " [ 7.44601965 -0.26117855 -2.1899972  -1.35116041]]\n",
      "Minibatch train loss at step 449: [ 0.00117733  0.0006658 ]\n",
      "Minibatch train loss mean at step 449: 0.0009215667378157377\n",
      "Minibatch train prediction at step 449: [3 0]\n",
      "Ground truth at step 449: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 450: [[-0.35656273 -1.22911561 -0.62222672  7.13721895]\n",
      " [ 7.31790876  0.12662522 -2.27661848 -1.38621771]]\n",
      "Minibatch train loss at step 450: [ 0.00121508  0.00098669]\n",
      "Minibatch train loss mean at step 450: 0.0011008810251951218\n",
      "Minibatch train prediction at step 450: [3 0]\n",
      "Ground truth at step 450: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 451: [[ 6.89684534 -2.63538241 -2.0019989  -1.56334615]\n",
      " [-0.25609693  7.38546324 -1.10434902 -1.51110506]]\n",
      "Minibatch train loss at step 451: [ 0.0004206   0.00082209]\n",
      "Minibatch train loss mean at step 451: 0.0006213439628481865\n",
      "Minibatch train prediction at step 451: [0 1]\n",
      "Ground truth at step 451: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 452: [[-2.83429217 -2.04515433  6.94331121 -2.13714504]\n",
      " [-0.35743907 -1.23001754 -0.62404025  7.14330482]]\n",
      "Minibatch train loss at step 452: [ 0.00029536  0.00120627]\n",
      "Minibatch train loss mean at step 452: 0.0007508115959353745\n",
      "Minibatch train prediction at step 452: [2 3]\n",
      "Ground truth at step 452: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 453: [[ 6.90578604 -2.63920116 -2.00413084 -1.56510234]\n",
      " [-0.02556565  7.27833462 -1.22664702 -1.40528429]]\n",
      "Minibatch train loss at step 453: [ 0.00041595  0.0010442 ]\n",
      "Minibatch train loss mean at step 453: 0.0007300793658941984\n",
      "Minibatch train prediction at step 453: [0 1]\n",
      "Ground truth at step 453: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 454: [[-2.83588576 -2.0469892   6.94807863 -2.13782144]\n",
      " [-0.35917151 -1.23108578 -0.62687576  7.15151978]]\n",
      "Minibatch train loss at step 454: [ 0.00029357  0.001194  ]\n",
      "Minibatch train loss mean at step 454: 0.0007437858730554581\n",
      "Minibatch train prediction at step 454: [2 3]\n",
      "Ground truth at step 454: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 455: [[ 7.59196377 -0.20936593 -2.11599874 -1.41313183]\n",
      " [ 7.58755398 -1.88155746 -2.02543569 -1.17555356]]\n",
      "Minibatch train loss at step 455: [ 0.00059265  0.00030048]\n",
      "Minibatch train loss mean at step 455: 0.0004465667880140245\n",
      "Minibatch train prediction at step 455: [0 0]\n",
      "Ground truth at step 455: [0 0]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 456: [[-0.36147261 -1.23236048 -0.63051826  7.16140127]\n",
      " [-2.82050347 -2.88342643  7.00609446 -2.21725321]]\n",
      "Minibatch train loss at step 456: [ 0.00117924  0.00020335]\n",
      "Minibatch train loss mean at step 456: 0.0006912941462360322\n",
      "Minibatch train prediction at step 456: [3 2]\n",
      "Ground truth at step 456: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 457: [[ 6.92324257 -2.64676976 -2.00808072 -1.56921518]\n",
      " [-2.83842754 -2.05045724  6.9561286  -2.13905406]]\n",
      "Minibatch train loss at step 457: [ 0.0004069   0.00029047]\n",
      "Minibatch train loss mean at step 457: 0.0003486842615529895\n",
      "Minibatch train prediction at step 457: [0 2]\n",
      "Ground truth at step 457: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 458: [[ 7.59858799 -0.21392117 -2.1171298  -1.41452873]\n",
      " [-2.82204604 -2.8841486   7.00962782 -2.21905732]]\n",
      "Minibatch train loss at step 458: [ 0.00058658  0.0002024 ]\n",
      "Minibatch train loss mean at step 458: 0.00039448647294193506\n",
      "Minibatch train prediction at step 458: [0 2]\n",
      "Ground truth at step 458: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 459: [[ 7.47066593 -0.27995604 -2.19339776 -1.35754895]\n",
      " [-0.26238355  7.40185642 -1.1054697  -1.51561046]]\n",
      "Minibatch train loss at step 459: [ 0.00064031  0.00080493]\n",
      "Minibatch train loss mean at step 459: 0.0007226205780170858\n",
      "Minibatch train prediction at step 459: [0 1]\n",
      "Ground truth at step 459: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 460: [[-2.84091735 -2.05437922  6.96450138 -2.14060712]\n",
      " [ 7.3464098   0.10516059 -2.28023624 -1.39260972]]\n",
      "Minibatch train loss at step 460: [ 0.00028737  0.00094214]\n",
      "Minibatch train loss mean at step 460: 0.0006147581152617931\n",
      "Minibatch train prediction at step 460: [2 0]\n",
      "Ground truth at step 460: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 461: [[-0.96471041 -3.5748682  -0.48707354  6.83907938]\n",
      " [ 7.34975815  0.10247977 -2.28045988 -1.3933419 ]]\n",
      "Minibatch train loss at step 461: [ 0.00109565  0.0009369 ]\n",
      "Minibatch train loss mean at step 461: 0.0010162758408114314\n",
      "Minibatch train prediction at step 461: [3 0]\n",
      "Ground truth at step 461: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 462: [[-0.03061679  7.2939086  -1.22852635 -1.40999794]\n",
      " [ 7.47889042 -0.28642991 -2.19427848 -1.35966933]]\n",
      "Minibatch train loss at step 462: [ 0.0010236   0.00063197]\n",
      "Minibatch train loss mean at step 462: 0.0008277849992737174\n",
      "Minibatch train prediction at step 462: [1 0]\n",
      "Ground truth at step 462: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 463: [[ 7.48205757 -0.28894222 -2.19446754 -1.3605907 ]\n",
      " [-0.36806715 -1.23716903 -0.64248425  7.19370365]]\n",
      "Minibatch train loss at step 463: [ 0.00062875  0.00113256]\n",
      "Minibatch train loss mean at step 463: 0.0008806561818346381\n",
      "Minibatch train prediction at step 463: [0 3]\n",
      "Ground truth at step 463: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 464: [[ 7.49377155 -1.83639991 -2.43343568 -1.72578073]\n",
      " [ 7.36142588  0.09272128 -2.28073883 -1.39598668]]\n",
      "Minibatch train loss at step 464: [ 0.0002366  0.0009188]\n",
      "Minibatch train loss mean at step 464: 0.0005777014885097742\n",
      "Minibatch train prediction at step 464: [0 0]\n",
      "Ground truth at step 464: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 465: [[ 7.48908854 -0.29468155 -2.19480944 -1.36254764]\n",
      " [-0.3696878  -1.23867571 -0.64589673  7.20300484]]\n",
      "Minibatch train loss at step 465: [ 0.0006216   0.00111958]\n",
      "Minibatch train loss mean at step 465: 0.0008705925429239869\n",
      "Minibatch train prediction at step 465: [0 3]\n",
      "Ground truth at step 465: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 466: [[-2.75144339 -3.26474404  6.4025898  -1.01969516]\n",
      " [-0.37068996 -1.23957217 -0.64802748  7.20854902]]\n",
      "Minibatch train loss at step 466: [ 0.0007667   0.00111184]\n",
      "Minibatch train loss mean at step 466: 0.0009392707142978907\n",
      "Minibatch train prediction at step 466: [2 3]\n",
      "Ground truth at step 466: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 467: [[ 7.49634647 -0.30066508 -2.19529676 -1.36438966]\n",
      " [-0.03080893  7.30061722 -1.22905207 -1.41348135]]\n",
      "Minibatch train loss at step 467: [ 0.00061434  0.00101598]\n",
      "Minibatch train loss mean at step 467: 0.0008151582442224026\n",
      "Minibatch train prediction at step 467: [0 1]\n",
      "Ground truth at step 467: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 468: [[ 7.49998617 -1.83937097 -2.43414831 -1.72892308]\n",
      " [ 7.62940645 -0.23865621 -2.11901522 -1.4217906 ]]\n",
      "Minibatch train loss at step 468: [ 0.00023446  0.00055834]\n",
      "Minibatch train loss mean at step 468: 0.0003963984199799597\n",
      "Minibatch train prediction at step 468: [0 0]\n",
      "Ground truth at step 468: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 469: [[-2.84743261 -2.06315994  6.98699284 -2.14333344]\n",
      " [-2.82992673 -2.88679743  7.02834034 -2.22863412]]\n",
      "Minibatch train loss at step 469: [ 0.00027927  0.00019715]\n",
      "Minibatch train loss mean at step 469: 0.00023821054492145777\n",
      "Minibatch train prediction at step 469: [2 2]\n",
      "Ground truth at step 469: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 470: [[ 7.50645638 -0.3083255  -2.19626808 -1.36718309]\n",
      " [ 0.23520675  7.3239255  -1.39066076 -1.66949797]]\n",
      "Minibatch train loss at step 470: [ 0.0006048  0.0011222]\n",
      "Minibatch train loss mean at step 470: 0.0008635033154860139\n",
      "Minibatch train prediction at step 470: [0 1]\n",
      "Ground truth at step 470: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 471: [[-0.96896964 -3.57384014 -0.49579591  6.87047911]\n",
      " [ 7.63877869 -0.24570295 -2.11985064 -1.42414129]]\n",
      "Minibatch train loss at step 471: [ 0.00105468  0.00055012]\n",
      "Minibatch train loss mean at step 471: 0.000802401511464268\n",
      "Minibatch train prediction at step 471: [3 0]\n",
      "Ground truth at step 471: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 472: [[ 7.39284134  0.06737422 -2.28255033 -1.40306294]\n",
      " [ 7.64178467 -0.24798565 -2.12007332 -1.42492688]]\n",
      "Minibatch train loss at step 472: [ 0.00087235  0.00054738]\n",
      "Minibatch train loss mean at step 472: 0.0007098645437508821\n",
      "Minibatch train prediction at step 472: [0 0]\n",
      "Ground truth at step 472: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 473: [[-0.37828457 -1.24502504 -0.66129047  7.24368477]\n",
      " [-2.85022378 -2.06641507  6.99622822 -2.14454508]]\n",
      "Minibatch train loss at step 473: [ 0.00106373  0.00027593]\n",
      "Minibatch train loss mean at step 473: 0.0006698329816572368\n",
      "Minibatch train prediction at step 473: [3 2]\n",
      "Ground truth at step 473: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 474: [[-0.03345106  7.31202316 -1.22995317 -1.41803801]\n",
      " [-2.85098934 -2.06753039  6.9988265  -2.14483285]]\n",
      "Minibatch train loss at step 474: [ 0.00100181  0.0002751 ]\n",
      "Minibatch train loss mean at step 474: 0.0006384534644894302\n",
      "Minibatch train prediction at step 474: [1 2]\n",
      "Ground truth at step 474: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 475: [[ 7.6517849  -0.25603217 -2.1205349  -1.42732239]\n",
      " [-2.60861158 -3.32999015  6.65305758 -1.52646554]]\n",
      "Minibatch train loss at step 475: [ 0.00053868  0.00042144]\n",
      "Minibatch train loss mean at step 475: 0.00048005805001594126\n",
      "Minibatch train prediction at step 475: [0 2]\n",
      "Ground truth at step 475: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 476: [[-2.60945201 -3.330724    6.6548562  -1.52671623]\n",
      " [ 0.23068531  7.33315372 -1.39168501 -1.67419195]]\n",
      "Minibatch train loss at step 476: [ 0.00042048  0.00110744]\n",
      "Minibatch train loss mean at step 476: 0.0007639594841748476\n",
      "Minibatch train prediction at step 476: [2 1]\n",
      "Ground truth at step 476: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 477: [[ 0.22823347  7.33590221 -1.39188826 -1.6744132 ]\n",
      " [ 7.51507902 -1.84606171 -2.43628597 -1.73603356]]\n",
      "Minibatch train loss at step 477: [ 0.00110244  0.00022957]\n",
      "Minibatch train loss mean at step 477: 0.0006660032086074352\n",
      "Minibatch train prediction at step 477: [1 0]\n",
      "Ground truth at step 477: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 478: [[ 7.4128828   0.05225997 -2.28429317 -1.40767276]\n",
      " [-0.03822039  7.32161331 -1.23044503 -1.42036033]]\n",
      "Minibatch train loss at step 478: [ 0.0008446   0.00098871]\n",
      "Minibatch train loss mean at step 478: 0.0009166542440652847\n",
      "Minibatch train prediction at step 478: [0 1]\n",
      "Ground truth at step 478: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 479: [[ 7.51904869 -1.84758365 -2.43698049 -1.73773086]\n",
      " [ 6.97829723 -2.67146063 -2.0202117  -1.58202589]]\n",
      "Minibatch train loss at step 479: [ 0.00022838  0.00037961]\n",
      "Minibatch train loss mean at step 479: 0.0003039942239411175\n",
      "Minibatch train prediction at step 479: [0 0]\n",
      "Ground truth at step 479: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 480: [[-0.97346795 -3.57267308 -0.50110865  6.89329052]\n",
      " [ 7.62165976 -1.88689375 -2.03221107 -1.19297147]]\n",
      "Minibatch train loss at step 480: [ 0.00102587  0.0002869 ]\n",
      "Minibatch train loss mean at step 480: 0.0006563805509358644\n",
      "Minibatch train prediction at step 480: [3 0]\n",
      "Ground truth at step 480: [3 0]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 481: [[-0.3863607  -1.24916995 -0.67117     7.27354431]\n",
      " [-0.27828634  7.4442277  -1.10704231 -1.52856803]]\n",
      "Minibatch train loss at step 481: [ 0.0010242   0.00076265]\n",
      "Minibatch train loss mean at step 481: 0.0008934234501793981\n",
      "Minibatch train prediction at step 481: [3 1]\n",
      "Ground truth at step 481: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 482: [[-0.38767588 -1.2495656  -0.67253053  7.27754927]\n",
      " [-2.83747125 -2.88927722  7.0480876  -2.2385385 ]]\n",
      "Minibatch train loss at step 482: [ 0.00101896  0.00019179]\n",
      "Minibatch train loss mean at step 482: 0.0006053739343769848\n",
      "Minibatch train prediction at step 482: [3 2]\n",
      "Ground truth at step 482: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 483: [[-2.83816695 -2.88960266  7.04992676 -2.23951769]\n",
      " [ 7.52877712 -1.85101795 -2.43859172 -1.74168205]]\n",
      "Minibatch train loss at step 483: [ 0.00019143  0.0002254 ]\n",
      "Minibatch train loss mean at step 483: 0.000208415585802868\n",
      "Minibatch train prediction at step 483: [2 0]\n",
      "Ground truth at step 483: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 484: [[ 6.98858786 -2.67583489 -2.02254248 -1.58448899]\n",
      " [ 7.5315752  -1.85192847 -2.43902087 -1.74273491]]\n",
      "Minibatch train loss at step 484: [ 0.00037472  0.00022457]\n",
      "Minibatch train loss mean at step 484: 0.000299644423648715\n",
      "Minibatch train prediction at step 484: [0 0]\n",
      "Ground truth at step 484: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 485: [[-2.83980846 -2.89036536  7.05446291 -2.24209118]\n",
      " [-0.97637022 -3.57183194 -0.50504762  6.90802002]]\n",
      "Minibatch train loss at step 485: [ 0.00019024  0.00100753]\n",
      "Minibatch train loss mean at step 485: 0.0005988830234855413\n",
      "Minibatch train prediction at step 485: [2 3]\n",
      "Ground truth at step 485: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 486: [[ 7.54585409 -0.33342376 -2.20362735 -1.37773323]\n",
      " [ 7.67439938 -0.26824346 -2.1258781  -1.43331969]]\n",
      "Minibatch train loss at step 486: [ 0.00056978  0.0005214 ]\n",
      "Minibatch train loss mean at step 486: 0.0005455909995362163\n",
      "Minibatch train prediction at step 486: [0 0]\n",
      "Ground truth at step 486: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 487: [[ 7.54779911 -0.334654   -2.20403528 -1.37828672]\n",
      " [-0.05282987  7.34684753 -1.23236918 -1.42398489]]\n",
      "Minibatch train loss at step 487: [ 0.00056823  0.00095417]\n",
      "Minibatch train loss mean at step 487: 0.0007612005574628711\n",
      "Minibatch train prediction at step 487: [0 1]\n",
      "Ground truth at step 487: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 488: [[ 7.54988289 -0.33580795 -2.2044723  -1.37897563]\n",
      " [ 7.67828989 -0.27050465 -2.12665606 -1.43451762]]\n",
      "Minibatch train loss at step 488: [ 0.00056656  0.00051843]\n",
      "Minibatch train loss mean at step 488: 0.0005424931878224015\n",
      "Minibatch train prediction at step 488: [0 0]\n",
      "Ground truth at step 488: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 489: [[ 7.63495636 -1.88822532 -2.03453565 -1.20038915]\n",
      " [-2.61956978 -3.33942103  6.67467642 -1.52736926]]\n",
      "Minibatch train loss at step 489: [ 0.00028189  0.00041071]\n",
      "Minibatch train loss mean at step 489: 0.00034630054142326117\n",
      "Minibatch train prediction at step 489: [0 2]\n",
      "Ground truth at step 489: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 490: [[ 7.55487013 -0.3390255  -2.20508218 -1.38067448]\n",
      " [-2.77072001 -3.28093982  6.43746758 -1.01670742]]\n",
      "Minibatch train loss at step 490: [ 0.00056215  0.00073906]\n",
      "Minibatch train loss mean at step 490: 0.0006506075151264668\n",
      "Minibatch train prediction at step 490: [0 2]\n",
      "Ground truth at step 490: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 491: [[-0.39765865 -1.25312781 -0.68407989  7.31019688]\n",
      " [ 7.55741453 -0.34073308 -2.2053597  -1.38154531]]\n",
      "Minibatch train loss at step 491: [ 0.00097716  0.00056001]\n",
      "Minibatch train loss mean at step 491: 0.0007685827440582216\n",
      "Minibatch train prediction at step 491: [3 0]\n",
      "Ground truth at step 491: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 492: [[ 7.55418205 -1.85945404 -2.44265223 -1.75139761]\n",
      " [-2.86317468 -2.07784629  7.03550148 -2.15057397]]\n",
      "Minibatch train loss at step 492: [ 0.00021813  0.00026282]\n",
      "Minibatch train loss mean at step 492: 0.00024047557963058352\n",
      "Minibatch train prediction at step 492: [0 2]\n",
      "Ground truth at step 492: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 493: [[ 7.69041014 -0.27858958 -2.1281364  -1.43822169]\n",
      " [ 0.19382824  7.37615299 -1.39595461 -1.67723691]]\n",
      "Minibatch train loss at step 493: [ 0.00050889  0.00103122]\n",
      "Minibatch train loss mean at step 493: 0.0007700592977926135\n",
      "Minibatch train prediction at step 493: [0 1]\n",
      "Ground truth at step 493: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 494: [[ 7.6436305  -1.88932872 -2.03495049 -1.20609486]\n",
      " [ 0.19178094  7.37851667 -1.39607    -1.6773771 ]]\n",
      "Minibatch train loss at step 494: [ 0.00027843  0.00102729]\n",
      "Minibatch train loss mean at step 494: 0.0006528644007630646\n",
      "Minibatch train prediction at step 494: [0 1]\n",
      "Ground truth at step 494: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 495: [[ 0.1882173   7.38201571 -1.39617133 -1.67701972]\n",
      " [ 7.01106977 -2.68537331 -2.02721834 -1.59050834]]\n",
      "Minibatch train loss at step 495: [ 0.0010211  0.000364 ]\n",
      "Minibatch train loss mean at step 495: 0.0006925505003891885\n",
      "Minibatch train prediction at step 495: [1 0]\n",
      "Ground truth at step 495: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 496: [[-0.29486823  7.4758172  -1.10947216 -1.53404891]\n",
      " [ 7.56904793 -0.34743765 -2.20747614 -1.38523853]]\n",
      "Minibatch train loss at step 496: [ 0.00073061  0.00055048]\n",
      "Minibatch train loss mean at step 496: 0.0006405407330021262\n",
      "Minibatch train prediction at step 496: [1 0]\n",
      "Ground truth at step 496: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 497: [[ 7.57106543 -0.34814054 -2.20808411 -1.38590419]\n",
      " [ 7.01563787 -2.68745351 -2.0279355  -1.59222829]]\n",
      "Minibatch train loss at step 497: [ 0.00054905  0.00036173]\n",
      "Minibatch train loss mean at step 497: 0.00045539060374721885\n",
      "Minibatch train prediction at step 497: [0 0]\n",
      "Ground truth at step 497: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 498: [[ 7.65187883 -1.89022124 -2.03519726 -1.21163774]\n",
      " [-0.29820246  7.48141813 -1.10991657 -1.5347656 ]]\n",
      "Minibatch train loss at step 498: [ 0.00027534  0.00072513]\n",
      "Minibatch train loss mean at step 498: 0.000500230526085943\n",
      "Minibatch train prediction at step 498: [0 1]\n",
      "Ground truth at step 498: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 499: [[-0.06743786  7.37496042 -1.23430479 -1.42989385]\n",
      " [-2.84934115 -2.89414167  7.08094692 -2.25686669]]\n",
      "Minibatch train loss at step 499: [ 0.00091785  0.00018321]\n",
      "Minibatch train loss mean at step 499: 0.0005505278240889311\n",
      "Minibatch train prediction at step 499: [1 2]\n",
      "Ground truth at step 499: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 500: [[-0.98343843 -3.57052851 -0.51224172  6.93964577]\n",
      " [ 7.65678167 -1.89063668 -2.03531194 -1.21500492]]\n",
      "Minibatch train loss at step 500: [ 0.00096942  0.00027343]\n",
      "Minibatch train loss mean at step 500: 0.0006214227178134024\n",
      "Minibatch train prediction at step 500: [3 0]\n",
      "Ground truth at step 500: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 501: [[-2.77822924 -3.28807449  6.4571867  -1.02390897]\n",
      " [-2.86839819 -2.08138514  7.05075932 -2.15368915]]\n",
      "Minibatch train loss at step 501: [ 0.00071941  0.00025794]\n",
      "Minibatch train loss mean at step 501: 0.0004886716487817466\n",
      "Minibatch train prediction at step 501: [2 2]\n",
      "Ground truth at step 501: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 502: [[-0.07371976  7.38553333 -1.23508191 -1.43143559]\n",
      " [-0.30551109  7.49422216 -1.11100006 -1.53635192]]\n",
      "Minibatch train loss at step 502: [ 0.00090427  0.00071238]\n",
      "Minibatch train loss mean at step 502: 0.0008083248394541442\n",
      "Minibatch train prediction at step 502: [1 1]\n",
      "Ground truth at step 502: [1 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 503: [[-0.30785319  7.49830961 -1.1113559  -1.53679919]\n",
      " [-0.07639508  7.38999987 -1.23545384 -1.43203235]]\n",
      "Minibatch train loss at step 503: [ 0.00070857  0.00089855]\n",
      "Minibatch train loss mean at step 503: 0.0008035603677853942\n",
      "Minibatch train prediction at step 503: [1 1]\n",
      "Ground truth at step 503: [1 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 504: [[-0.31078029  7.50341034 -1.11181283 -1.53729856]\n",
      " [ 0.15367341  7.41977262 -1.39886677 -1.67527723]]\n",
      "Minibatch train loss at step 504: [ 0.00070356  0.00095846]\n",
      "Minibatch train loss mean at step 504: 0.00083101203199476\n",
      "Minibatch train prediction at step 504: [1 1]\n",
      "Ground truth at step 504: [1 1]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 505: [[ 7.46117496  0.03463626 -2.30160856 -1.42355549]\n",
      " [-2.85281801 -2.89614463  7.09209394 -2.26292062]]\n",
      "Minibatch train loss at step 505: [ 0.000791    0.00018047]\n",
      "Minibatch train loss mean at step 505: 0.0004857324529439211\n",
      "Minibatch train prediction at step 505: [0 2]\n",
      "Ground truth at step 505: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 506: [[-0.4120442  -1.25583637 -0.69368744  7.34330273]\n",
      " [-0.31716096  7.51432276 -1.11281788 -1.53827691]]\n",
      "Minibatch train loss at step 506: [ 0.00093559  0.00069308]\n",
      "Minibatch train loss mean at step 506: 0.000814337283372879\n",
      "Minibatch train prediction at step 506: [3 1]\n",
      "Ground truth at step 506: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 507: [[-2.87258077 -2.0826354   7.06281662 -2.15759134]\n",
      " [-0.09017477  7.41289377 -1.23753667 -1.4346875 ]]\n",
      "Minibatch train loss at step 507: [ 0.000254    0.00086973]\n",
      "Minibatch train loss mean at step 507: 0.0005618665018118918\n",
      "Minibatch train prediction at step 507: [2 1]\n",
      "Ground truth at step 507: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 508: [[-0.09372372  7.41880751 -1.2381053  -1.43538904]\n",
      " [ 0.13623524  7.44210291 -1.40162027 -1.67583454]]\n",
      "Minibatch train loss at step 508: [ 0.00086258  0.00092511]\n",
      "Minibatch train loss mean at step 508: 0.0008938483661040664\n",
      "Minibatch train prediction at step 508: [1 1]\n",
      "Ground truth at step 508: [1 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 509: [[-2.87427497 -2.08325577  7.06746054 -2.15896177]\n",
      " [-2.6329565  -3.35146046  6.70906687 -1.53893256]]\n",
      "Minibatch train loss at step 509: [ 0.00025257  0.00039212]\n",
      "Minibatch train loss mean at step 509: 0.00032234712853096426\n",
      "Minibatch train prediction at step 509: [2 2]\n",
      "Ground truth at step 509: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 510: [[-2.87525654 -2.08394456  7.07027054 -2.15993524]\n",
      " [ 7.46338367  0.04194565 -2.30807543 -1.42558372]]\n",
      "Minibatch train loss at step 510: [ 0.00025162  0.00079302]\n",
      "Minibatch train loss mean at step 510: 0.0005223212065175176\n",
      "Minibatch train prediction at step 510: [2 0]\n",
      "Ground truth at step 510: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 511: [[-2.87625718 -2.08505535  7.07347822 -2.16096139]\n",
      " [-2.63427854 -3.35310292  6.71326017 -1.5404532 ]]\n",
      "Minibatch train loss at step 511: [ 0.00025067  0.00038986]\n",
      "Minibatch train loss mean at step 511: 0.0003202616353519261\n",
      "Minibatch train prediction at step 511: [2 2]\n",
      "Ground truth at step 511: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 512: [[-0.10837429  7.4422574  -1.23989606 -1.43824685]\n",
      " [ 7.71464014 -0.27527526 -2.14294672 -1.44908488]]\n",
      "Minibatch train loss at step 512: [ 0.00083424  0.00049591]\n",
      "Minibatch train loss mean at step 512: 0.0006650715367868543\n",
      "Minibatch train prediction at step 512: [1 0]\n",
      "Ground truth at step 512: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 513: [[-2.87840915 -2.08776665  7.08053493 -2.16344523]\n",
      " [ 0.11434075  7.47017574 -1.40448368 -1.67675793]]\n",
      "Minibatch train loss at step 513: [ 0.00024828  0.00088486]\n",
      "Minibatch train loss mean at step 513: 0.0005665693315677345\n",
      "Minibatch train prediction at step 513: [2 1]\n",
      "Ground truth at step 513: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 514: [[-0.42128536 -1.25619543 -0.69773924  7.35973501]\n",
      " [-2.87958527 -2.08932924  7.08423233 -2.1647315 ]]\n",
      "Minibatch train loss at step 514: [ 0.00091511  0.00024697]\n",
      "Minibatch train loss mean at step 514: 0.0005810398142784834\n",
      "Minibatch train prediction at step 514: [3 2]\n",
      "Ground truth at step 514: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 515: [[ 0.10568296  7.48105431 -1.40528178 -1.67697835]\n",
      " [-2.78632784 -3.30090785  6.48754025 -1.03666401]]\n",
      "Minibatch train loss at step 515: [ 0.00086973  0.00068963]\n",
      "Minibatch train loss mean at step 515: 0.0007796782883815467\n",
      "Minibatch train prediction at step 515: [1 2]\n",
      "Ground truth at step 515: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 516: [[-2.85881591 -2.90025091  7.11403561 -2.27428317]\n",
      " [ 7.46775341  0.04711052 -2.31433725 -1.42744613]]\n",
      "Minibatch train loss at step 516: [ 0.0001751   0.00079207]\n",
      "Minibatch train loss mean at step 516: 0.00048358674393966794\n",
      "Minibatch train prediction at step 516: [2 0]\n",
      "Ground truth at step 516: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 517: [[-0.12405749  7.46700859 -1.24151373 -1.44174862]\n",
      " [ 7.68693876 -1.89487338 -2.03596044 -1.23524666]]\n",
      "Minibatch train loss at step 517: [ 0.00080505  0.00026223]\n",
      "Minibatch train loss mean at step 517: 0.000533639919012785\n",
      "Minibatch train prediction at step 517: [1 0]\n",
      "Ground truth at step 517: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 518: [[-2.86012554 -2.90107727  7.11905622 -2.27709532]\n",
      " [ 7.46992254  0.04767129 -2.31607723 -1.42784131]]\n",
      "Minibatch train loss at step 518: [ 0.00017391  0.00079052]\n",
      "Minibatch train loss mean at step 518: 0.0004822165647055954\n",
      "Minibatch train prediction at step 518: [2 0]\n",
      "Ground truth at step 518: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 519: [[-0.98937333 -3.57218719 -0.51186544  6.96264315]\n",
      " [ 7.05309534 -2.70495653 -2.0349102  -1.6068393 ]]\n",
      "Minibatch train loss at step 519: [ 0.00094548  0.00034422]\n",
      "Minibatch train loss mean at step 519: 0.0006448478670790792\n",
      "Minibatch train prediction at step 519: [3 0]\n",
      "Ground truth at step 519: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 520: [[ 7.57779074 -1.86990523 -2.44871545 -1.76675558]\n",
      " [-0.42717972 -1.25776947 -0.70121264  7.37385511]]\n",
      "Minibatch train loss at step 520: [ 0.00021062  0.00089843]\n",
      "Minibatch train loss mean at step 520: 0.0005545275053009391\n",
      "Minibatch train prediction at step 520: [0 3]\n",
      "Ground truth at step 520: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 521: [[-2.8865335  -2.09965825  7.10860443 -2.17244434]\n",
      " [-0.35809383  7.58224535 -1.11808217 -1.5453887 ]]\n",
      "Minibatch train loss at step 521: [ 0.00023899  0.00063101]\n",
      "Minibatch train loss mean at step 521: 0.0004350000526756048\n",
      "Minibatch train prediction at step 521: [2 1]\n",
      "Ground truth at step 521: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 522: [[-2.7909987  -3.30824113  6.5061121  -1.04596567]\n",
      " [-0.13552295  7.48639965 -1.2427386  -1.44511139]]\n",
      "Minibatch train loss at step 522: [ 0.00067116  0.00078326]\n",
      "Minibatch train loss mean at step 522: 0.000727208680473268\n",
      "Minibatch train prediction at step 522: [2 1]\n",
      "Ground truth at step 522: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 523: [[ 0.07819924  7.51710415 -1.40782547 -1.67863834]\n",
      " [-2.88830757 -2.10223246  7.1149683  -2.17445707]]\n",
      "Minibatch train loss at step 523: [ 0.00082209  0.00023696]\n",
      "Minibatch train loss mean at step 523: 0.0005295234150253236\n",
      "Minibatch train prediction at step 523: [1 2]\n",
      "Ground truth at step 523: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 524: [[ 7.60256577 -0.34322354 -2.23041511 -1.39563537]\n",
      " [-0.14055762  7.49476957 -1.24343657 -1.44645917]]\n",
      "Minibatch train loss at step 524: [ 0.00053129  0.00077396]\n",
      "Minibatch train loss mean at step 524: 0.0006526293000206351\n",
      "Minibatch train prediction at step 524: [0 1]\n",
      "Ground truth at step 524: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 525: [[-2.86516285 -2.90396833  7.13627243 -2.28675866]\n",
      " [ 7.60362625 -0.34301767 -2.23124218 -1.39592421]]\n",
      "Minibatch train loss at step 525: [ 0.00016974  0.0005307 ]\n",
      "Minibatch train loss mean at step 525: 0.00035021884832531214\n",
      "Minibatch train prediction at step 525: [2 0]\n",
      "Ground truth at step 525: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 526: [[ 7.70061827 -1.89722645 -2.03631878 -1.24404013]\n",
      " [-0.99248606 -3.57249522 -0.51228225  6.9740963 ]]\n",
      "Minibatch train loss at step 526: [ 0.00025722  0.00093333]\n",
      "Minibatch train loss mean at step 526: 0.0005952756037004292\n",
      "Minibatch train prediction at step 526: [0 3]\n",
      "Ground truth at step 526: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 527: [[ 7.73095942 -0.27237567 -2.15410876 -1.45460069]\n",
      " [-2.64657712 -3.36691236  6.75143623 -1.55685854]]\n",
      "Minibatch train loss at step 527: [ 0.00048769  0.0003696 ]\n",
      "Minibatch train loss mean at step 527: 0.00042864258284680545\n",
      "Minibatch train prediction at step 527: [0 2]\n",
      "Ground truth at step 527: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 528: [[-2.86748362 -2.90522313  7.14375734 -2.29092169]\n",
      " [-2.7953124  -3.31475711  6.5221777  -1.05385208]]\n",
      "Minibatch train loss at step 528: [ 0.00016795  0.00065567]\n",
      "Minibatch train loss mean at step 528: 0.0004118131473660469\n",
      "Minibatch train prediction at step 528: [2 2]\n",
      "Ground truth at step 528: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 529: [[ 7.06919003 -2.71215725 -2.0379827  -1.61204553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ 0.06054765  7.54115057 -1.40937722 -1.6805141 ]]\n",
      "Minibatch train loss at step 529: [ 0.00033707  0.00079207]\n",
      "Minibatch train loss mean at step 529: 0.0005645686760544777\n",
      "Minibatch train prediction at step 529: [0 1]\n",
      "Ground truth at step 529: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 530: [[-2.64966917 -3.36944461  6.75972557 -1.56077147]\n",
      " [ 7.70769262 -1.89810503 -2.03660035 -1.24864781]]\n",
      "Minibatch train loss at step 530: [ 0.00036519  0.00025484]\n",
      "Minibatch train loss mean at step 530: 0.00031001376919448376\n",
      "Minibatch train prediction at step 530: [2 0]\n",
      "Ground truth at step 530: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 531: [[ 7.61093283 -0.34461442 -2.23489451 -1.39803267]\n",
      " [-2.65079856 -3.3703208   6.76285791 -1.56245601]]\n",
      "Minibatch train loss at step 531: [ 0.00052581  0.00036364]\n",
      "Minibatch train loss mean at step 531: 0.00044472728041000664\n",
      "Minibatch train prediction at step 531: [0 2]\n",
      "Ground truth at step 531: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 532: [[-2.65205216 -3.37118483  6.76621008 -1.56431472]\n",
      " [ 7.4868803   0.04628251 -2.32562613 -1.43187094]]\n",
      "Minibatch train loss at step 532: [ 0.00036173  0.00077528]\n",
      "Minibatch train loss mean at step 532: 0.00056850491091609\n",
      "Minibatch train prediction at step 532: [2 0]\n",
      "Ground truth at step 532: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 533: [[ 7.61398792 -0.34616655 -2.23577428 -1.3988893 ]\n",
      " [ 7.48893023  0.04499765 -2.32587886 -1.43226242]]\n",
      "Minibatch train loss at step 533: [ 0.00052355  0.00077277]\n",
      "Minibatch train loss mean at step 533: 0.0006481614545919001\n",
      "Minibatch train prediction at step 533: [0 0]\n",
      "Ground truth at step 533: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 534: [[ 7.61658335 -0.34817165 -2.23591852 -1.39962494]\n",
      " [-2.87288642 -2.9077003   7.16133308 -2.30130339]]\n",
      "Minibatch train loss at step 534: [ 0.0005214  0.0001639]\n",
      "Minibatch train loss mean at step 534: 0.00034265199792571366\n",
      "Minibatch train prediction at step 534: [0 2]\n",
      "Ground truth at step 534: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 535: [[ 7.61932659 -0.35033783 -2.2359972  -1.40044415]\n",
      " [-0.43810278 -1.26163852 -0.70857894  7.40400457]]\n",
      "Minibatch train loss at step 535: [ 0.00051914  0.00086461]\n",
      "Minibatch train loss mean at step 535: 0.000691874825861305\n",
      "Minibatch train prediction at step 535: [0 3]\n",
      "Ground truth at step 535: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 536: [[-0.38033134  7.62340784 -1.12083387 -1.55239832]\n",
      " [-2.87475157 -2.90835071  7.16707611 -2.30466175]]\n",
      "Minibatch train loss at step 536: [ 0.00059694  0.00016259]\n",
      "Minibatch train loss mean at step 536: 0.00037976467865519226\n",
      "Minibatch train prediction at step 536: [1 2]\n",
      "Ground truth at step 536: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 537: [[ 7.62483931 -0.35454953 -2.23632503 -1.40203404]\n",
      " [ 7.58542681 -1.87454832 -2.45130157 -1.77408588]]\n",
      "Minibatch train loss at step 537: [ 0.00051473  0.00020788]\n",
      "Minibatch train loss mean at step 537: 0.0003613059234339744\n",
      "Minibatch train prediction at step 537: [0 0]\n",
      "Ground truth at step 537: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 538: [[-0.99940574 -3.57242584 -0.51185048  6.99229002]\n",
      " [-2.87669897 -2.90910244  7.17314243 -2.30823421]]\n",
      "Minibatch train loss at step 538: [ 0.00091439  0.00016128]\n",
      "Minibatch train loss mean at step 538: 0.0005378355272114277\n",
      "Minibatch train prediction at step 538: [3 2]\n",
      "Ground truth at step 538: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 539: [[-0.43961975 -1.263466   -0.71121114  7.41294909]\n",
      " [ 0.04708778  7.5642004  -1.41000438 -1.68595862]]\n",
      "Minibatch train loss at step 539: [ 0.0008552  0.0007661]\n",
      "Minibatch train loss mean at step 539: 0.000810651108622551\n",
      "Minibatch train prediction at step 539: [3 1]\n",
      "Ground truth at step 539: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 540: [[-2.80725384 -3.3262794   6.55879974 -1.07435071]\n",
      " [ 7.08781719 -2.72025633 -2.03992081 -1.61977458]]\n",
      "Minibatch train loss at step 540: [ 0.00062041  0.00032884]\n",
      "Minibatch train loss mean at step 540: 0.0004746277118101716\n",
      "Minibatch train prediction at step 540: [2 0]\n",
      "Ground truth at step 540: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 541: [[ 7.63427496 -0.36136332 -2.23728132 -1.40477288]\n",
      " [-1.00132561 -3.57203341 -0.51308972  6.99937725]]\n",
      "Minibatch train loss at step 541: [ 0.00050711  0.00090665]\n",
      "Minibatch train loss mean at step 541: 0.0007068796548992395\n",
      "Minibatch train prediction at step 541: [0 3]\n",
      "Ground truth at step 541: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 542: [[-2.66351056 -3.37786937  6.79309082 -1.57708323]\n",
      " [-0.44187266 -1.26489127 -0.71415359  7.42200804]]\n",
      "Minibatch train loss at step 542: [ 0.00034803  0.00084555]\n",
      "Minibatch train loss mean at step 542: 0.0005967910401523113\n",
      "Minibatch train prediction at step 542: [2 3]\n",
      "Ground truth at step 542: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 543: [[-2.66483951 -3.37873697  6.79590559 -1.57807422]\n",
      " [-2.81066346 -3.32939339  6.56737852 -1.07793975]]\n",
      "Minibatch train loss at step 543: [ 0.00034672  0.00061302]\n",
      "Minibatch train loss mean at step 543: 0.0004798721638508141\n",
      "Minibatch train prediction at step 543: [2 2]\n",
      "Ground truth at step 543: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 544: [[-2.66645503 -3.37969112  6.79976654 -1.58007455]\n",
      " [ 7.51879025  0.02218197 -2.32702303 -1.43821418]]\n",
      "Minibatch train loss at step 544: [ 0.00034481  0.00073644]\n",
      "Minibatch train loss mean at step 544: 0.000540627574082464\n",
      "Minibatch train prediction at step 544: [2 0]\n",
      "Ground truth at step 544: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 545: [[ 7.5922699  -1.8779062  -2.45260739 -1.77863944]\n",
      " [ 7.76524544 -0.29416093 -2.15981936 -1.46307051]]\n",
      "Minibatch train loss at step 545: [ 0.00020561  0.00046314]\n",
      "Minibatch train loss mean at step 545: 0.00033437745878472924\n",
      "Minibatch train prediction at step 545: [0 0]\n",
      "Ground truth at step 545: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 546: [[ 7.76752949 -0.29601178 -2.1599853  -1.46350491]\n",
      " [ 7.7336216  -1.90201783 -2.03721333 -1.26568747]]\n",
      "Minibatch train loss at step 546: [ 0.00046147  0.0002459 ]\n",
      "Minibatch train loss mean at step 546: 0.0003536852018442005\n",
      "Minibatch train prediction at step 546: [0 0]\n",
      "Ground truth at step 546: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 547: [[-1.00691962 -3.57081842 -0.51510227  7.01575565]\n",
      " [-2.67107487 -3.38208342  6.81059408 -1.58582091]]\n",
      "Minibatch train loss at step 547: [ 0.00088903  0.00033933]\n",
      "Minibatch train loss mean at step 547: 0.0006141782505437732\n",
      "Minibatch train prediction at step 547: [3 2]\n",
      "Ground truth at step 547: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 548: [[ 7.73725605 -1.90243876 -2.03734851 -1.26812041]\n",
      " [ 0.04054784  7.57756662 -1.41076791 -1.68985987]]\n",
      "Minibatch train loss at step 548: [ 0.00024471  0.00075193]\n",
      "Minibatch train loss mean at step 548: 0.0004983172984793782\n",
      "Minibatch train prediction at step 548: [0 1]\n",
      "Ground truth at step 548: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 549: [[ 7.59786415 -1.88007247 -2.45377135 -1.78145635]\n",
      " [-0.44672978 -1.26865804 -0.72126919  7.44389486]]\n",
      "Minibatch train loss at step 549: [ 0.00020407  0.0008228 ]\n",
      "Minibatch train loss mean at step 549: 0.0005134335369803011\n",
      "Minibatch train prediction at step 549: [0 3]\n",
      "Ground truth at step 549: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 550: [[ 0.03841824  7.58099508 -1.41075385 -1.69029772]\n",
      " [-0.38553214  7.64119196 -1.1225282  -1.55815232]]\n",
      "Minibatch train loss at step 550: [ 0.00074824  0.00058384]\n",
      "Minibatch train loss mean at step 550: 0.0006660354556515813\n",
      "Minibatch train prediction at step 550: [1 1]\n",
      "Ground truth at step 550: [1 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 551: [[-1.01062989 -3.56968451 -0.51739258  7.02811623]\n",
      " [ 7.60180616 -1.88141179 -2.45451832 -1.78315771]]\n",
      "Minibatch train loss at step 551: [ 0.0008758   0.00020299]\n",
      "Minibatch train loss mean at step 551: 0.0005393987521529198\n",
      "Minibatch train prediction at step 551: [3 0]\n",
      "Ground truth at step 551: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 552: [[ 0.03424073  7.58658314 -1.41082311 -1.69024181]\n",
      " [-2.88942385 -2.91384912  7.2094326  -2.32992435]]\n",
      "Minibatch train loss at step 552: [ 0.00074192  0.00015329]\n",
      "Minibatch train loss mean at step 552: 0.0004476065805647522\n",
      "Minibatch train prediction at step 552: [1 2]\n",
      "Ground truth at step 552: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 553: [[ 7.53790092  0.00795791 -2.32895398 -1.44093108]\n",
      " [ 7.65661669 -0.37774408 -2.24127889 -1.41002882]]\n",
      "Minibatch train loss at step 553: [ 0.0007144   0.00048971]\n",
      "Minibatch train loss mean at step 553: 0.0006020577857270837\n",
      "Minibatch train prediction at step 553: [0 0]\n",
      "Ground truth at step 553: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 554: [[-2.89097309 -2.91453671  7.2135272  -2.33221507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-2.82487178 -3.34093022  6.60229254 -1.09387958]]\n",
      "Minibatch train loss at step 554: [ 0.00015234  0.00058288]\n",
      "Minibatch train loss mean at step 554: 0.00036761030787602067\n",
      "Minibatch train prediction at step 554: [2 2]\n",
      "Ground truth at step 554: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 555: [[-0.16617492  7.55494213 -1.24913025 -1.46432555]\n",
      " [-0.45207566 -1.2709192  -0.72679728  7.46154785]]\n",
      "Minibatch train loss at step 555: [ 0.00071417  0.00080458]\n",
      "Minibatch train loss mean at step 555: 0.0007593717891722918\n",
      "Minibatch train prediction at step 555: [1 3]\n",
      "Ground truth at step 555: [1 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 556: [[-2.82708883 -3.34290457  6.60805941 -1.09664643]\n",
      " [-2.68262672 -3.38878036  6.83466911 -1.59468043]]\n",
      "Minibatch train loss at step 556: [ 0.00057812  0.00032825]\n",
      "Minibatch train loss mean at step 556: 0.00045318284537643194\n",
      "Minibatch train prediction at step 556: [2 2]\n",
      "Ground truth at step 556: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 557: [[-2.90434241 -2.12234139  7.18750095 -2.19291925]\n",
      " [ 7.66319561 -0.381814   -2.24304628 -1.41139233]]\n",
      "Minibatch train loss at step 557: [ 0.00021634  0.00048494]\n",
      "Minibatch train loss mean at step 557: 0.00035064324038103223\n",
      "Minibatch train prediction at step 557: [2 0]\n",
      "Ground truth at step 557: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 558: [[-2.68537998 -3.39035201  6.8415699  -1.59846163]\n",
      " [ 7.61574554 -1.88629651 -2.45703411 -1.78907359]]\n",
      "Minibatch train loss at step 558: [ 0.00032479  0.0001993 ]\n",
      "Minibatch train loss mean at step 558: 0.0002620453014969826\n",
      "Minibatch train prediction at step 558: [2 0]\n",
      "Ground truth at step 558: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 559: [[-1.01731014 -3.56740379 -0.52155977  7.05082512]\n",
      " [ 7.66634417 -0.38368374 -2.24393773 -1.41212118]]\n",
      "Minibatch train loss at step 559: [ 0.00085198  0.0004828 ]\n",
      "Minibatch train loss mean at step 559: 0.0006673918105661869\n",
      "Minibatch train prediction at step 559: [3 0]\n",
      "Ground truth at step 559: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 560: [[ 7.62010717 -1.88768482 -2.4579134  -1.79082489]\n",
      " [-2.89611626 -2.91668081  7.22959042 -2.34207916]]\n",
      "Minibatch train loss at step 560: [ 0.00019799  0.00014888]\n",
      "Minibatch train loss mean at step 560: 0.00017343417857773602\n",
      "Minibatch train prediction at step 560: [0 2]\n",
      "Ground truth at step 560: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 561: [[-2.89703202 -2.91705561  7.23230171 -2.34372759]\n",
      " [ 7.66966915 -0.38593519 -2.24455857 -1.41306496]]\n",
      "Minibatch train loss at step 561: [ 0.0001484  0.0004803]\n",
      "Minibatch train loss mean at step 561: 0.000314351316774264\n",
      "Minibatch train prediction at step 561: [2 0]\n",
      "Ground truth at step 561: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 562: [[ 7.75998878 -1.90500271 -2.03884745 -1.28285253]\n",
      " [ 7.62584352 -1.88945127 -2.45896006 -1.7929126 ]]\n",
      "Minibatch train loss at step 562: [ 0.0002372   0.00019656]\n",
      "Minibatch train loss mean at step 562: 0.00021687758271582425\n",
      "Minibatch train prediction at step 562: [0 0]\n",
      "Ground truth at step 562: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 563: [[-2.8354342  -3.35045838  6.63139057 -1.10966098]\n",
      " [ 7.67320395 -0.3884196  -2.24501538 -1.41418493]]\n",
      "Minibatch train loss at step 563: [ 0.0005581   0.00047768]\n",
      "Minibatch train loss mean at step 563: 0.0005178890423849225\n",
      "Minibatch train prediction at step 563: [2 0]\n",
      "Ground truth at step 563: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 564: [[-0.39576569  7.66325378 -1.12461555 -1.56272042]\n",
      " [-2.69296956 -3.39442301  6.85887432 -1.60679138]]\n",
      "Minibatch train loss at step 564: [ 0.00056716  0.00031681]\n",
      "Minibatch train loss mean at step 564: 0.00044198211980983615\n",
      "Minibatch train prediction at step 564: [1 2]\n",
      "Ground truth at step 564: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 565: [[-0.17190097  7.57026005 -1.25116742 -1.46947765]\n",
      " [-2.90732884 -2.12628388  7.20266294 -2.19761062]]\n",
      "Minibatch train loss at step 565: [ 0.00070011  0.00021217]\n",
      "Minibatch train loss mean at step 565: 0.00045613973634317517\n",
      "Minibatch train prediction at step 565: [1 2]\n",
      "Ground truth at step 565: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 566: [[ 7.12549496 -2.73342681 -2.04561591 -1.63091087]\n",
      " [-2.90156054 -2.91893721  7.24646044 -2.35249138]]\n",
      "Minibatch train loss at step 566: [ 0.00031371  0.00014542]\n",
      "Minibatch train loss mean at step 566: 0.00022956720204092562\n",
      "Minibatch train prediction at step 566: [0 2]\n",
      "Ground truth at step 566: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 567: [[ 7.64058352 -1.89372623 -2.46183848 -1.79810441]\n",
      " [ 7.12724638 -2.73416042 -2.04584599 -1.6316092 ]]\n",
      "Minibatch train loss at step 567: [ 0.00019298  0.00031299]\n",
      "Minibatch train loss mean at step 567: 0.0002529878984205425\n",
      "Minibatch train prediction at step 567: [0 0]\n",
      "Ground truth at step 567: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 568: [[ 7.76945734 -1.90635586 -2.03907752 -1.28913164]\n",
      " [-1.02374184 -3.56575871 -0.52376676  7.06982183]]\n",
      "Minibatch train loss at step 568: [ 0.0002341   0.00083293]\n",
      "Minibatch train loss mean at step 568: 0.0005335127934813499\n",
      "Minibatch train prediction at step 568: [0 3]\n",
      "Ground truth at step 568: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 569: [[-2.904217   -2.92018414  7.25467873 -2.35748887]\n",
      " [-0.39888442  7.67110157 -1.12549925 -1.56464136]]\n",
      "Minibatch train loss at step 569: [ 0.00014376  0.00056144]\n",
      "Minibatch train loss mean at step 569: 0.0003525966894812882\n",
      "Minibatch train prediction at step 569: [2 1]\n",
      "Ground truth at step 569: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 570: [[ 7.77357054 -1.90663481 -2.03946018 -1.2917155 ]\n",
      " [ 7.13455057 -2.73713613 -2.04711056 -1.63399017]]\n",
      "Minibatch train loss at step 570: [ 0.00023279  0.00031002]\n",
      "Minibatch train loss mean at step 570: 0.00027140197926200926\n",
      "Minibatch train prediction at step 570: [0 0]\n",
      "Ground truth at step 570: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 571: [[ 0.0104492   7.62306547 -1.412238   -1.69451606]\n",
      " [ 7.80559587 -0.31887016 -2.1681571  -1.47286451]]\n",
      "Minibatch train loss at step 571: [ 0.00070285  0.00043621]\n",
      "Minibatch train loss mean at step 571: 0.0005695301224477589\n",
      "Minibatch train prediction at step 571: [1 0]\n",
      "Ground truth at step 571: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 572: [[-2.91075993 -2.1298964   7.21519184 -2.20284152]\n",
      " [-2.84378386 -3.35920238  6.65633583 -1.12339222]]\n",
      "Minibatch train loss at step 572: [ 0.00020871  0.00053749]\n",
      "Minibatch train loss mean at step 572: 0.00037310156039893627\n",
      "Minibatch train prediction at step 572: [2 2]\n",
      "Ground truth at step 572: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 573: [[ 7.65560865 -1.89806128 -2.46493983 -1.80375218]\n",
      " [-2.90775537 -2.92203188  7.26526737 -2.36380696]]\n",
      "Minibatch train loss at step 573: [ 0.00018917  0.00014161]\n",
      "Minibatch train loss mean at step 573: 0.00016538891941308975\n",
      "Minibatch train prediction at step 573: [0 2]\n",
      "Ground truth at step 573: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 574: [[-0.40293446  7.6799202  -1.12649715 -1.5663476 ]\n",
      " [ 7.56953239 -0.00911762 -2.33614492 -1.44843721]]\n",
      "Minibatch train loss at step 574: [ 0.00055477  0.00068224]\n",
      "Minibatch train loss mean at step 574: 0.0006185028469190001\n",
      "Minibatch train prediction at step 574: [1 0]\n",
      "Ground truth at step 574: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 575: [[-2.90951729 -2.922961    7.27092218 -2.36725855]\n",
      " [ 7.5711484  -0.00991749 -2.33641505 -1.4488337 ]]\n",
      "Minibatch train loss at step 575: [ 0.00014054  0.00068057]\n",
      "Minibatch train loss mean at step 575: 0.00041055522160604596\n",
      "Minibatch train prediction at step 575: [2 0]\n",
      "Ground truth at step 575: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 576: [[-2.91249371 -2.13296461  7.22325993 -2.20573759]\n",
      " [ 7.81178904 -0.32149625 -2.16983104 -1.47484207]]\n",
      "Minibatch train loss at step 576: [ 0.00020633  0.0004324 ]\n",
      "Minibatch train loss mean at step 576: 0.0003193638985976577\n",
      "Minibatch train prediction at step 576: [2 0]\n",
      "Ground truth at step 576: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 577: [[ 7.78853559 -1.90799677 -2.04040909 -1.30131459]\n",
      " [-0.18025698  7.59143496 -1.25397587 -1.47651064]]\n",
      "Minibatch train loss at step 577: [ 0.00022814  0.00068057]\n",
      "Minibatch train loss mean at step 577: 0.00045435657375492156\n",
      "Minibatch train prediction at step 577: [0 1]\n",
      "Ground truth at step 577: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 578: [[-0.40520969  7.68655205 -1.127051   -1.56824732]\n",
      " [-0.18093404  7.59345341 -1.25417137 -1.4773246 ]]\n",
      "Minibatch train loss at step 578: [ 0.00055012  0.00067879]\n",
      "Minibatch train loss mean at step 578: 0.000614452175796032\n",
      "Minibatch train prediction at step 578: [1 1]\n",
      "Ground truth at step 578: [1 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 579: [[-1.02826166 -3.5654757  -0.52694845  7.08905554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ 7.81697369 -0.32431564 -2.17054629 -1.47641087]]\n",
      "Minibatch train loss at step 579: [ 0.00081411  0.00042918]\n",
      "Minibatch train loss mean at step 579: 0.0006216434412635863\n",
      "Minibatch train prediction at step 579: [3 0]\n",
      "Ground truth at step 579: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 580: [[ 7.69692707 -0.4011451  -2.25076389 -1.42231226]\n",
      " [ 7.15734005 -2.74676657 -2.05097628 -1.64168358]]\n",
      "Minibatch train loss at step 580: [ 0.00046135  0.00030108]\n",
      "Minibatch train loss mean at step 580: 0.0003812150389421731\n",
      "Minibatch train prediction at step 580: [0 0]\n",
      "Ground truth at step 580: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 581: [[-0.40836254  7.69412565 -1.12772834 -1.57009649]\n",
      " [ 7.82046318 -0.32604888 -2.17104602 -1.4776479 ]]\n",
      "Minibatch train loss at step 581: [ 0.00054488  0.00042704]\n",
      "Minibatch train loss mean at step 581: 0.0004859560285694897\n",
      "Minibatch train prediction at step 581: [1 0]\n",
      "Ground truth at step 581: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 582: [[ 7.1622057  -2.7488091  -2.05201387 -1.64303339]\n",
      " [ 7.58583307 -0.01821055 -2.33763719 -1.45281529]]\n",
      "Minibatch train loss at step 582: [ 0.00029917  0.00066592]\n",
      "Minibatch train loss mean at step 582: 0.0004825451469514519\n",
      "Minibatch train prediction at step 582: [0 0]\n",
      "Ground truth at step 582: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 583: [[ 7.16524887 -2.75012374 -2.05261254 -1.64395416]\n",
      " [-0.18571132  7.60696697 -1.25567222 -1.48238719]]\n",
      "Minibatch train loss at step 583: [ 0.00029798  0.00066699]\n",
      "Minibatch train loss mean at step 583: 0.00048248536768369377\n",
      "Minibatch train prediction at step 583: [0 1]\n",
      "Ground truth at step 583: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 584: [[-2.70920634 -3.40649819  6.89939404 -1.62447786]\n",
      " [ 7.82690954 -0.32969296 -2.17163205 -1.4798193 ]]\n",
      "Minibatch train loss at step 584: [ 0.00029929  0.00042298]\n",
      "Minibatch train loss mean at step 584: 0.00036113703390583396\n",
      "Minibatch train prediction at step 584: [2 0]\n",
      "Ground truth at step 584: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 585: [[ 7.82913542 -0.33099225 -2.17180896 -1.48054743]\n",
      " [ 7.6779151  -1.90572107 -2.46936297 -1.81294608]]\n",
      "Minibatch train loss at step 585: [ 0.00042167  0.00018368]\n",
      "Minibatch train loss mean at step 585: 0.0003026791091542691\n",
      "Minibatch train prediction at step 585: [0 0]\n",
      "Ground truth at step 585: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 586: [[ 7.59640884 -0.02467171 -2.33796406 -1.45588517]\n",
      " [-2.71043086 -3.40741158  6.90220451 -1.62537336]]\n",
      "Minibatch train loss at step 586: [ 0.00065544  0.00029798]\n",
      "Minibatch train loss mean at step 586: 0.0004767075297422707\n",
      "Minibatch train prediction at step 586: [0 2]\n",
      "Ground truth at step 586: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 587: [[-0.18821928  7.61652756 -1.25665486 -1.48666203]\n",
      " [-0.41257885  7.7073431  -1.12880492 -1.57420671]]\n",
      "Minibatch train loss at step 587: [ 0.00065901  0.00053594]\n",
      "Minibatch train loss mean at step 587: 0.00059747532941401\n",
      "Minibatch train prediction at step 587: [1 1]\n",
      "Ground truth at step 587: [1 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 588: [[ 7.80896473 -1.91091323 -2.04263735 -1.31337357]\n",
      " [-1.03028595 -3.5653162  -0.53157443  7.10517788]]\n",
      "Minibatch train loss at step 588: [ 0.00022194  0.00079826]\n",
      "Minibatch train loss mean at step 588: 0.0005101037095300853\n",
      "Minibatch train prediction at step 588: [0 3]\n",
      "Ground truth at step 588: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 589: [[-1.03063107 -3.56515741 -0.53249317  7.10786581]\n",
      " [-2.8542974  -3.37166977  6.6874671  -1.13689137]]\n",
      "Minibatch train loss at step 589: [ 0.00079552  0.00051426]\n",
      "Minibatch train loss mean at step 589: 0.0006548902601934969\n",
      "Minibatch train prediction at step 589: [3 2]\n",
      "Ground truth at step 589: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 590: [[ -2.27397773e-04   7.65757608e+00  -1.41593599e+00  -1.71042132e+00]\n",
      " [  7.60714579e+00  -3.14859152e-02  -2.33813405e+00  -1.45881367e+00]]\n",
      "Minibatch train loss at step 590: [ 0.00067223  0.00064471]\n",
      "Minibatch train loss mean at step 590: 0.0006584739894606173\n",
      "Minibatch train prediction at step 590: [1 0]\n",
      "Ground truth at step 590: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 591: [[-1.03182018 -3.56449795 -0.53497756  7.11517334]\n",
      " [-0.41600901  7.71694946 -1.12959802 -1.57696426]]\n",
      "Minibatch train loss at step 591: [ 0.00078826  0.00052951]\n",
      "Minibatch train loss mean at step 591: 0.0006588826654478908\n",
      "Minibatch train prediction at step 591: [3 1]\n",
      "Ground truth at step 591: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 592: [[ -2.11160397e-03   7.66207743e+00  -1.41621709e+00  -1.71203256e+00]\n",
      " [  7.84443426e+00  -3.40486258e-01  -2.17280364e+00  -1.48536921e+00]]\n",
      "Minibatch train loss at step 592: [ 0.00066806  0.00041202]\n",
      "Minibatch train loss mean at step 592: 0.0005400428199209273\n",
      "Minibatch train prediction at step 592: [1 0]\n",
      "Ground truth at step 592: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 593: [[ 7.84635115 -0.34152582 -2.17303038 -1.48593068]\n",
      " [ 7.81839705 -1.91200566 -2.04425192 -1.31860149]]\n",
      "Minibatch train loss at step 593: [ 0.00041107  0.0002192 ]\n",
      "Minibatch train loss mean at step 593: 0.0003151351120322943\n",
      "Minibatch train prediction at step 593: [0 0]\n",
      "Ground truth at step 593: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 594: [[-0.46658498 -1.27821434 -0.73842061  7.51571417]\n",
      " [ 7.69144964 -1.91145182 -2.47193789 -1.81958246]]\n",
      "Minibatch train loss at step 594: [ 0.000753    0.00018011]\n",
      "Minibatch train loss mean at step 594: 0.00046655445476062596\n",
      "Minibatch train prediction at step 594: [3 0]\n",
      "Ground truth at step 594: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 595: [[ -1.95988193e-01   7.63881063e+00  -1.25876474e+00  -1.49507761e+00]\n",
      " [ -6.73399633e-03   7.67007685e+00  -1.41622698e+00  -1.71338797e+00]]\n",
      "Minibatch train loss at step 595: [ 0.00064031  0.00066068]\n",
      "Minibatch train loss mean at step 595: 0.0006504922639578581\n",
      "Minibatch train prediction at step 595: [1 1]\n",
      "Ground truth at step 595: [1 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 596: [[-2.91765857 -2.14084101  7.25661945 -2.20819163]\n",
      " [ 7.72884703 -0.42146528 -2.254807   -1.4327935 ]]\n",
      "Minibatch train loss at step 596: [ 0.00019858  0.00043967]\n",
      "Minibatch train loss mean at step 596: 0.0003191246942151338\n",
      "Minibatch train prediction at step 596: [2 0]\n",
      "Ground truth at step 596: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 597: [[-2.71957326 -3.41356397  6.91903973 -1.6276933 ]\n",
      " [-0.46862799 -1.27868462 -0.73984098  7.52172422]]\n",
      "Minibatch train loss at step 597: [ 0.0002919  0.0007474]\n",
      "Minibatch train loss mean at step 597: 0.000519651104696095\n",
      "Minibatch train prediction at step 597: [2 3]\n",
      "Ground truth at step 597: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 598: [[-2.72043848 -3.41423082  6.92072105 -1.62791586]\n",
      " [-1.03576398 -3.56211901 -0.54419565  7.14095354]]\n",
      "Minibatch train loss at step 598: [ 0.00029131  0.00076289]\n",
      "Minibatch train loss mean at step 598: 0.0005270959227345884\n",
      "Minibatch train prediction at step 598: [2 3]\n",
      "Ground truth at step 598: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 599: [[-2.86290407 -3.37942195  6.70315838 -1.1369729 ]\n",
      " [-2.92407632 -2.92871404  7.30884504 -2.38704109]]\n",
      "Minibatch train loss at step 599: [ 0.00050532  0.00013327]\n",
      "Minibatch train loss mean at step 599: 0.00031929340912029147\n",
      "Minibatch train prediction at step 599: [2 2]\n",
      "Ground truth at step 599: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 600: [[ 7.20517206 -2.76540709 -2.06152654 -1.65340126]\n",
      " [ 7.83122206 -1.91332912 -2.04633784 -1.32602453]]\n",
      "Minibatch train loss at step 600: [ 0.00028344  0.00021539]\n",
      "Minibatch train loss mean at step 600: 0.00024941377341747284\n",
      "Minibatch train prediction at step 600: [0 0]\n",
      "Ground truth at step 600: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 601: [[-0.01893353  7.68712139 -1.41619313 -1.71386707]\n",
      " [-2.91971564 -2.14340162  7.26574993 -2.20927668]]\n",
      "Minibatch train loss at step 601: [ 0.00064388  0.00019644]\n",
      "Minibatch train loss mean at step 601: 0.0004201590199954808\n",
      "Minibatch train prediction at step 601: [1 2]\n",
      "Ground truth at step 601: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 602: [[-0.42907506  7.74418926 -1.13170779 -1.58262289]\n",
      " [-2.92612648 -2.92968512  7.31439972 -2.39018488]]\n",
      "Minibatch train loss at step 602: [ 0.00051068  0.00013231]\n",
      "Minibatch train loss mean at step 602: 0.0003214974422007799\n",
      "Minibatch train prediction at step 602: [1 2]\n",
      "Ground truth at step 602: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 603: [[ 7.70124435 -1.91588616 -2.47390056 -1.82528377]\n",
      " [ 7.83729553 -1.91378427 -2.0470562  -1.32977509]]\n",
      "Minibatch train loss at step 603: [ 0.00017761  0.0002136 ]\n",
      "Minibatch train loss mean at step 603: 0.00019560314831323922\n",
      "Minibatch train prediction at step 603: [0 0]\n",
      "Ground truth at step 603: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 604: [[ 7.70279026 -1.91640711 -2.47423339 -1.82598472]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ 7.63043356 -0.04184508 -2.34279346 -1.46339202]]\n",
      "Minibatch train loss at step 604: [ 0.00017713  0.00062434]\n",
      "Minibatch train loss mean at step 604: 0.0004007359384559095\n",
      "Minibatch train prediction at step 604: [0 0]\n",
      "Ground truth at step 604: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 605: [[-1.04038274 -3.56075501 -0.54976344  7.16195297]\n",
      " [ 7.21535301 -2.76884675 -2.06373572 -1.65591407]]\n",
      "Minibatch train loss at step 605: [ 0.00074335  0.00027975]\n",
      "Minibatch train loss mean at step 605: 0.000511548132635653\n",
      "Minibatch train prediction at step 605: [3 0]\n",
      "Ground truth at step 605: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 606: [[ 7.84410906 -1.91427076 -2.04788017 -1.33399463]\n",
      " [ 7.74126053 -0.42680079 -2.25945163 -1.43583941]]\n",
      "Minibatch train loss at step 606: [ 0.00021157  0.00043228]\n",
      "Minibatch train loss mean at step 606: 0.00032192637445405126\n",
      "Minibatch train prediction at step 606: [0 0]\n",
      "Ground truth at step 606: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 607: [[ 7.8468132  -1.91442108 -2.04824185 -1.33566332]\n",
      " [-0.43409696  7.7542181  -1.1325748  -1.5845021 ]]\n",
      "Minibatch train loss at step 607: [ 0.00021098  0.00050389]\n",
      "Minibatch train loss mean at step 607: 0.0003574340371415019\n",
      "Minibatch train prediction at step 607: [0 1]\n",
      "Ground truth at step 607: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 608: [[-0.43494236  7.75614309 -1.13275492 -1.5849334 ]\n",
      " [-2.92996716 -2.93166161  7.32461309 -2.3958354 ]]\n",
      "Minibatch train loss at step 608: [ 0.0005027   0.00013041]\n",
      "Minibatch train loss mean at step 608: 0.00031655243947170675\n",
      "Minibatch train prediction at step 608: [1 2]\n",
      "Ground truth at step 608: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 609: [[ 7.22512388 -2.77210879 -2.06600523 -1.65807569]\n",
      " [-0.03250953  7.70531082 -1.41617775 -1.7136364 ]]\n",
      "Minibatch train loss at step 609: [ 0.00027641  0.00062637]\n",
      "Minibatch train loss mean at step 609: 0.00045138795394450426\n",
      "Minibatch train prediction at step 609: [0 1]\n",
      "Ground truth at step 609: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 610: [[-2.73110819 -3.42266607  6.94267321 -1.63298452]\n",
      " [ 7.86945915 -0.35079134 -2.1792171  -1.49239171]]\n",
      "Minibatch train loss at step 610: [ 0.00028296  0.00039832]\n",
      "Minibatch train loss mean at step 610: 0.00034064045757986605\n",
      "Minibatch train prediction at step 610: [2 0]\n",
      "Ground truth at step 610: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 611: [[-0.43827075  7.76310205 -1.13350534 -1.58616197]\n",
      " [-0.47761416 -1.28161538 -0.74700928  7.54954243]]\n",
      "Minibatch train loss at step 611: [ 0.00049805  0.00072167]\n",
      "Minibatch train loss mean at step 611: 0.0006098612793721259\n",
      "Minibatch train prediction at step 611: [1 3]\n",
      "Ground truth at step 611: [1 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 612: [[ 7.64126778 -0.04583359 -2.34543967 -1.46562839]\n",
      " [-0.21615236  7.68288708 -1.26378226 -1.50817561]]\n",
      "Minibatch train loss at step 612: [ 0.00061541  0.00060302]\n",
      "Minibatch train loss mean at step 612: 0.0006092122639529407\n",
      "Minibatch train prediction at step 612: [0 1]\n",
      "Ground truth at step 612: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 613: [[-2.87398529 -3.39127851  6.72828436 -1.14269865]\n",
      " [-2.7332499  -3.42449307  6.94732904 -1.63396502]]\n",
      "Minibatch train loss at step 613: [ 0.00048947  0.00028141]\n",
      "Minibatch train loss mean at step 613: 0.000385443156119436\n",
      "Minibatch train prediction at step 613: [2 2]\n",
      "Ground truth at step 613: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 614: [[ 7.64446449 -0.04690997 -2.34630585 -1.46616971]\n",
      " [ 7.75131798 -0.43087998 -2.26291609 -1.43871868]]\n",
      "Minibatch train loss at step 614: [ 0.00061279  0.0004262 ]\n",
      "Minibatch train loss mean at step 614: 0.0005194940022192895\n",
      "Minibatch train prediction at step 614: [0 0]\n",
      "Ground truth at step 614: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 615: [[-0.21919429  7.6911869  -1.26500809 -1.51100564]\n",
      " [ 7.64703655 -0.04849891 -2.34641147 -1.46660638]]\n",
      "Minibatch train loss at step 615: [ 0.00059658  0.00061052]\n",
      "Minibatch train loss mean at step 615: 0.0006035532569512725\n",
      "Minibatch train prediction at step 615: [1 0]\n",
      "Ground truth at step 615: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 616: [[-2.93479252 -2.9340167   7.33817482 -2.40335345]\n",
      " [ 7.72205353 -1.92331684 -2.47800183 -1.83473873]]\n",
      "Minibatch train loss at step 616: [ 0.0001279  0.0001726]\n",
      "Minibatch train loss mean at step 616: 0.00015025176980998367\n",
      "Minibatch train prediction at step 616: [2 0]\n",
      "Ground truth at step 616: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 617: [[ 7.65277767 -0.05214432 -2.34644461 -1.46762609]\n",
      " [ 7.87015772 -1.91657782 -2.05072808 -1.35036492]]\n",
      "Minibatch train loss at step 617: [ 0.00060528  0.00020418]\n",
      "Minibatch train loss mean at step 617: 0.0004047327092848718\n",
      "Minibatch train prediction at step 617: [0 0]\n",
      "Ground truth at step 617: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 618: [[-0.48106375 -1.28365254 -0.7506749   7.56337452]\n",
      " [-0.44472146  7.77939224 -1.13532853 -1.59003747]]\n",
      "Minibatch train loss at step 618: [ 0.00070952  0.00048757]\n",
      "Minibatch train loss mean at step 618: 0.0005985433235764503\n",
      "Minibatch train prediction at step 618: [3 1]\n",
      "Ground truth at step 618: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 619: [[-2.93660879 -2.93469381  7.34363842 -2.40649319]\n",
      " [-2.87871575 -3.39564633  6.74106169 -1.14964306]]\n",
      "Minibatch train loss at step 619: [ 0.00012695  0.00048006]\n",
      "Minibatch train loss mean at step 619: 0.0003035048139281571\n",
      "Minibatch train prediction at step 619: [2 2]\n",
      "Ground truth at step 619: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 620: [[-0.48214379 -1.28456604 -0.75227201  7.56858492]\n",
      " [-2.92384148 -2.15271616  7.29806137 -2.21183276]]\n",
      "Minibatch train loss at step 620: [ 0.00070499  0.00018917]\n",
      "Minibatch train loss mean at step 620: 0.00044708041241392493\n",
      "Minibatch train prediction at step 620: [3 2]\n",
      "Ground truth at step 620: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 621: [[ 7.76622629 -0.44096264 -2.26486111 -1.44248509]\n",
      " [ 7.8892107  -0.36146742 -2.18260717 -1.4970454 ]]\n",
      "Minibatch train loss at step 621: [ 0.00041667  0.00038712]\n",
      "Minibatch train loss mean at step 621: 0.0004018928448203951\n",
      "Minibatch train prediction at step 621: [0 0]\n",
      "Ground truth at step 621: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 622: [[-2.92398477 -2.1544075   7.30253601 -2.21206498]\n",
      " [-0.44708467  7.78709412 -1.13640344 -1.5922668 ]]\n",
      "Minibatch train loss at step 622: [ 0.00018809  0.00048304]\n",
      "Minibatch train loss mean at step 622: 0.00033556658308953047\n",
      "Minibatch train prediction at step 622: [2 1]\n",
      "Ground truth at step 622: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 623: [[ 7.89347076 -0.36428124 -2.18311143 -1.49788153]\n",
      " [-0.22306198  7.70909166 -1.26782894 -1.51930666]]\n",
      "Minibatch train loss at step 623: [ 0.00038473  0.00058336]\n",
      "Minibatch train loss mean at step 623: 0.0004840464098379016\n",
      "Minibatch train prediction at step 623: [0 1]\n",
      "Ground truth at step 623: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 624: [[-1.04822803 -3.56085944 -0.55546153  7.19479847]\n",
      " [-2.8825047  -3.39989161  6.7527442  -1.15674329]]\n",
      "Minibatch train loss at step 624: [ 0.00071476  0.00047148]\n",
      "Minibatch train loss mean at step 624: 0.0005931212799623609\n",
      "Minibatch train prediction at step 624: [3 2]\n",
      "Ground truth at step 624: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 625: [[-1.04884136 -3.56082749 -0.5557456   7.19684029]\n",
      " [-0.22445785  7.71428633 -1.26871669 -1.52166831]]\n",
      "Minibatch train loss at step 625: [ 0.00071309  0.00057955]\n",
      "Minibatch train loss mean at step 625: 0.000646320404484868\n",
      "Minibatch train prediction at step 625: [3 1]\n",
      "Ground truth at step 625: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 626: [[-0.45016849  7.79602671 -1.13758206 -1.59491241]\n",
      " [-0.046476    7.73718786 -1.42003512 -1.72309613]]\n",
      "Minibatch train loss at step 626: [ 0.00047744  0.00059968]\n",
      "Minibatch train loss mean at step 626: 0.0005385598633438349\n",
      "Minibatch train prediction at step 626: [1 1]\n",
      "Ground truth at step 626: [1 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 627: [[ 7.8872714  -1.91930687 -2.05217719 -1.36111617]\n",
      " [-0.04831308  7.74009371 -1.42025471 -1.72342074]]\n",
      "Minibatch train loss at step 627: [ 0.00019966  0.00059706]\n",
      "Minibatch train loss mean at step 627: 0.0003983579226769507\n",
      "Minibatch train prediction at step 627: [0 1]\n",
      "Ground truth at step 627: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 628: [[-2.74604249 -3.43289852  6.97783804 -1.64744103]\n",
      " [-1.05089521 -3.56024098 -0.55844635  7.20607233]]\n",
      "Minibatch train loss at step 628: [ 0.0002695   0.00070487]\n",
      "Minibatch train loss mean at step 628: 0.00048718517064116895\n",
      "Minibatch train prediction at step 628: [2 3]\n",
      "Ground truth at step 628: [2 3]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 629: [[ 7.89088058 -1.91970098 -2.05274868 -1.36324024]\n",
      " [-0.23087458  7.72940683 -1.27059114 -1.52686059]]\n",
      "Minibatch train loss at step 629: [ 0.00019858  0.00056775]\n",
      "Minibatch train loss mean at step 629: 0.0003831674112007022\n",
      "Minibatch train prediction at step 629: [0 1]\n",
      "Ground truth at step 629: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 630: [[-0.05600791  7.75001144 -1.42031014 -1.72303605]\n",
      " [ 7.7351265  -1.92944181 -2.48044372 -1.84236729]]\n",
      "Minibatch train loss at step 630: [ 0.00058801  0.00016938]\n",
      "Minibatch train loss mean at step 630: 0.00037869386142119765\n",
      "Minibatch train prediction at step 630: [1 0]\n",
      "Ground truth at step 630: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 631: [[ 7.26259565 -2.78520656 -2.0739398  -1.66837895]\n",
      " [ 7.78114462 -0.44793609 -2.26952386 -1.44620562]]\n",
      "Minibatch train loss at step 631: [ 0.00026354  0.00040821]\n",
      "Minibatch train loss mean at step 631: 0.0003358727553859353\n",
      "Minibatch train prediction at step 631: [0 0]\n",
      "Ground truth at step 631: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 632: [[-1.05375493 -3.55913043 -0.56315011  7.21999168]\n",
      " [-0.2371569   7.74236679 -1.27183521 -1.53082335]]\n",
      "Minibatch train loss at step 632: [ 0.00069237  0.00055786]\n",
      "Minibatch train loss mean at step 632: 0.0006251146551221609\n",
      "Minibatch train prediction at step 632: [3 1]\n",
      "Ground truth at step 632: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 633: [[-2.89047503 -3.40850925  6.77048922 -1.16179574]\n",
      " [ 7.7831068  -0.44799858 -2.27062607 -1.44686842]]\n",
      "Minibatch train loss at step 633: [ 0.00046052  0.00040726]\n",
      "Minibatch train loss mean at step 633: 0.0004338869184721261\n",
      "Minibatch train prediction at step 633: [2 0]\n",
      "Ground truth at step 633: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 634: [[-2.94535661 -2.93891168  7.36953831 -2.42118335]\n",
      " [ 7.78426552 -0.44823927 -2.27111864 -1.44729018]]\n",
      "Minibatch train loss at step 634: [ 0.00012242  0.00040654]\n",
      "Minibatch train loss mean at step 634: 0.0002644803316798061\n",
      "Minibatch train prediction at step 634: [2 0]\n",
      "Ground truth at step 634: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 635: [[-2.94598436 -2.9392271   7.37132359 -2.42217159]\n",
      " [-0.24285839  7.75483179 -1.27289128 -1.53508902]]\n",
      "Minibatch train loss at step 635: [ 0.00012206  0.00054857]\n",
      "Minibatch train loss mean at step 635: 0.00033531637745909393\n",
      "Minibatch train prediction at step 635: [2 1]\n",
      "Ground truth at step 635: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 636: [[ 7.78687096 -0.44902921 -2.27198029 -1.44828331]\n",
      " [-1.05670011 -3.55794477 -0.56821269  7.23407364]]\n",
      "Minibatch train loss at step 636: [ 0.00040523  0.00067998]\n",
      "Minibatch train loss mean at step 636: 0.000542603200301528\n",
      "Minibatch train prediction at step 636: [0 3]\n",
      "Ground truth at step 636: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 637: [[ 7.27294254 -2.78824687 -2.07699418 -1.66984332]\n",
      " [ 7.91030455 -0.36642778 -2.18948483 -1.50439763]]\n",
      "Minibatch train loss at step 637: [ 0.00026032  0.00037699]\n",
      "Minibatch train loss mean at step 637: 0.0003186535614077002\n",
      "Minibatch train prediction at step 637: [0 0]\n",
      "Ground truth at step 637: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 638: [[-0.24766275  7.76636124 -1.27382302 -1.53954506]\n",
      " [-0.07570684  7.77453136 -1.41948986 -1.72207487]]\n",
      "Minibatch train loss at step 638: [ 0.00053999  0.00056632]\n",
      "Minibatch train loss mean at step 638: 0.0005531568313017488\n",
      "Minibatch train prediction at step 638: [1 1]\n",
      "Ground truth at step 638: [1 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 639: [[ 7.91290522 -0.36681858 -2.19022346 -1.50571001]\n",
      " [ 7.79121399 -0.45072949 -2.27305269 -1.45000839]]\n",
      "Minibatch train loss at step 639: [ 0.00037568  0.00040273]\n",
      "Minibatch train loss mean at step 639: 0.0003892020904459059\n",
      "Minibatch train prediction at step 639: [0 0]\n",
      "Ground truth at step 639: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 640: [[ 7.9102006  -1.92183971 -2.0563159  -1.3740958 ]\n",
      " [ 7.9145627  -0.36733595 -2.19053888 -1.5064311 ]]\n",
      "Minibatch train loss at step 640: [ 0.00019346  0.00037496]\n",
      "Minibatch train loss mean at step 640: 0.0002842100220732391\n",
      "Minibatch train prediction at step 640: [0 0]\n",
      "Ground truth at step 640: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 641: [[-0.25269446  7.77832937 -1.27460611 -1.54419291]\n",
      " [ 7.79499912 -0.45262337 -2.27361536 -1.45141399]]\n",
      "Minibatch train loss at step 641: [ 0.00053153  0.0004007 ]\n",
      "Minibatch train loss mean at step 641: 0.00046611676225438714\n",
      "Minibatch train prediction at step 641: [1 0]\n",
      "Ground truth at step 641: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 642: [[ 7.69434261 -0.06765547 -2.35551262 -1.47722566]\n",
      " [-2.75783491 -3.44157934  6.99976444 -1.65116441]]\n",
      "Minibatch train loss at step 642: [ 0.00057252  0.00026199]\n",
      "Minibatch train loss mean at step 642: 0.0004172525950707495\n",
      "Minibatch train prediction at step 642: [0 2]\n",
      "Ground truth at step 642: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 643: [[-2.7585814  -3.44198847  7.00116158 -1.65149951]\n",
      " [ 7.92046976 -0.37002462 -2.19119096 -1.50871766]]\n",
      "Minibatch train loss at step 643: [ 0.00026151  0.00037186]\n",
      "Minibatch train loss mean at step 643: 0.0003166873939335346\n",
      "Minibatch train prediction at step 643: [2 0]\n",
      "Ground truth at step 643: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 644: [[ 7.91715813 -1.92294478 -2.05746198 -1.37804747]\n",
      " [-0.49797252 -1.28885663 -0.76345927  7.6137867 ]]\n",
      "Minibatch train loss at step 644: [ 0.00019179  0.0006658 ]\n",
      "Minibatch train loss mean at step 644: 0.0004287949705030769\n",
      "Minibatch train prediction at step 644: [0 3]\n",
      "Ground truth at step 644: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 645: [[-0.25708124  7.79157877 -1.27549398 -1.55033851]\n",
      " [-2.90011191 -3.41733623  6.78901815 -1.16577888]]\n",
      "Minibatch train loss at step 645: [ 0.00052236  0.00044979]\n",
      "Minibatch train loss mean at step 645: 0.00048607628559693694\n",
      "Minibatch train prediction at step 645: [1 2]\n",
      "Ground truth at step 645: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 646: [[-0.49916217 -1.28942013 -0.76453376  7.61781549]\n",
      " [-0.25826138  7.79504013 -1.27583218 -1.55186045]]\n",
      "Minibatch train loss at step 646: [ 0.00066246  0.00052009]\n",
      "Minibatch train loss mean at step 646: 0.0005912794731557369\n",
      "Minibatch train prediction at step 646: [3 1]\n",
      "Ground truth at step 646: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 647: [[ 7.92303467 -1.92376614 -2.05811477 -1.38175833]\n",
      " [-2.95298553 -2.94229841  7.39034748 -2.43213296]]\n",
      "Minibatch train loss at step 647: [ 0.00019012  0.00011896]\n",
      "Minibatch train loss mean at step 647: 0.00015454227104783058\n",
      "Minibatch train prediction at step 647: [0 2]\n",
      "Ground truth at step 647: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 648: [[-2.90277147 -3.41982007  6.79565573 -1.16928387]\n",
      " [ 7.92984247 -0.37437129 -2.19263887 -1.51178622]]\n",
      "Minibatch train loss at step 648: [ 0.00044527  0.00036698]\n",
      "Minibatch train loss mean at step 648: 0.00040612241718918085\n",
      "Minibatch train prediction at step 648: [2 0]\n",
      "Ground truth at step 648: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 649: [[-2.76400065 -3.44507384  7.01327944 -1.65665138]\n",
      " [-0.0928864   7.80055666 -1.41871357 -1.72536051]]\n",
      "Minibatch train loss at step 649: [ 0.0002571  0.000545 ]\n",
      "Minibatch train loss mean at step 649: 0.0004010484553873539\n",
      "Minibatch train prediction at step 649: [2 1]\n",
      "Ground truth at step 649: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 650: [[-2.90473533 -3.42167068  6.80129766 -1.17320716]\n",
      " [-0.26364383  7.8095541  -1.27753603 -1.55771255]]\n",
      "Minibatch train loss at step 650: [ 0.00044133  0.0005102 ]\n",
      "Minibatch train loss mean at step 650: 0.0004757696879096329\n",
      "Minibatch train prediction at step 650: [2 1]\n",
      "Ground truth at step 650: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 651: [[ 7.81366205 -0.46263617 -2.27708244 -1.45720398]\n",
      " [-2.76616955 -3.4461987   7.01905107 -1.6601882 ]]\n",
      "Minibatch train loss at step 651: [ 0.0003901   0.00025484]\n",
      "Minibatch train loss mean at step 651: 0.0003224664251320064\n",
      "Minibatch train prediction at step 651: [0 2]\n",
      "Ground truth at step 651: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 652: [[ 7.29681206 -2.7959795  -2.08225965 -1.67629576]\n",
      " [-0.50435251 -1.29132283 -0.76893187  7.63223028]]\n",
      "Minibatch train loss at step 652: [ 0.00025257  0.00065019]\n",
      "Minibatch train loss mean at step 652: 0.00045138353016227484\n",
      "Minibatch train prediction at step 652: [0 3]\n",
      "Ground truth at step 652: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 653: [[ 7.93474722 -1.92546618 -2.05873227 -1.38969779]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ 7.71367693 -0.07573975 -2.35947704 -1.48182499]]\n",
      "Minibatch train loss at step 653: [ 0.00018702  0.00055762]\n",
      "Minibatch train loss mean at step 653: 0.0003723233239725232\n",
      "Minibatch train prediction at step 653: [0 0]\n",
      "Ground truth at step 653: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 654: [[-0.2688891   7.82321596 -1.27921164 -1.56291366]\n",
      " [ 7.93699169 -1.92575896 -2.05886483 -1.39121819]]\n",
      "Minibatch train loss at step 654: [ 0.00050103  0.00018655]\n",
      "Minibatch train loss mean at step 654: 0.0003437877167016268\n",
      "Minibatch train prediction at step 654: [1 0]\n",
      "Ground truth at step 654: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 655: [[-2.93504882 -2.16491866  7.35637283 -2.22005939]\n",
      " [ 7.93964529 -1.92606187 -2.05905843 -1.39301562]]\n",
      "Minibatch train loss at step 655: [ 0.00017653  0.00018583]\n",
      "Minibatch train loss mean at step 655: 0.00018118169100489467\n",
      "Minibatch train prediction at step 655: [2 0]\n",
      "Ground truth at step 655: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 656: [[ 7.71893072 -0.07801909 -2.36053252 -1.48278761]\n",
      " [-2.91081786 -3.4275105   6.81992435 -1.18693066]]\n",
      "Minibatch train loss at step 656: [ 0.00055369  0.00042799]\n",
      "Minibatch train loss mean at step 656: 0.0004908409900963306\n",
      "Minibatch train prediction at step 656: [0 2]\n",
      "Ground truth at step 656: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 657: [[-2.77236533 -3.44962001  7.03581333 -1.67029464]\n",
      " [-0.48797256  7.87746716 -1.14482832 -1.61503756]]\n",
      "Minibatch train loss at step 657: [ 0.00024852  0.00042882]\n",
      "Minibatch train loss mean at step 657: 0.0003386717871762812\n",
      "Minibatch train prediction at step 657: [2 1]\n",
      "Ground truth at step 657: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 658: [[-0.48875028  7.87962055 -1.14510524 -1.61568952]\n",
      " [ 7.8246994  -0.46825722 -2.28007078 -1.45996177]]\n",
      "Minibatch train loss at step 658: [ 0.00042763  0.0003839 ]\n",
      "Minibatch train loss mean at step 658: 0.000405765458708629\n",
      "Minibatch train prediction at step 658: [1 0]\n",
      "Ground truth at step 658: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 659: [[-2.77437663 -3.45071602  7.04156303 -1.67386377]\n",
      " [-2.93592119 -2.16780281  7.36492014 -2.22206497]]\n",
      "Minibatch train loss at step 659: [ 0.00024638  0.00017475]\n",
      "Minibatch train loss mean at step 659: 0.000210560392588377\n",
      "Minibatch train prediction at step 659: [2 2]\n",
      "Ground truth at step 659: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 660: [[-1.06990778 -3.55758548 -0.57372385  7.27416372]\n",
      " [-2.93635964 -2.16886234  7.36744165 -2.22289658]]\n",
      "Minibatch train loss at step 660: [ 0.00064793  0.00017415]\n",
      "Minibatch train loss mean at step 660: 0.0004110402660444379\n",
      "Minibatch train prediction at step 660: [3 2]\n",
      "Ground truth at step 660: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 661: [[-0.10711543  7.8238616  -1.42007482 -1.72869527]\n",
      " [ 7.95479584 -1.92793858 -2.06003809 -1.40330553]]\n",
      "Minibatch train loss at step 661: [ 0.000527    0.00018202]\n",
      "Minibatch train loss mean at step 661: 0.00035451032454147935\n",
      "Minibatch train prediction at step 661: [1 0]\n",
      "Ground truth at step 661: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 662: [[-2.77751231 -3.45279026  7.04995728 -1.67823529]\n",
      " [-0.51052725 -1.29514313 -0.77507621  7.65414619]]\n",
      "Minibatch train loss at step 662: [ 0.0002434   0.00063256]\n",
      "Minibatch train loss mean at step 662: 0.00043797926628030837\n",
      "Minibatch train prediction at step 662: [2 3]\n",
      "Ground truth at step 662: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 663: [[ 7.73155069 -0.08391332 -2.36238718 -1.48489296]\n",
      " [ 7.83197546 -0.47194511 -2.28194046 -1.46179533]]\n",
      "Minibatch train loss at step 663: [ 0.00054404  0.00038009]\n",
      "Minibatch train loss mean at step 663: 0.0004620642866939306\n",
      "Minibatch train prediction at step 663: [0 0]\n",
      "Ground truth at step 663: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 664: [[-0.11073401  7.82917976 -1.42010438 -1.72895849]\n",
      " [ 7.95349932 -0.3829349  -2.19858289 -1.51851082]]\n",
      "Minibatch train loss at step 664: [ 0.00052283  0.00035554]\n",
      "Minibatch train loss mean at step 664: 0.00043918626033701\n",
      "Minibatch train prediction at step 664: [1 0]\n",
      "Ground truth at step 664: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 665: [[ 7.73610878 -0.08669813 -2.36253357 -1.48536694]\n",
      " [ 7.96384144 -1.92898226 -2.0609405  -1.40921962]]\n",
      "Minibatch train loss at step 665: [ 0.00054047  0.00017987]\n",
      "Minibatch train loss mean at step 665: 0.00036016933154314756\n",
      "Minibatch train prediction at step 665: [0 0]\n",
      "Ground truth at step 665: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 666: [[-1.07330954 -3.55776596 -0.57430452  7.2837925 ]\n",
      " [-2.96326923 -2.94653916  7.42371178 -2.45273399]]\n",
      "Minibatch train loss at step 666: [ 0.00064066  0.0001136 ]\n",
      "Minibatch train loss mean at step 666: 0.0003771319461520761\n",
      "Minibatch train prediction at step 666: [3 2]\n",
      "Ground truth at step 666: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 667: [[ 7.96865129 -1.92954969 -2.06155419 -1.412251  ]\n",
      " [ 7.95956659 -0.38746461 -2.19889498 -1.519104  ]]\n",
      "Minibatch train loss at step 667: [ 0.00017868  0.0003522 ]\n",
      "Minibatch train loss mean at step 667: 0.0002654400886967778\n",
      "Minibatch train prediction at step 667: [0 0]\n",
      "Ground truth at step 667: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 668: [[-0.49546444  7.89826345 -1.14682698 -1.62164772]\n",
      " [ 7.84216213 -0.47911438 -2.28279519 -1.46399438]]\n",
      "Minibatch train loss at step 668: [ 0.00041762  0.00037413]\n",
      "Minibatch train loss mean at step 668: 0.0003958750457968563\n",
      "Minibatch train prediction at step 668: [1 0]\n",
      "Ground truth at step 668: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 669: [[ 7.74826527 -1.93990302 -2.48472834 -1.85868001]\n",
      " [-0.51370144 -1.29824042 -0.77905864  7.67008543]]\n",
      "Minibatch train loss at step 669: [ 0.00016521  0.00062041]\n",
      "Minibatch train loss mean at step 669: 0.00039281073259189725\n",
      "Minibatch train prediction at step 669: [0 3]\n",
      "Ground truth at step 669: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 670: [[ 7.84648657 -0.48218364 -2.28307748 -1.46500838]\n",
      " [ 7.74935198 -0.09607881 -2.36194587 -1.486709  ]]\n",
      "Minibatch train loss at step 670: [ 0.00037174  0.00052951]\n",
      "Minibatch train loss mean at step 670: 0.0004506256664171815\n",
      "Minibatch train prediction at step 670: [0 0]\n",
      "Ground truth at step 670: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 671: [[ 7.97840834 -1.93065917 -2.06301451 -1.41829586]\n",
      " [ 7.84921074 -0.48428759 -2.28308153 -1.46566129]]\n",
      "Minibatch train loss at step 671: [ 0.00017618  0.0003702 ]\n",
      "Minibatch train loss mean at step 671: 0.0002731856657192111\n",
      "Minibatch train prediction at step 671: [0 0]\n",
      "Ground truth at step 671: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 672: [[ 7.75202847 -1.94156551 -2.48530006 -1.86058366]\n",
      " [ 7.97118378 -0.39718664 -2.19889045 -1.52032292]]\n",
      "Minibatch train loss at step 672: [ 0.00016426  0.00034577]\n",
      "Minibatch train loss mean at step 672: 0.00025501163327135146\n",
      "Minibatch train prediction at step 672: [0 0]\n",
      "Ground truth at step 672: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 673: [[ 7.98341608 -1.93130016 -2.06376553 -1.42140889]\n",
      " [-2.78712249 -3.4585979   7.07070351 -1.68532848]]\n",
      "Minibatch train loss at step 673: [ 0.0001751   0.00023648]\n",
      "Minibatch train loss mean at step 673: 0.00020579318515956402\n",
      "Minibatch train prediction at step 673: [0 2]\n",
      "Ground truth at step 673: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 674: [[ 7.97642183 -0.40182585 -2.19869018 -1.5209446 ]\n",
      " [-0.11544907  7.84035921 -1.41913104 -1.73244929]]\n",
      "Minibatch train loss at step 674: [ 0.00034303  0.00051533]\n",
      "Minibatch train loss mean at step 674: 0.00042917681275866926\n",
      "Minibatch train prediction at step 674: [0 1]\n",
      "Ground truth at step 674: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 675: [[ 7.8600564  -0.49256304 -2.28297353 -1.46845233]\n",
      " [-0.27854031  7.86349249 -1.28440917 -1.58293331]]\n",
      "Minibatch train loss at step 675: [ 0.00036388  0.00047625]\n",
      "Minibatch train loss mean at step 675: 0.00042006332660093904\n",
      "Minibatch train prediction at step 675: [0 1]\n",
      "Ground truth at step 675: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 676: [[-1.07796574 -3.55707431 -0.57973796  7.30356646]\n",
      " [-0.27882576  7.86512947 -1.28453314 -1.58383238]]\n",
      "Minibatch train loss at step 676: [ 0.00062506  0.00047541]\n",
      "Minibatch train loss mean at step 676: 0.0005502350395545363\n",
      "Minibatch train prediction at step 676: [3 1]\n",
      "Ground truth at step 676: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 677: [[-2.78981733 -3.4599421   7.07594061 -1.68656754]\n",
      " [-0.11793943  7.84465408 -1.41845345 -1.73317015]]\n",
      "Minibatch train loss at step 677: [ 0.00023493  0.00051223]\n",
      "Minibatch train loss mean at step 677: 0.00037358212284743786\n",
      "Minibatch train prediction at step 677: [2 1]\n",
      "Ground truth at step 677: [2 1]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 678: [[-2.9687202  -2.94790936  7.4365325  -2.45958734]\n",
      " [-1.07904768 -3.55661678 -0.58177608  7.30878735]]\n",
      "Minibatch train loss at step 678: [ 0.00011157  0.00062077]\n",
      "Minibatch train loss mean at step 678: 0.0003661710652522743\n",
      "Minibatch train prediction at step 678: [2 3]\n",
      "Ground truth at step 678: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 679: [[ 7.33343935 -2.80779815 -2.08804679 -1.68793833]\n",
      " [-1.07982981 -3.55619955 -0.58347738  7.31264925]]\n",
      "Minibatch train loss at step 679: [ 0.00024113  0.00061755]\n",
      "Minibatch train loss mean at step 679: 0.0004293415695428848\n",
      "Minibatch train prediction at step 679: [0 3]\n",
      "Ground truth at step 679: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 680: [[ 7.33533478 -2.80825114 -2.0886476  -1.68803787]\n",
      " [-0.50006306  7.9153285  -1.14753938 -1.62861991]]\n",
      "Minibatch train loss at step 680: [ 0.00024054  0.00040892]\n",
      "Minibatch train loss mean at step 680: 0.0003247294225730002\n",
      "Minibatch train prediction at step 680: [0 1]\n",
      "Ground truth at step 680: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 681: [[ 7.76565742 -1.94714391 -2.4875896  -1.86698246]\n",
      " [ 7.98988438 -0.41103131 -2.19940233 -1.52341533]]\n",
      "Minibatch train loss at step 681: [ 0.0001614   0.00033611]\n",
      "Minibatch train loss mean at step 681: 0.00024875503731891513\n",
      "Minibatch train prediction at step 681: [0 0]\n",
      "Ground truth at step 681: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 682: [[-0.28386948  7.87882948 -1.28559721 -1.58940148]\n",
      " [-0.51889867 -1.30264974 -0.78513592  7.69560528]]\n",
      "Minibatch train loss at step 682: [ 0.00046695  0.00060159]\n",
      "Minibatch train loss mean at step 682: 0.0005342702497728169\n",
      "Minibatch train prediction at step 682: [1 3]\n",
      "Ground truth at step 682: [1 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 683: [[-0.50264305  7.92144823 -1.14804053 -1.63038373]\n",
      " [ 7.87474108 -0.50141817 -2.2845037  -1.47259557]]\n",
      "Minibatch train loss at step 683: [ 0.00040559  0.00035613]\n",
      "Minibatch train loss mean at step 683: 0.00038086046697571874\n",
      "Minibatch train prediction at step 683: [1 0]\n",
      "Ground truth at step 683: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 684: [[-2.79539871 -3.46383762  7.08434296 -1.68536758]\n",
      " [-0.12877369  7.85874605 -1.41707182 -1.73266327]]\n",
      "Minibatch train loss at step 684: [ 0.00023279  0.00050151]\n",
      "Minibatch train loss mean at step 684: 0.00036714778980240226\n",
      "Minibatch train prediction at step 684: [2 1]\n",
      "Ground truth at step 684: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 685: [[-2.93461561 -3.44774079  6.87159348 -1.20439124]\n",
      " [ 7.34633017 -2.81124878 -2.09188604 -1.68964386]]\n",
      "Minibatch train loss at step 685: [ 0.00039891  0.00023744]\n",
      "Minibatch train loss mean at step 685: 0.00031817532726563513\n",
      "Minibatch train prediction at step 685: [2 0]\n",
      "Ground truth at step 685: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 686: [[-0.28848565  7.89006186 -1.28705478 -1.59328723]\n",
      " [ 7.7849617  -0.1181281  -2.36249852 -1.49256289]]\n",
      "Minibatch train loss at step 686: [ 0.00045992  0.0005021 ]\n",
      "Minibatch train loss mean at step 686: 0.0004810127429664135\n",
      "Minibatch train prediction at step 686: [1 0]\n",
      "Ground truth at step 686: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 687: [[-0.50707477  7.93154192 -1.14909863 -1.63294888]\n",
      " [-0.2897054   7.89311075 -1.28740609 -1.59437668]]\n",
      "Minibatch train loss at step 687: [ 0.00040034  0.00045814]\n",
      "Minibatch train loss mean at step 687: 0.00042923970613628626\n",
      "Minibatch train prediction at step 687: [1 1]\n",
      "Ground truth at step 687: [1 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 688: [[ 7.35334015 -2.8133173  -2.09362626 -1.6913836 ]\n",
      " [ 7.99964523 -0.41460943 -2.20175004 -1.52598226]]\n",
      "Minibatch train loss at step 688: [ 0.00023529  0.0003317 ]\n",
      "Minibatch train loss mean at step 688: 0.00028349796775728464\n",
      "Minibatch train prediction at step 688: [0 0]\n",
      "Ground truth at step 688: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 689: [[-1.08712828 -3.55314159 -0.59900004  7.34740543]\n",
      " [-0.50984651  7.93788338 -1.1498332  -1.6346153 ]]\n",
      "Minibatch train loss at step 689: [ 0.00058944  0.00039689]\n",
      "Minibatch train loss mean at step 689: 0.0004931617877446115\n",
      "Minibatch train prediction at step 689: [3 1]\n",
      "Ground truth at step 689: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 690: [[ 7.77681971 -1.95176625 -2.48968315 -1.87290609]\n",
      " [-2.79987121 -3.46701908  7.09353065 -1.68753684]]\n",
      "Minibatch train loss at step 690: [ 0.00015889  0.00023005]\n",
      "Minibatch train loss mean at step 690: 0.00019447039812803268\n",
      "Minibatch train prediction at step 690: [0 2]\n",
      "Ground truth at step 690: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 691: [[-0.29562113  7.90736055 -1.28931403 -1.59928417]\n",
      " [ 7.77854204 -1.95237589 -2.4900198  -1.87367153]]\n",
      "Minibatch train loss at step 691: [ 0.00044944  0.0001583 ]\n",
      "Minibatch train loss mean at step 691: 0.0003038673021364957\n",
      "Minibatch train prediction at step 691: [1 0]\n",
      "Ground truth at step 691: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 692: [[-1.08942807 -3.55222654 -0.60348922  7.35706902]\n",
      " [ 7.36342239 -2.81629038 -2.09628654 -1.69370556]]\n",
      "Minibatch train loss at step 692: [ 0.00058169  0.00023243]\n",
      "Minibatch train loss mean at step 692: 0.0004070612194482237\n",
      "Minibatch train prediction at step 692: [3 0]\n",
      "Ground truth at step 692: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 693: [[ 8.01823807 -1.93560934 -2.07114005 -1.44067693]\n",
      " [-2.80212092 -3.46865129  7.0978241  -1.68814456]]\n",
      "Minibatch train loss at step 693: [ 0.000167    0.00022874]\n",
      "Minibatch train loss mean at step 693: 0.00019786736811511219\n",
      "Minibatch train prediction at step 693: [0 2]\n",
      "Ground truth at step 693: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 694: [[-0.14606296  7.88327694 -1.41750395 -1.73320794]\n",
      " [-1.09145367 -3.55136657 -0.60758597  7.36527395]]\n",
      "Minibatch train loss at step 694: [ 0.00048363  0.00057514]\n",
      "Minibatch train loss mean at step 694: 0.0005293864523991942\n",
      "Minibatch train prediction at step 694: [1 3]\n",
      "Ground truth at step 694: [1 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 695: [[-0.14820455  7.88583231 -1.41739869 -1.73291004]\n",
      " [-2.97654128 -2.95110536  7.45340157 -2.46741366]]\n",
      "Minibatch train loss at step 695: [ 0.00048173  0.00010895]\n",
      "Minibatch train loss mean at step 695: 0.0002953396178781986\n",
      "Minibatch train prediction at step 695: [1 2]\n",
      "Ground truth at step 695: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 696: [[ 7.8905592  -0.50607222 -2.29016423 -1.47734714]\n",
      " [-1.09399307 -3.55018735 -0.61277819  7.37522507]]\n",
      "Minibatch train loss at step 696: [ 0.00034898  0.00056716]\n",
      "Minibatch train loss mean at step 696: 0.0004580700187943876\n",
      "Minibatch train prediction at step 696: [0 3]\n",
      "Ground truth at step 696: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 697: [[-0.30459929  7.9263525  -1.29157841 -1.60508418]\n",
      " [ 8.00871754 -0.41492754 -2.20535755 -1.52956975]]\n",
      "Minibatch train loss at step 697: [ 0.000438    0.00032813]\n",
      "Minibatch train loss mean at step 697: 0.00038306377246044576\n",
      "Minibatch train prediction at step 697: [1 0]\n",
      "Ground truth at step 697: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 698: [[-2.97821999 -2.95196247  7.45695257 -2.46899199]\n",
      " [-1.09697604 -3.54873967 -0.61905563  7.38668108]]\n",
      "Minibatch train loss at step 698: [ 0.00010836  0.00055798]\n",
      "Minibatch train loss mean at step 698: 0.0003331687767058611\n",
      "Minibatch train prediction at step 698: [2 3]\n",
      "Ground truth at step 698: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 699: [[-2.80734682 -3.47256017  7.10584068 -1.6872766 ]\n",
      " [ 7.38061237 -2.82054496 -2.10154247 -1.69602466]]\n",
      "Minibatch train loss at step 699: [ 0.00022671  0.00022754]\n",
      "Minibatch train loss mean at step 699: 0.000227127515245229\n",
      "Minibatch train prediction at step 699: [2 0]\n",
      "Ground truth at step 699: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 700: [[-0.15987742  7.89897776 -1.41612065 -1.73067009]\n",
      " [-2.94630766 -2.18551803  7.42525625 -2.22334313]]\n",
      "Minibatch train loss at step 700: [ 0.00047196  0.00016283]\n",
      "Minibatch train loss mean at step 700: 0.0003173920267727226\n",
      "Minibatch train prediction at step 700: [1 2]\n",
      "Ground truth at step 700: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 701: [[ 7.79934311 -0.11758725 -2.36895943 -1.49619746]\n",
      " [ 7.38569975 -2.82174063 -2.10312176 -1.69665575]]\n",
      "Minibatch train loss at step 701: [ 0.0004946   0.00022611]\n",
      "Minibatch train loss mean at step 701: 0.00036035533412359655\n",
      "Minibatch train prediction at step 701: [0 0]\n",
      "Ground truth at step 701: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 702: [[ 7.80055809 -0.11797425 -2.36914015 -1.49637461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-0.31145996  7.94001675 -1.29278803 -1.60913992]]\n",
      "Minibatch train loss at step 702: [ 0.00049376  0.00042978]\n",
      "Minibatch train loss mean at step 702: 0.00046176923206076026\n",
      "Minibatch train prediction at step 702: [0 1]\n",
      "Ground truth at step 702: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 703: [[-0.16612518  7.90616846 -1.41520858 -1.72962284]\n",
      " [ 8.03250694 -1.93673003 -2.07561755 -1.44720221]]\n",
      "Minibatch train loss at step 703: [ 0.00046683  0.00016402]\n",
      "Minibatch train loss mean at step 703: 0.0003154261503368616\n",
      "Minibatch train prediction at step 703: [1 0]\n",
      "Ground truth at step 703: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 704: [[-1.10575616 -3.54489946 -0.63680637  7.41868114]\n",
      " [-0.52854979  7.97709846 -1.15346885 -1.64433002]]\n",
      "Minibatch train loss at step 704: [ 0.00053308  0.00037687]\n",
      "Minibatch train loss mean at step 704: 0.0004549749137368053\n",
      "Minibatch train prediction at step 704: [3 1]\n",
      "Ground truth at step 704: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 705: [[-2.94876409 -2.18752933  7.43181849 -2.22409415]\n",
      " [-2.98255301 -2.95415902  7.46651602 -2.47330213]]\n",
      "Minibatch train loss at step 705: [ 0.00016152  0.00010693]\n",
      "Minibatch train loss mean at step 705: 0.00013422028860077262\n",
      "Minibatch train prediction at step 705: [2 2]\n",
      "Ground truth at step 705: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 706: [[ 7.39904451 -2.82524991 -2.10699916 -1.69882739]\n",
      " [ 7.90093517 -0.50963408 -2.29337502 -1.48030329]]\n",
      "Minibatch train loss at step 706: [ 0.00022254  0.0003441 ]\n",
      "Minibatch train loss mean at step 706: 0.0002833185135386884\n",
      "Minibatch train prediction at step 706: [0 0]\n",
      "Ground truth at step 706: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 707: [[ 7.80754757 -0.12026668 -2.36995649 -1.49740875]\n",
      " [ 7.40194511 -2.82602715 -2.10782886 -1.69935822]]\n",
      "Minibatch train loss at step 707: [ 0.00048947  0.00022182]\n",
      "Minibatch train loss mean at step 707: 0.0003556483134161681\n",
      "Minibatch train prediction at step 707: [0 0]\n",
      "Ground truth at step 707: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 708: [[ 7.79899168 -1.96118569 -2.49373555 -1.88492453]\n",
      " [-1.11126709 -3.54276085 -0.64793038  7.43808222]]\n",
      "Minibatch train loss at step 708: [ 0.00015377  0.00051843]\n",
      "Minibatch train loss mean at step 708: 0.0003360970877110958\n",
      "Minibatch train prediction at step 708: [0 3]\n",
      "Ground truth at step 708: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 709: [[ 8.02223587 -0.41957459 -2.20801044 -1.5333271 ]\n",
      " [-2.9850049  -2.95542073  7.47209549 -2.47583294]]\n",
      "Minibatch train loss at step 709: [ 0.00032253  0.00010609]\n",
      "Minibatch train loss mean at step 709: 0.00021430948982015252\n",
      "Minibatch train prediction at step 709: [0 2]\n",
      "Ground truth at step 709: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 710: [[-0.1777516   7.92082405 -1.41351032 -1.7287122 ]\n",
      " [ 8.02385998 -0.42077124 -2.20800543 -1.53360438]]\n",
      "Minibatch train loss at step 710: [ 0.00045671  0.00032169]\n",
      "Minibatch train loss mean at step 710: 0.00038919990765862167\n",
      "Minibatch train prediction at step 710: [1 0]\n",
      "Ground truth at step 710: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 711: [[-2.95558643 -3.46548605  6.89932156 -1.19531024]\n",
      " [-2.81622553 -3.48011398  7.11890078 -1.68466747]]\n",
      "Minibatch train loss at step 711: [ 0.00038914  0.00022361]\n",
      "Minibatch train loss mean at step 711: 0.00030637712916359305\n",
      "Minibatch train prediction at step 711: [2 2]\n",
      "Ground truth at step 711: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 712: [[-0.53521508  7.99243498 -1.15410793 -1.64904201]\n",
      " [ 7.91034985 -0.51546323 -2.29412675 -1.48271728]]\n",
      "Minibatch train loss at step 712: [ 0.00036936  0.00033933]\n",
      "Minibatch train loss mean at step 712: 0.00035434632445685565\n",
      "Minibatch train prediction at step 712: [1 0]\n",
      "Ground truth at step 712: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 713: [[-0.53322583 -1.30494809 -0.79171711  7.73735285]\n",
      " [-0.53587335  7.99423885 -1.15423632 -1.64963114]]\n",
      "Minibatch train loss at step 713: [ 0.00057168  0.00036853]\n",
      "Minibatch train loss mean at step 713: 0.0004701053549069911\n",
      "Minibatch train prediction at step 713: [3 1]\n",
      "Ground truth at step 713: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 714: [[-0.53667414  7.99639559 -1.15450001 -1.65026343]\n",
      " [-2.98818493 -2.95688891  7.48019171 -2.48009777]]\n",
      "Minibatch train loss at step 714: [ 0.00036745  0.0001049 ]\n",
      "Minibatch train loss mean at step 714: 0.00023617669648956507\n",
      "Minibatch train prediction at step 714: [1 2]\n",
      "Ground truth at step 714: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 715: [[ 8.03168869 -0.42565307 -2.20876551 -1.53455865]\n",
      " [-2.95187521 -2.1936245   7.44704103 -2.22469902]]\n",
      "Minibatch train loss at step 715: [ 0.000318    0.00015854]\n",
      "Minibatch train loss mean at step 715: 0.0002382678067078814\n",
      "Minibatch train prediction at step 715: [0 2]\n",
      "Ground truth at step 715: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 716: [[-0.53833407  8.00096703 -1.15513372 -1.65162957]\n",
      " [-1.12167442 -3.53910041 -0.66578668  7.47018433]]\n",
      "Minibatch train loss at step 716: [ 0.00036543  0.00049483]\n",
      "Minibatch train loss mean at step 716: 0.0004301317094359547\n",
      "Minibatch train prediction at step 716: [1 3]\n",
      "Ground truth at step 716: [1 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 717: [[-0.3239952   7.97138786 -1.29593658 -1.62068343]\n",
      " [-0.18662588  7.93361235 -1.41281879 -1.72864115]]\n",
      "Minibatch train loss at step 717: [ 0.00041226  0.00044825]\n",
      "Minibatch train loss mean at step 717: 0.00043025281047448516\n",
      "Minibatch train prediction at step 717: [1 1]\n",
      "Ground truth at step 717: [1 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 718: [[-2.9612298  -3.47045779  6.90915918 -1.19748878]\n",
      " [-2.95332408 -2.19586778  7.45233727 -2.22566891]]\n",
      "Minibatch train loss at step 718: [ 0.00038414  0.00015746]\n",
      "Minibatch train loss mean at step 718: 0.00027080043219029903\n",
      "Minibatch train prediction at step 718: [2 2]\n",
      "Ground truth at step 718: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 719: [[ 7.8277092  -0.12980902 -2.37164736 -1.50006437]\n",
      " [-2.99126935 -2.9585948   7.48911524 -2.48499227]]\n",
      "Minibatch train loss at step 719: [ 0.00047601  0.00010371]\n",
      "Minibatch train loss mean at step 719: 0.0002898576494771987\n",
      "Minibatch train prediction at step 719: [0 2]\n",
      "Ground truth at step 719: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 720: [[-0.53667116 -1.30652821 -0.79438913  7.74887943]\n",
      " [ 7.92146587 -0.52054131 -2.29679012 -1.48493302]]\n",
      "Minibatch train loss at step 720: [ 0.00056358  0.00033433]\n",
      "Minibatch train loss mean at step 720: 0.0004489540297072381\n",
      "Minibatch train prediction at step 720: [3 0]\n",
      "Ground truth at step 720: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 721: [[ 7.80932236 -1.96644223 -2.49557018 -1.89162338]\n",
      " [ 7.92309427 -0.52141654 -2.29711103 -1.48520684]]\n",
      "Minibatch train loss at step 721: [ 0.00015162  0.00033349]\n",
      "Minibatch train loss mean at step 721: 0.00024255734751932323\n",
      "Minibatch train prediction at step 721: [0 0]\n",
      "Ground truth at step 721: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 722: [[ 7.92495871 -0.52247941 -2.29735422 -1.4855988 ]\n",
      " [-2.82460499 -3.48627043  7.13594627 -1.68985474]]\n",
      "Minibatch train loss at step 722: [ 0.00033266  0.00021849]\n",
      "Minibatch train loss mean at step 722: 0.0002755722962319851\n",
      "Minibatch train prediction at step 722: [0 2]\n",
      "Ground truth at step 722: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 723: [[-2.993752   -2.95986414  7.49677706 -2.48945045]\n",
      " [ 7.92698479 -0.52370107 -2.29753375 -1.4860605 ]]\n",
      "Minibatch train loss at step 723: [ 0.00010251  0.00033159]\n",
      "Minibatch train loss mean at step 723: 0.00021704999380744994\n",
      "Minibatch train prediction at step 723: [2 0]\n",
      "Ground truth at step 723: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 724: [[ 7.81324863 -1.96797574 -2.49624944 -1.89344239]\n",
      " [ 7.43719578 -2.83578753 -2.11753201 -1.7062155 ]]\n",
      "Minibatch train loss at step 724: [ 0.00015067  0.00021229]\n",
      "Minibatch train loss mean at step 724: 0.00018147920491173863\n",
      "Minibatch train prediction at step 724: [0 0]\n",
      "Ground truth at step 724: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 725: [[ 7.81548738 -1.9686904  -2.49665284 -1.8942523 ]\n",
      " [ 7.43908787 -2.8363378  -2.11798167 -1.7067287 ]]\n",
      "Minibatch train loss at step 725: [ 0.00015031  0.00021181]\n",
      "Minibatch train loss mean at step 725: 0.00018106204515788704\n",
      "Minibatch train prediction at step 725: [0 0]\n",
      "Ground truth at step 725: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 726: [[-1.13234949 -3.53616428 -0.6798327   7.49895573]\n",
      " [-0.5393616  -1.30888331 -0.79758602  7.76065826]]\n",
      "Minibatch train loss at step 726: [ 0.00047494  0.00055548]\n",
      "Minibatch train loss mean at step 726: 0.0005152082303538918\n",
      "Minibatch train prediction at step 726: [3 3]\n",
      "Ground truth at step 726: [3 3]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 727: [[ 7.8213172  -1.97045016 -2.49768996 -1.89620996]\n",
      " [-0.19638854  7.94961596 -1.41284859 -1.73001504]]\n",
      "Minibatch train loss at step 727: [ 0.00014912  0.00043824]\n",
      "Minibatch train loss mean at step 727: 0.00029367810930125415\n",
      "Minibatch train prediction at step 727: [0 1]\n",
      "Ground truth at step 727: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 728: [[ 7.84524632 -0.13943438 -2.37296796 -1.50284827]\n",
      " [ 7.44619751 -2.83820844 -2.11981964 -1.70844293]]\n",
      "Minibatch train loss at step 728: [ 0.00046421  0.00021002]\n",
      "Minibatch train loss mean at step 728: 0.00033711857395246625\n",
      "Minibatch train prediction at step 728: [0 0]\n",
      "Ground truth at step 728: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 729: [[-2.95674324 -2.20461845  7.47224283 -2.22899866]\n",
      " [-0.54128206 -1.31025517 -0.79979217  7.76806402]]\n",
      "Minibatch train loss at step 729: [ 0.00015341  0.00055024]\n",
      "Minibatch train loss mean at step 729: 0.000351824244717136\n",
      "Minibatch train prediction at step 729: [2 3]\n",
      "Ground truth at step 729: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 730: [[-2.99760246 -2.96159434  7.50759888 -2.49578667]\n",
      " [ 7.45163107 -2.83963132 -2.12122369 -1.70974326]]\n",
      "Minibatch train loss at step 730: [ 0.00010108  0.00020859]\n",
      "Minibatch train loss mean at step 730: 0.00015483943570870906\n",
      "Minibatch train prediction at step 730: [2 0]\n",
      "Ground truth at step 730: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 731: [[-0.54281086 -1.31152523 -0.80175155  7.77429581]\n",
      " [-2.99819803 -2.96189904  7.50925016 -2.49674511]]\n",
      "Minibatch train loss at step 731: [ 0.00054595  0.00010085]\n",
      "Minibatch train loss mean at step 731: 0.0003233973402529955\n",
      "Minibatch train prediction at step 731: [3 2]\n",
      "Ground truth at step 731: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 732: [[-0.19989952  7.9549818  -1.4125489  -1.72992015]\n",
      " [ 8.06528664 -1.9416374  -2.08408427 -1.46242106]]\n",
      "Minibatch train loss at step 732: [ 0.0004349   0.00015699]\n",
      "Minibatch train loss mean at step 732: 0.0002959432022180408\n",
      "Minibatch train prediction at step 732: [1 0]\n",
      "Ground truth at step 732: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 733: [[-2.97210741 -3.48063612  6.93015099 -1.20424426]\n",
      " [ 8.06674004 -1.9417696  -2.08441162 -1.46320045]]\n",
      "Minibatch train loss at step 733: [ 0.00037329  0.00015663]\n",
      "Minibatch train loss mean at step 733: 0.00026496127247810364\n",
      "Minibatch train prediction at step 733: [2 0]\n",
      "Ground truth at step 733: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 734: [[-3.00021291 -2.96307278  7.51532269 -2.50040412]\n",
      " [-0.54578859 -1.3136133  -0.80524105  7.78479624]]\n",
      "Minibatch train loss at step 734: [  9.98923933e-05   5.38680877e-04]\n",
      "Minibatch train loss mean at step 734: 0.00031928662792779505\n",
      "Minibatch train prediction at step 734: [2 3]\n",
      "Ground truth at step 734: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 735: [[-0.5490545   8.03195095 -1.15876675 -1.66148758]\n",
      " [-2.95845532 -2.20979834  7.48276138 -2.23085999]]\n",
      "Minibatch train loss at step 735: [ 0.00035113  0.00015138]\n",
      "Minibatch train loss mean at step 735: 0.0002512566279619932\n",
      "Minibatch train prediction at step 735: [1 2]\n",
      "Ground truth at step 735: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 736: [[-2.83403945 -3.49342608  7.15459776 -1.69536841]\n",
      " [-0.20445596  7.96039295 -1.41224873 -1.728531  ]]\n",
      "Minibatch train loss at step 736: [ 0.000213    0.00043133]\n",
      "Minibatch train loss mean at step 736: 0.00032216485124081373\n",
      "Minibatch train prediction at step 736: [2 1]\n",
      "Ground truth at step 736: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 737: [[ 7.85835075 -0.1457783  -2.37561965 -1.5036236 ]\n",
      " [ 7.46948624 -2.84406948 -2.12581587 -1.71434319]]\n",
      "Minibatch train loss at step 737: [ 0.00045575  0.00020383]\n",
      "Minibatch train loss mean at step 737: 0.0003297897637821734\n",
      "Minibatch train prediction at step 737: [0 0]\n",
      "Ground truth at step 737: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 738: [[ 7.47209883 -2.84473753 -2.12643933 -1.715096  ]\n",
      " [-0.20744215  7.96385336 -1.41200423 -1.72761655]]\n",
      "Minibatch train loss at step 738: [ 0.00020323  0.00042906]\n",
      "Minibatch train loss mean at step 738: 0.00031614629551768303\n",
      "Minibatch train prediction at step 738: [0 1]\n",
      "Ground truth at step 738: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 739: [[-1.14347541 -3.53499246 -0.69180131  7.52868319]\n",
      " [ 8.06493092 -0.44292483 -2.21533632 -1.53859401]]\n",
      "Minibatch train loss at step 739: [ 0.00045599  0.00030358]\n",
      "Minibatch train loss mean at step 739: 0.000379785371478647\n",
      "Minibatch train prediction at step 739: [3 0]\n",
      "Ground truth at step 739: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 740: [[-0.33455557  8.00446224 -1.30243051 -1.63308978]\n",
      " [-1.1445272  -3.53480911 -0.6931355   7.53156042]]\n",
      "Minibatch train loss at step 740: [ 0.00039498  0.00045408]\n",
      "Minibatch train loss mean at step 740: 0.00042453291825950146\n",
      "Minibatch train prediction at step 740: [1 3]\n",
      "Ground truth at step 740: [1 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 741: [[ 7.84675694 -1.97870457 -2.50216532 -1.90611494]\n",
      " [-3.00482893 -2.96584988  7.52953863 -2.50894213]]\n",
      "Minibatch train loss at step 741: [  1.44232836e-04   9.78660391e-05]\n",
      "Minibatch train loss mean at step 741: 0.00012104943743906915\n",
      "Minibatch train prediction at step 741: [0 2]\n",
      "Ground truth at step 741: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 742: [[-2.96158957 -2.21584129  7.49564075 -2.23452806]\n",
      " [ 8.06811714 -0.44423383 -2.21625996 -1.53885114]]\n",
      "Minibatch train loss at step 742: [ 0.00014876  0.00030239]\n",
      "Minibatch train loss mean at step 742: 0.00022557518968824297\n",
      "Minibatch train prediction at step 742: [2 0]\n",
      "Ground truth at step 742: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 743: [[ 7.86572075 -0.1484199  -2.37743402 -1.50378394]\n",
      " [-1.14862823 -3.53366566 -0.69923162  7.54310942]]\n",
      "Minibatch train loss at step 743: [ 0.00045158  0.00044658]\n",
      "Minibatch train loss mean at step 743: 0.0004490797291509807\n",
      "Minibatch train prediction at step 743: [0 3]\n",
      "Ground truth at step 743: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 744: [[-0.33703732  8.01029778 -1.303262   -1.6350348 ]\n",
      " [-2.98013353 -3.4892869   6.94688559 -1.20958388]]\n",
      "Minibatch train loss at step 744: [ 0.00039188  0.00036495]\n",
      "Minibatch train loss mean at step 744: 0.0003784177824854851\n",
      "Minibatch train prediction at step 744: [1 2]\n",
      "Ground truth at step 744: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 745: [[-2.84052014 -3.49887776  7.1674571  -1.69872904]\n",
      " [-0.55576974  8.04726505 -1.16040444 -1.66523278]]\n",
      "Minibatch train loss at step 745: [ 0.00020931  0.00034434]\n",
      "Minibatch train loss mean at step 745: 0.0002768229751382023\n",
      "Minibatch train prediction at step 745: [2 1]\n",
      "Ground truth at step 745: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 746: [[ 7.85352516 -1.9813292  -2.50331831 -1.90919042]\n",
      " [ 8.08562279 -1.9436667  -2.08837223 -1.47351849]]\n",
      "Minibatch train loss at step 746: [ 0.00014292  0.00015281]\n",
      "Minibatch train loss mean at step 746: 0.0001478681806474924\n",
      "Minibatch train prediction at step 746: [0 0]\n",
      "Ground truth at step 746: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 747: [[-2.84218979 -3.50000358  7.17101812 -1.70011961]\n",
      " [ 8.08718109 -1.94385457 -2.0886898  -1.47439492]]\n",
      "Minibatch train loss at step 747: [ 0.00020848  0.00015234]\n",
      "Minibatch train loss mean at step 747: 0.00018040659779217094\n",
      "Minibatch train prediction at step 747: [2 0]\n",
      "Ground truth at step 747: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 748: [[ 7.8730855  -0.15207131 -2.37842298 -1.50482821]\n",
      " [-1.15554523 -3.5316236  -0.70847028  7.56161737]]\n",
      "Minibatch train loss at step 748: [ 0.00044682  0.0004349 ]\n",
      "Minibatch train loss mean at step 748: 0.00044085795525461435\n",
      "Minibatch train prediction at step 748: [0 3]\n",
      "Ground truth at step 748: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 749: [[-1.15704679 -3.5311389  -0.71050727  7.5657196 ]\n",
      " [-0.55819494  8.05364227 -1.1611985  -1.66728914]]\n",
      "Minibatch train loss at step 749: [ 0.0004324   0.00034136]\n",
      "Minibatch train loss mean at step 749: 0.0003868774510920048\n",
      "Minibatch train prediction at step 749: [3 1]\n",
      "Ground truth at step 749: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 750: [[ 8.09282875 -1.94441676 -2.08992147 -1.47761166]\n",
      " [-2.96569157 -2.22155762  7.50926208 -2.23904824]]\n",
      "Minibatch train loss at step 750: [ 0.00015127  0.00014602]\n",
      "Minibatch train loss mean at step 750: 0.00014864292461425066\n",
      "Minibatch train prediction at step 750: [0 2]\n",
      "Ground truth at step 750: [0 2]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 751: [[-2.84576082 -3.50236702  7.1779356  -1.70202947]\n",
      " [ 8.07886791 -0.45052797 -2.21788359 -1.5405035 ]]\n",
      "Minibatch train loss at step 751: [ 0.00020645  0.00029762]\n",
      "Minibatch train loss mean at step 751: 0.0002520352427382022\n",
      "Minibatch train prediction at step 751: [2 0]\n",
      "Ground truth at step 751: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 752: [[ 8.08013916 -0.45142055 -2.21797514 -1.5407095 ]\n",
      " [-0.34105361  8.02173042 -1.30527186 -1.63966548]]\n",
      "Minibatch train loss at step 752: [ 0.00029714  0.00038593]\n",
      "Minibatch train loss mean at step 752: 0.00034153490560129285\n",
      "Minibatch train prediction at step 752: [0 1]\n",
      "Ground truth at step 752: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 753: [[-2.98847389 -3.49618864  6.96147394 -1.2138375 ]\n",
      " [-0.55777341 -1.32150865 -0.81725264  7.82816315]]\n",
      "Minibatch train loss at step 753: [ 0.00035792  0.00051009]\n",
      "Minibatch train loss mean at step 753: 0.00043400353752076626\n",
      "Minibatch train prediction at step 753: [2 3]\n",
      "Ground truth at step 753: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 754: [[-3.01233768 -2.96949506  7.54937744 -2.52031422]\n",
      " [-1.16437387 -3.52884698 -0.72052723  7.58651114]]\n",
      "Minibatch train loss at step 754: [  9.52436894e-05   4.19886172e-04]\n",
      "Minibatch train loss mean at step 754: 0.00025756494142115116\n",
      "Minibatch train prediction at step 754: [2 3]\n",
      "Ground truth at step 754: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 755: [[-0.22288571  7.98694086 -1.41224074 -1.72803295]\n",
      " [-3.01299906 -2.96981788  7.55119038 -2.5214107 ]]\n",
      "Minibatch train loss at step 755: [  4.15000628e-04   9.50052927e-05]\n",
      "Minibatch train loss mean at step 755: 0.00025500296032987535\n",
      "Minibatch train prediction at step 755: [1 2]\n",
      "Ground truth at step 755: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 756: [[ 7.50806665 -2.85312223 -2.13614154 -1.72205913]\n",
      " [ 7.86538172 -1.98613727 -2.50546765 -1.91502535]]\n",
      "Minibatch train loss at step 756: [ 0.00019441  0.00014054]\n",
      "Minibatch train loss mean at step 756: 0.0001674746599746868\n",
      "Minibatch train prediction at step 756: [0 0]\n",
      "Ground truth at step 756: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 757: [[-3.01441288 -2.97061324  7.55532694 -2.52386689]\n",
      " [-2.96956182 -2.22647715  7.52153111 -2.24367714]]\n",
      "Minibatch train loss at step 757: [  9.45284992e-05   1.43636877e-04]\n",
      "Minibatch train loss mean at step 757: 0.00011908268788829446\n",
      "Minibatch train prediction at step 757: [2 2]\n",
      "Ground truth at step 757: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 758: [[ 7.97236347 -0.54827046 -2.30772114 -1.49383545]\n",
      " [ 7.88724422 -0.15967718 -2.38022137 -1.5070641 ]]\n",
      "Minibatch train loss at step 758: [ 0.00031097  0.00043788]\n",
      "Minibatch train loss mean at step 758: 0.0003744238638319075\n",
      "Minibatch train prediction at step 758: [0 0]\n",
      "Ground truth at step 758: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 759: [[-3.01588917 -2.97146678  7.55977011 -2.52647948]\n",
      " [-2.85304332 -3.50710225  7.19269848 -1.70698285]]\n",
      "Minibatch train loss at step 759: [  9.38133089e-05   2.02277704e-04]\n",
      "Minibatch train loss mean at step 759: 0.0001480455102864653\n",
      "Minibatch train prediction at step 759: [2 2]\n",
      "Ground truth at step 759: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 760: [[-0.56343293  8.06897831 -1.16317606 -1.67285764]\n",
      " [-0.56118327 -1.32388031 -0.82060003  7.84123182]]\n",
      "Minibatch train loss at step 760: [ 0.0003348   0.00050186]\n",
      "Minibatch train loss mean at step 760: 0.000418333598645404\n",
      "Minibatch train prediction at step 760: [1 3]\n",
      "Ground truth at step 760: [1 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 761: [[ 7.97656679 -0.55105394 -2.30822563 -1.49457169]\n",
      " [-2.97162628 -2.22987342  7.52906656 -2.24617124]]\n",
      "Minibatch train loss at step 761: [ 0.00030894  0.00014209]\n",
      "Minibatch train loss mean at step 761: 0.00022551506117451936\n",
      "Minibatch train prediction at step 761: [0 2]\n",
      "Ground truth at step 761: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 762: [[ 8.11172295 -1.946473   -2.0940125  -1.48820949]\n",
      " [-0.56415653  8.07161236 -1.16358185 -1.67387784]]\n",
      "Minibatch train loss at step 762: [ 0.00014745  0.00033361]\n",
      "Minibatch train loss mean at step 762: 0.00024053108063526452\n",
      "Minibatch train prediction at step 762: [0 1]\n",
      "Ground truth at step 762: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 763: [[ 8.11330128 -1.94667923 -2.0943253  -1.48912382]\n",
      " [ 7.89568472 -0.16469954 -2.38084984 -1.50834072]]\n",
      "Minibatch train loss at step 763: [ 0.00014721  0.00043252]\n",
      "Minibatch train loss mean at step 763: 0.0002898648090194911\n",
      "Minibatch train prediction at step 763: [0 0]\n",
      "Ground truth at step 763: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 764: [[ 7.89780378 -0.16605093 -2.38088989 -1.5087018 ]\n",
      " [-2.99800777 -3.5047133   6.98106194 -1.22141218]]\n",
      "Minibatch train loss at step 764: [ 0.00043109  0.00034815]\n",
      "Minibatch train loss mean at step 764: 0.00038961839163675904\n",
      "Minibatch train prediction at step 764: [0 2]\n",
      "Ground truth at step 764: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 765: [[-0.56346494 -1.32628632 -0.82358778  7.85182714]\n",
      " [-0.34500942  8.03696632 -1.30868864 -1.64670444]]\n",
      "Minibatch train loss at step 765: [ 0.00049531  0.00037866]\n",
      "Minibatch train loss mean at step 765: 0.00043698365334421396\n",
      "Minibatch train prediction at step 765: [3 1]\n",
      "Ground truth at step 765: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 766: [[ 8.11899662 -1.94749689 -2.09528351 -1.49258518]\n",
      " [ 7.87550116 -1.99025154 -2.50720525 -1.92007637]]\n",
      "Minibatch train loss at step 766: [ 0.00014602  0.00013863]\n",
      "Minibatch train loss mean at step 766: 0.00014232576359063387\n",
      "Minibatch train prediction at step 766: [0 0]\n",
      "Ground truth at step 766: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 767: [[ 7.87713575 -1.99081063 -2.50748587 -1.92070806]\n",
      " [-0.3453263   8.03933907 -1.30932307 -1.64798009]]\n",
      "Minibatch train loss at step 767: [ 0.00013827  0.00037735]\n",
      "Minibatch train loss mean at step 767: 0.0002578092971816659\n",
      "Minibatch train prediction at step 767: [0 1]\n",
      "Ground truth at step 767: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 768: [[-3.00066304 -3.50745392  6.98877764 -1.22611606]\n",
      " [-0.22771135  7.99929333 -1.41423059 -1.73184419]]\n",
      "Minibatch train loss at step 768: [ 0.0003441   0.00040833]\n",
      "Minibatch train loss mean at step 768: 0.000376212818082422\n",
      "Minibatch train prediction at step 768: [2 1]\n",
      "Ground truth at step 768: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 769: [[-1.17654598 -3.52701211 -0.73162442  7.62085199]\n",
      " [-3.00127649 -3.50813556  6.99109459 -1.22796488]]\n",
      "Minibatch train loss at step 769: [ 0.0004013   0.00034267]\n",
      "Minibatch train loss mean at step 769: 0.0003719825763255358\n",
      "Minibatch train prediction at step 769: [3 2]\n",
      "Ground truth at step 769: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 770: [[ 8.10364532 -0.4658826  -2.22135568 -1.54401147]\n",
      " [-2.97606492 -2.23730159  7.54611778 -2.25235581]]\n",
      "Minibatch train loss at step 770: [ 0.00028713  0.00013887]\n",
      "Minibatch train loss mean at step 770: 0.0002130015636794269\n",
      "Minibatch train prediction at step 770: [0 2]\n",
      "Ground truth at step 770: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 771: [[-0.56813651  8.08521748 -1.16556501 -1.67914152]\n",
      " [-2.86225557 -3.51280451  7.21469021 -1.71738815]]\n",
      "Minibatch train loss at step 771: [ 0.00032801  0.00019608]\n",
      "Minibatch train loss mean at step 771: 0.0002620450977701694\n",
      "Minibatch train prediction at step 771: [1 2]\n",
      "Ground truth at step 771: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 772: [[ 8.12987137 -1.94912994 -2.09684825 -1.49946404]\n",
      " [ 8.10601234 -0.46704319 -2.22190213 -1.54435647]]\n",
      "Minibatch train loss at step 772: [ 0.00014399  0.00028618]\n",
      "Minibatch train loss mean at step 772: 0.00021508750796783715\n",
      "Minibatch train prediction at step 772: [0 0]\n",
      "Ground truth at step 772: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 773: [[-0.56933236  8.0886631  -1.16610301 -1.68028963]\n",
      " [ 8.10731411 -0.46776232 -2.22214174 -1.54453921]]\n",
      "Minibatch train loss at step 773: [ 0.00032646  0.00028558]\n",
      "Minibatch train loss mean at step 773: 0.0003060228191316128\n",
      "Minibatch train prediction at step 773: [1 0]\n",
      "Ground truth at step 773: [1 0]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 774: [[ 7.9154563  -0.17551316 -2.38288307 -1.51146758]\n",
      " [-1.17975318 -3.52678776 -0.73192608  7.62806368]]\n",
      "Minibatch train loss at step 774: [ 0.00042048  0.00039784]\n",
      "Minibatch train loss mean at step 774: 0.0004091617010999471\n",
      "Minibatch train prediction at step 774: [0 3]\n",
      "Ground truth at step 774: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 775: [[-2.97879863 -2.24132133  7.55597448 -2.25675344]\n",
      " [-1.18045747 -3.5266397  -0.73247665  7.630229  ]]\n",
      "Minibatch train loss at step 775: [ 0.00013696  0.00039677]\n",
      "Minibatch train loss mean at step 775: 0.0002668655361048877\n",
      "Minibatch train prediction at step 775: [2 3]\n",
      "Ground truth at step 775: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 776: [[ 7.8914094  -1.99579048 -2.51023412 -1.92629302]\n",
      " [-0.57067895  8.093606   -1.16682804 -1.68232298]]\n",
      "Minibatch train loss at step 776: [ 0.00013565  0.00032444]\n",
      "Minibatch train loss mean at step 776: 0.00023004300601314753\n",
      "Minibatch train prediction at step 776: [0 1]\n",
      "Ground truth at step 776: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 777: [[-0.57118142  8.09537029 -1.16708803 -1.6830442 ]\n",
      " [ 7.89327908 -1.99645507 -2.5105598  -1.92700315]]\n",
      "Minibatch train loss at step 777: [ 0.00032372  0.00013529]\n",
      "Minibatch train loss mean at step 777: 0.00022950669517740607\n",
      "Minibatch train prediction at step 777: [1 0]\n",
      "Ground truth at step 777: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 778: [[ 8.00153065 -0.56587619 -2.3122623  -1.49919653]\n",
      " [-2.86771202 -3.51583672  7.2277689  -1.7239219 ]]\n",
      "Minibatch train loss at step 778: [ 0.0002981   0.00019227]\n",
      "Minibatch train loss mean at step 778: 0.0002451820473652333\n",
      "Minibatch train prediction at step 778: [0 2]\n",
      "Ground truth at step 778: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 779: [[-0.56986851 -1.3313669  -0.83006573  7.87660551]\n",
      " [-0.57229882  8.09922409 -1.16770709 -1.6845448 ]]\n",
      "Minibatch train loss at step 779: [ 0.0004803   0.00032229]\n",
      "Minibatch train loss mean at step 779: 0.0004012940335087478\n",
      "Minibatch train prediction at step 779: [3 1]\n",
      "Ground truth at step 779: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 780: [[-0.35139486  8.05904007 -1.31381822 -1.65624511]\n",
      " [ 8.00447559 -0.56741136 -2.31277275 -1.49979734]]\n",
      "Minibatch train loss at step 780: [ 0.00036781  0.00029679]\n",
      "Minibatch train loss mean at step 780: 0.0003322996781207621\n",
      "Minibatch train prediction at step 780: [1 0]\n",
      "Ground truth at step 780: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 781: [[ 7.53909111 -2.861444   -2.14349151 -1.72896743]\n",
      " [ 8.11887074 -0.47446558 -2.22366285 -1.54657924]]\n",
      "Minibatch train loss at step 781: [ 0.00018714  0.00028094]\n",
      "Minibatch train loss mean at step 781: 0.00023403894738294184\n",
      "Minibatch train prediction at step 781: [0 0]\n",
      "Ground truth at step 781: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 782: [[ 8.14478493 -1.95154142 -2.09980845 -1.50810719]\n",
      " [-1.18504906 -3.52544999 -0.73735052  7.64642572]]\n",
      "Minibatch train loss at step 782: [ 0.00014101  0.00038855]\n",
      "Minibatch train loss mean at step 782: 0.00026478071231395006\n",
      "Minibatch train prediction at step 782: [0 3]\n",
      "Ground truth at step 782: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 783: [[ 8.1218605  -0.47603804 -2.22411275 -1.54723048]\n",
      " [ 8.00925255 -0.56978327 -2.31357241 -1.50089836]]\n",
      "Minibatch train loss at step 783: [ 0.00027975  0.000295  ]\n",
      "Minibatch train loss mean at step 783: 0.00028737226966768503\n",
      "Minibatch train prediction at step 783: [0 0]\n",
      "Ground truth at step 783: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 784: [[ 7.93324757 -0.18482429 -2.38471293 -1.51500976]\n",
      " [-2.98340583 -2.24742675  7.57087851 -2.26131392]]\n",
      "Minibatch train loss at step 784: [ 0.00040988  0.00013422]\n",
      "Minibatch train loss mean at step 784: 0.00027204869547858834\n",
      "Minibatch train prediction at step 784: [0 2]\n",
      "Ground truth at step 784: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 785: [[ 7.54515219 -2.86306882 -2.14516401 -1.73014677]\n",
      " [-0.57287306 -1.33344948 -0.83311152  7.88757467]]\n",
      "Minibatch train loss at step 785: [ 0.00018571  0.00047363]\n",
      "Minibatch train loss mean at step 785: 0.0003296681970823556\n",
      "Minibatch train prediction at step 785: [0 3]\n",
      "Ground truth at step 785: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 786: [[ 8.12764645 -0.48008585 -2.22429061 -1.54835379]\n",
      " [ 7.93808079 -0.18805113 -2.38467813 -1.51602101]]\n",
      "Minibatch train loss at step 786: [ 0.00027736  0.0004069 ]\n",
      "Minibatch train loss mean at step 786: 0.00034212961327284575\n",
      "Minibatch train prediction at step 786: [0 0]\n",
      "Ground truth at step 786: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 787: [[ 8.01808834 -0.57571399 -2.31373668 -1.50283146]\n",
      " [-1.18771732 -3.52501607 -0.74098736  7.65770102]]\n",
      "Minibatch train loss at step 787: [ 0.00029107  0.00038307]\n",
      "Minibatch train loss mean at step 787: 0.0003370660124346614\n",
      "Minibatch train prediction at step 787: [0 3]\n",
      "Ground truth at step 787: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 788: [[ 8.02071857 -0.577672   -2.31361866 -1.50340068]\n",
      " [ 7.91314888 -2.00357962 -2.51390958 -1.93483579]]\n",
      "Minibatch train loss at step 788: [ 0.00028999  0.00013172]\n",
      "Minibatch train loss mean at step 788: 0.000210855869227089\n",
      "Minibatch train prediction at step 788: [0 0]\n",
      "Ground truth at step 788: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 789: [[-0.57535434  8.11480713 -1.16991603 -1.69199145]\n",
      " [-0.35178676  8.06915283 -1.31640542 -1.66256285]]\n",
      "Minibatch train loss at step 789: [ 0.00031609  0.0003634 ]\n",
      "Minibatch train loss mean at step 789: 0.0003397480759304017\n",
      "Minibatch train prediction at step 789: [1 1]\n",
      "Ground truth at step 789: [1 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 790: [[-2.8754282  -3.52068806  7.24162674 -1.72628772]\n",
      " [-3.03000951 -2.97718692  7.5992589  -2.55057764]]\n",
      "Minibatch train loss at step 790: [  1.88928869e-04   8.88069771e-05]\n",
      "Minibatch train loss mean at step 790: 0.0001388679229421541\n",
      "Minibatch train prediction at step 790: [2 2]\n",
      "Ground truth at step 790: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 791: [[ 8.13901997 -0.48889598 -2.22404003 -1.55054724]\n",
      " [-3.03043628 -2.97731924  7.60027504 -2.55112529]]\n",
      "Minibatch train loss at step 791: [  2.72475299e-04   8.85685804e-05]\n",
      "Minibatch train loss mean at step 791: 0.00018052193627227098\n",
      "Minibatch train prediction at step 791: [0 2]\n",
      "Ground truth at step 791: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 792: [[ 8.03025723 -0.58414418 -2.3135252  -1.50564146]\n",
      " [-0.57539099 -1.33688211 -0.83705485  7.90194559]]\n",
      "Minibatch train loss at step 792: [ 0.00028582  0.0004654 ]\n",
      "Minibatch train loss mean at step 792: 0.0003756134829018265\n",
      "Minibatch train prediction at step 792: [0 3]\n",
      "Ground truth at step 792: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 793: [[-0.57599241 -1.33746433 -0.83784866  7.90443134]\n",
      " [-2.98598862 -2.25336266  7.58456659 -2.26193428]]\n",
      "Minibatch train loss at step 793: [ 0.00046397  0.00013196]\n",
      "Minibatch train loss mean at step 793: 0.0002979650453198701\n",
      "Minibatch train prediction at step 793: [3 2]\n",
      "Ground truth at step 793: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 794: [[-0.35157415  8.07417107 -1.31773448 -1.66596711]\n",
      " [ 7.92312717 -2.00737023 -2.51540899 -1.93891776]]\n",
      "Minibatch train loss at step 794: [ 0.00036138  0.00013005]\n",
      "Minibatch train loss mean at step 794: 0.00024571307585574687\n",
      "Minibatch train prediction at step 794: [1 0]\n",
      "Ground truth at step 794: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 795: [[-0.2320749   8.02399254 -1.4203968  -1.74591327]\n",
      " [-0.57663256  8.12284851 -1.17100561 -1.69616234]]\n",
      "Minibatch train loss at step 795: [ 0.00039582  0.00031299]\n",
      "Minibatch train loss mean at step 795: 0.0003544051432982087\n",
      "Minibatch train prediction at step 795: [1 1]\n",
      "Ground truth at step 795: [1 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 796: [[ 8.14828014 -0.49505109 -2.22475696 -1.55221665]\n",
      " [ 8.16247368 -1.95454967 -2.10440779 -1.51717734]]\n",
      "Minibatch train loss at step 796: [ 0.00026878  0.0001378 ]\n",
      "Minibatch train loss mean at step 796: 0.00020328864047769457\n",
      "Minibatch train prediction at step 796: [0 0]\n",
      "Ground truth at step 796: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 797: [[-3.01994586 -3.52451444  7.03094244 -1.24286377]\n",
      " [-1.19207585 -3.52450514 -0.74664813  7.67696619]]\n",
      "Minibatch train loss at step 797: [ 0.0003242   0.00037389]\n",
      "Minibatch train loss mean at step 797: 0.0003490431699901819\n",
      "Minibatch train prediction at step 797: [2 3]\n",
      "Ground truth at step 797: [2 3]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 798: [[-2.88029861 -3.52388787  7.25032282 -1.72721016]\n",
      " [-0.23437206  8.0282383  -1.42076755 -1.74654603]]\n",
      "Minibatch train loss at step 798: [ 0.00018702  0.00039355]\n",
      "Minibatch train loss mean at step 798: 0.0002902867563534528\n",
      "Minibatch train prediction at step 798: [2 1]\n",
      "Ground truth at step 798: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 799: [[-3.03400826 -2.97881627  7.60987902 -2.55661845]\n",
      " [-2.88103414 -3.52430415  7.25198126 -1.72784638]]\n",
      "Minibatch train loss at step 799: [  8.73765894e-05   1.86545134e-04]\n",
      "Minibatch train loss mean at step 799: 0.00013696086534764618\n",
      "Minibatch train prediction at step 799: [2 2]\n",
      "Ground truth at step 799: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 800: [[ 7.56678963 -2.86885929 -2.15122128 -1.7343303 ]\n",
      " [-3.02214265 -3.52630568  7.03597116 -1.24505293]]\n",
      "Minibatch train loss at step 800: [ 0.00018082  0.00032193]\n",
      "Minibatch train loss mean at step 800: 0.00025137828197330236\n",
      "Minibatch train prediction at step 800: [0 2]\n",
      "Ground truth at step 800: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 801: [[-2.99031854 -2.25798249  7.59684944 -2.26577711]\n",
      " [-3.02297544 -3.52696776  7.03847122 -1.24678051]]\n",
      "Minibatch train loss at step 801: [ 0.00012981  0.00032074]\n",
      "Minibatch train loss mean at step 801: 0.00022527560940943658\n",
      "Minibatch train prediction at step 801: [2 2]\n",
      "Ground truth at step 801: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 802: [[-3.03577662 -2.97971845  7.61565208 -2.56032896]\n",
      " [-3.02384233 -3.52772117  7.04155302 -1.2492348 ]]\n",
      "Minibatch train loss at step 802: [  8.67805938e-05   3.18953185e-04]\n",
      "Minibatch train loss mean at step 802: 0.00020286688231863081\n",
      "Minibatch train prediction at step 802: [2 2]\n",
      "Ground truth at step 802: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 803: [[-2.99167085 -2.25974321  7.60136843 -2.26833534]\n",
      " [-0.35650486  8.08764172 -1.32076406 -1.6706934 ]]\n",
      "Minibatch train loss at step 803: [ 0.00012898  0.00035494]\n",
      "Minibatch train loss mean at step 803: 0.0002419591910438612\n",
      "Minibatch train prediction at step 803: [2 1]\n",
      "Ground truth at step 803: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 804: [[-0.24117525  8.03718948 -1.42076409 -1.7455864 ]\n",
      " [-1.19575036 -3.52446532 -0.74602395  7.68621397]]\n",
      "Minibatch train loss at step 804: [ 0.00038831  0.00037008]\n",
      "Minibatch train loss mean at step 804: 0.0003791924100369215\n",
      "Minibatch train prediction at step 804: [1 3]\n",
      "Ground truth at step 804: [1 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 805: [[ 7.93470049 -2.01184154 -2.51769447 -1.9443804 ]\n",
      " [ 8.15814686 -0.49889535 -2.22768235 -1.55370498]]\n",
      "Minibatch train loss at step 805: [ 0.00012802  0.00026532]\n",
      "Minibatch train loss mean at step 805: 0.00019667361630126834\n",
      "Minibatch train prediction at step 805: [0 0]\n",
      "Ground truth at step 805: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 806: [[-0.58402914  8.14140606 -1.17363524 -1.70137942]\n",
      " [ 8.0498209  -0.59348911 -2.31776881 -1.50890815]]\n",
      "Minibatch train loss at step 806: [ 0.00030549  0.00027831]\n",
      "Minibatch train loss mean at step 806: 0.00029190085479058325\n",
      "Minibatch train prediction at step 806: [1 0]\n",
      "Ground truth at step 806: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 807: [[ 7.97476482 -0.20795494 -2.38870597 -1.52290761]\n",
      " [ 8.05094719 -0.59384382 -2.31811953 -1.5091157 ]]\n",
      "Minibatch train loss at step 807: [ 0.00038593  0.00027784]\n",
      "Minibatch train loss mean at step 807: 0.0003318817471154034\n",
      "Minibatch train prediction at step 807: [0 0]\n",
      "Ground truth at step 807: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 808: [[-0.36042497  8.09570599 -1.32218695 -1.67307842]\n",
      " [-0.58592224 -1.34348297 -0.84678668  7.93384314]]\n",
      "Minibatch train loss at step 808: [ 0.00035101  0.00044658]\n",
      "Minibatch train loss mean at step 808: 0.000398793607018888\n",
      "Minibatch train prediction at step 808: [1 3]\n",
      "Ground truth at step 808: [1 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 809: [[ 7.97809315 -0.20949717 -2.38915682 -1.5235616 ]\n",
      " [-3.03985286 -2.98194242  7.63000679 -2.56976986]]\n",
      "Minibatch train loss at step 809: [  3.84256913e-04   8.49926146e-05]\n",
      "Minibatch train loss mean at step 809: 0.0002346247638342902\n",
      "Minibatch train prediction at step 809: [0 2]\n",
      "Ground truth at step 809: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 810: [[-0.58653992  8.14822292 -1.1744231  -1.70361376]\n",
      " [ 8.05607796 -0.59659529 -2.31878948 -1.51012874]]\n",
      "Minibatch train loss at step 810: [ 0.00030298  0.00027593]\n",
      "Minibatch train loss mean at step 810: 0.00028945779195055366\n",
      "Minibatch train prediction at step 810: [1 0]\n",
      "Ground truth at step 810: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 811: [[-0.58710772  8.15005493 -1.17464793 -1.70430541]\n",
      " [-1.19901323 -3.52429199 -0.74578851  7.69510651]]\n",
      "Minibatch train loss at step 811: [ 0.00030215  0.00036638]\n",
      "Minibatch train loss mean at step 811: 0.00033426604932174087\n",
      "Minibatch train prediction at step 811: [1 3]\n",
      "Ground truth at step 811: [1 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 812: [[-3.03123212 -3.53407454  7.06586123 -1.26697314]\n",
      " [-3.04142928 -2.98271728  7.6351037  -2.57288694]]\n",
      "Minibatch train loss at step 812: [  3.06559290e-04   8.43966191e-05]\n",
      "Minibatch train loss mean at step 812: 0.00019547795818652958\n",
      "Minibatch train prediction at step 812: [2 2]\n",
      "Ground truth at step 812: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 813: [[-3.03195667 -3.53464365  7.06800413 -1.26838434]\n",
      " [-2.99892521 -2.26685882  7.62025833 -2.27801347]]\n",
      "Minibatch train loss at step 813: [ 0.00030549  0.00012552]\n",
      "Minibatch train loss mean at step 813: 0.00021550312521867454\n",
      "Minibatch train prediction at step 813: [2 2]\n",
      "Ground truth at step 813: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 814: [[-0.25067067  8.05210972 -1.42126799 -1.74640203]\n",
      " [-1.20043099 -3.52411914 -0.74617589  7.69984865]]\n",
      "Minibatch train loss at step 814: [ 0.00038021  0.00036436]\n",
      "Minibatch train loss mean at step 814: 0.0003722808905877173\n",
      "Minibatch train prediction at step 814: [1 3]\n",
      "Ground truth at step 814: [1 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 815: [[-0.58940655 -1.34601939 -0.8501907   7.94595337]\n",
      " [ 7.94372082 -2.01577783 -2.51937652 -1.94898593]]\n",
      "Minibatch train loss at step 815: [ 0.00043979  0.00012647]\n",
      "Minibatch train loss mean at step 815: 0.0002831293095368892\n",
      "Minibatch train prediction at step 815: [3 0]\n",
      "Ground truth at step 815: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 816: [[-3.04387808 -2.98409271  7.64353752 -2.57831454]\n",
      " [ 7.98954201 -0.21482064 -2.39085579 -1.52566779]]\n",
      "Minibatch train loss at step 816: [  8.34430248e-05   3.78060387e-04]\n",
      "Minibatch train loss mean at step 816: 0.00023075169883668423\n",
      "Minibatch train prediction at step 816: [2 0]\n",
      "Ground truth at step 816: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 817: [[-0.25373259  8.05660534 -1.42140234 -1.74629343]\n",
      " [ 8.18256092 -1.95858812 -2.10711145 -1.52876365]]\n",
      "Minibatch train loss at step 817: [ 0.0003777   0.00013398]\n",
      "Minibatch train loss mean at step 817: 0.00025584257673472166\n",
      "Minibatch train prediction at step 817: [1 0]\n",
      "Ground truth at step 817: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 818: [[-0.25508401  8.05833054 -1.42130506 -1.74604011]\n",
      " [-2.89617729 -3.53159666  7.29308939 -1.75201809]]\n",
      "Minibatch train loss at step 818: [ 0.00037675  0.00017546]\n",
      "Minibatch train loss mean at step 818: 0.00027610512915998697\n",
      "Minibatch train prediction at step 818: [1 2]\n",
      "Ground truth at step 818: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 819: [[ 8.06906605 -0.60299379 -2.32126164 -1.51221395]\n",
      " [-1.2030189  -3.52363515 -0.74722117  7.70902634]]\n",
      "Minibatch train loss at step 819: [ 0.00027105  0.00036054]\n",
      "Minibatch train loss mean at step 819: 0.00031579413916915655\n",
      "Minibatch train prediction at step 819: [0 3]\n",
      "Ground truth at step 819: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 820: [[-3.04629493 -2.98547959  7.65148449 -2.58324099]\n",
      " [-3.03724003 -3.53887796  7.08312988 -1.27777684]]\n",
      "Minibatch train loss at step 820: [  8.24894232e-05   2.98455532e-04]\n",
      "Minibatch train loss mean at step 820: 0.00019047247769776732\n",
      "Minibatch train prediction at step 820: [2 2]\n",
      "Ground truth at step 820: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 821: [[ 8.1869688  -1.95931792 -2.10791588 -1.53123569]\n",
      " [-3.03805876 -3.53945565  7.0854907  -1.27928936]]\n",
      "Minibatch train loss at step 821: [ 0.00013327  0.00029726]\n",
      "Minibatch train loss mean at step 821: 0.00021526544878724962\n",
      "Minibatch train prediction at step 821: [0 2]\n",
      "Ground truth at step 821: [0 2]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 822: [[ 7.59085751 -2.87469983 -2.15693855 -1.74020827]\n",
      " [ 8.17942333 -0.50865942 -2.23222303 -1.55738568]]\n",
      "Minibatch train loss at step 822: [ 0.00017546  0.0002577 ]\n",
      "Minibatch train loss mean at step 822: 0.00021657897741533816\n",
      "Minibatch train prediction at step 822: [0 0]\n",
      "Ground truth at step 822: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 823: [[ 7.95051956 -2.01895022 -2.52049923 -1.95252824]\n",
      " [-0.59494573  8.17050457 -1.17718327 -1.71029365]]\n",
      "Minibatch train loss at step 823: [ 0.00012528  0.00029428]\n",
      "Minibatch train loss mean at step 823: 0.00020978276734240353\n",
      "Minibatch train prediction at step 823: [0 1]\n",
      "Ground truth at step 823: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 824: [[-0.26366526  8.06849575 -1.420398   -1.74390304]\n",
      " [-0.59553373  8.17205238 -1.17740405 -1.71071243]]\n",
      "Minibatch train loss at step 824: [ 0.00037103  0.00029369]\n",
      "Minibatch train loss mean at step 824: 0.00033235910814255476\n",
      "Minibatch train prediction at step 824: [1 1]\n",
      "Ground truth at step 824: [1 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 825: [[ 8.18256664 -0.50977129 -2.2331419  -1.5578891 ]\n",
      " [ 8.07612419 -0.6059494  -2.32302213 -1.51310158]]\n",
      "Minibatch train loss at step 825: [ 0.00025662  0.00026854]\n",
      "Minibatch train loss mean at step 825: 0.00026258357684127986\n",
      "Minibatch train prediction at step 825: [0 0]\n",
      "Ground truth at step 825: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 826: [[-3.00545883 -2.2741313   7.64239216 -2.28726149]\n",
      " [ 7.59642315 -2.87600708 -2.15824866 -1.74176776]]\n",
      "Minibatch train loss at step 826: [ 0.00012182  0.00017427]\n",
      "Minibatch train loss mean at step 826: 0.00014804663078393787\n",
      "Minibatch train prediction at step 826: [2 0]\n",
      "Ground truth at step 826: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 827: [[ 8.00314999 -0.21936609 -2.39400196 -1.52736127]\n",
      " [-3.05037451 -2.98775434  7.66537046 -2.59223127]]\n",
      "Minibatch train loss at step 827: [  3.71625501e-04   8.08206314e-05]\n",
      "Minibatch train loss mean at step 827: 0.00022622307005804032\n",
      "Minibatch train prediction at step 827: [0 2]\n",
      "Ground truth at step 827: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 828: [[-0.59596926 -1.34994709 -0.85546064  7.96579838]\n",
      " [-3.00640202 -2.27555513  7.64585304 -2.28836441]]\n",
      "Minibatch train loss at step 828: [ 0.0004287   0.00012123]\n",
      "Minibatch train loss mean at step 828: 0.0002749662089627236\n",
      "Minibatch train prediction at step 828: [3 2]\n",
      "Ground truth at step 828: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 829: [[-1.20723045 -3.52323651 -0.74771613  7.72297287]\n",
      " [ 7.60172558 -2.87737942 -2.15956402 -1.74327111]]\n",
      "Minibatch train loss at step 829: [ 0.00035482  0.0001732 ]\n",
      "Minibatch train loss mean at step 829: 0.0002640096063259989\n",
      "Minibatch train prediction at step 829: [3 0]\n",
      "Ground truth at step 829: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 830: [[ 8.08354092 -0.60953945 -2.32421088 -1.51430583]\n",
      " [-1.2076019  -3.5232501  -0.74809939  7.72484159]]\n",
      "Minibatch train loss at step 830: [ 0.00026568  0.00035411]\n",
      "Minibatch train loss mean at step 830: 0.0003098951419815421\n",
      "Minibatch train prediction at step 830: [0 3]\n",
      "Ground truth at step 830: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 831: [[ 7.60617018 -2.87837768 -2.1608386  -1.74420619]\n",
      " [ 8.19064808 -0.51393902 -2.23444843 -1.55937898]]\n",
      "Minibatch train loss at step 831: [ 0.00017224  0.00025376]\n",
      "Minibatch train loss mean at step 831: 0.00021300348453223705\n",
      "Minibatch train prediction at step 831: [0 0]\n",
      "Ground truth at step 831: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 832: [[-3.04519773 -3.54517102  7.10659981 -1.29236603]\n",
      " [-3.00851727 -2.27921033  7.65350437 -2.29037905]]\n",
      "Minibatch train loss at step 832: [ 0.00028773  0.00012004]\n",
      "Minibatch train loss mean at step 832: 0.00020388318807817996\n",
      "Minibatch train prediction at step 832: [2 2]\n",
      "Ground truth at step 832: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 833: [[-0.59991324  8.18499374 -1.17878771 -1.71501493]\n",
      " [-3.00914764 -2.28036189  7.65577793 -2.291116  ]]\n",
      "Minibatch train loss at step 833: [ 0.00028904  0.00011968]\n",
      "Minibatch train loss mean at step 833: 0.00020435986516531557\n",
      "Minibatch train prediction at step 833: [1 2]\n",
      "Ground truth at step 833: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 834: [[ 7.6136384  -2.88008475 -2.16289186 -1.74594474]\n",
      " [ 8.01436806 -0.22549087 -2.39501333 -1.5295707 ]]\n",
      "Minibatch train loss at step 834: [ 0.00017057  0.00036567]\n",
      "Minibatch train loss mean at step 834: 0.0002681206096895039\n",
      "Minibatch train prediction at step 834: [0 0]\n",
      "Ground truth at step 834: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 835: [[ 8.09136677 -0.61426061 -2.32485008 -1.51586163]\n",
      " [-0.27256307  8.08199978 -1.42013597 -1.74384773]]\n",
      "Minibatch train loss at step 835: [ 0.00026282  0.000364  ]\n",
      "Minibatch train loss mean at step 835: 0.00031341041903942823\n",
      "Minibatch train prediction at step 835: [0 1]\n",
      "Ground truth at step 835: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 836: [[ 8.20522499 -1.96202278 -2.11149383 -1.54118991]\n",
      " [-0.27324244  8.08313084 -1.42005658 -1.74394083]]\n",
      "Minibatch train loss at step 836: [ 0.00012993  0.0003634 ]\n",
      "Minibatch train loss mean at step 836: 0.0002466663718223572\n",
      "Minibatch train prediction at step 836: [0 1]\n",
      "Ground truth at step 836: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 837: [[-0.37365735  8.13428688 -1.32934785 -1.68726218]\n",
      " [ 8.09475994 -0.61606342 -2.32514453 -1.51652193]]\n",
      "Minibatch train loss at step 837: [ 0.00033361  0.00026151]\n",
      "Minibatch train loss mean at step 837: 0.0002975610550493002\n",
      "Minibatch train prediction at step 837: [1 0]\n",
      "Ground truth at step 837: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 838: [[-0.2757383   8.08645058 -1.41972101 -1.74370205]\n",
      " [ 8.09646893 -0.6168431  -2.32533598 -1.5169127 ]]\n",
      "Minibatch train loss at step 838: [ 0.00036162  0.00026103]\n",
      "Minibatch train loss mean at step 838: 0.00031132492586039007\n",
      "Minibatch train prediction at step 838: [1 0]\n",
      "Ground truth at step 838: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 839: [[ 8.2023325  -0.52055216 -2.23567748 -1.56199634]\n",
      " [-0.37513009  8.13757324 -1.32959461 -1.68838704]]\n",
      "Minibatch train loss at step 839: [ 0.00024959  0.00033218]\n",
      "Minibatch train loss mean at step 839: 0.0002908871101681143\n",
      "Minibatch train prediction at step 839: [0 1]\n",
      "Ground truth at step 839: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 840: [[-3.049402   -3.54942846  7.11814642 -1.29793322]\n",
      " [ 7.96451998 -2.0252018  -2.52326751 -1.96018732]]\n",
      "Minibatch train loss at step 840: [ 0.00028284  0.00012278]\n",
      "Minibatch train loss mean at step 840: 0.00020281082834117115\n",
      "Minibatch train prediction at step 840: [2 0]\n",
      "Ground truth at step 840: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 841: [[ 8.21174908 -1.96319556 -2.11289382 -1.54467762]\n",
      " [-0.28049257  8.09261036 -1.41909885 -1.74323535]]\n",
      "Minibatch train loss at step 841: [ 0.00012886  0.00035852]\n",
      "Minibatch train loss mean at step 841: 0.00024368710000999272\n",
      "Minibatch train prediction at step 841: [0 1]\n",
      "Ground truth at step 841: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 842: [[-3.05706477 -2.99093413  7.6847024  -2.60374236]\n",
      " [ 7.6311841  -2.88457012 -2.16753435 -1.7507242 ]]\n",
      "Minibatch train loss at step 842: [  7.86750388e-05   1.66879079e-04]\n",
      "Minibatch train loss mean at step 842: 0.0001227770553668961\n",
      "Minibatch train prediction at step 842: [2 0]\n",
      "Ground truth at step 842: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 843: [[-0.60649729  8.20120811 -1.17970145 -1.72015023]\n",
      " [-3.0574789  -2.99117208  7.68604136 -2.60454154]]\n",
      "Minibatch train loss at step 843: [  2.82962807e-04   7.86750388e-05]\n",
      "Minibatch train loss mean at step 843: 0.00018081892631016672\n",
      "Minibatch train prediction at step 843: [1 2]\n",
      "Ground truth at step 843: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 844: [[ 8.10591221 -0.62022001 -2.32694507 -1.51910841]\n",
      " [ 7.63544655 -2.88565278 -2.16865635 -1.75201917]]\n",
      "Minibatch train loss at step 844: [ 0.0002577   0.00016593]\n",
      "Minibatch train loss mean at step 844: 0.0002118114207405597\n",
      "Minibatch train prediction at step 844: [0 0]\n",
      "Ground truth at step 844: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 845: [[ 7.97032928 -2.02733517 -2.52452087 -1.96268225]\n",
      " [-0.38095471  8.14886093 -1.3306489  -1.69160974]]\n",
      "Minibatch train loss at step 845: [ 0.00012171  0.00032718]\n",
      "Minibatch train loss mean at step 845: 0.0002244406205136329\n",
      "Minibatch train prediction at step 845: [0 1]\n",
      "Ground truth at step 845: [0 1]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 846: [[ 8.03199577 -0.23121805 -2.39790916 -1.5338558 ]\n",
      " [-3.05163884 -3.5515902   7.1264019  -1.30330908]]\n",
      "Minibatch train loss at step 846: [ 0.00035744  0.00027927]\n",
      "Minibatch train loss mean at step 846: 0.0003183565568178892\n",
      "Minibatch train prediction at step 846: [0 2]\n",
      "Ground truth at step 846: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 847: [[-3.05929351 -2.99230146  7.69212627 -2.6081543 ]\n",
      " [-0.28982577  8.10472393 -1.41841769 -1.74219489]]\n",
      "Minibatch train loss at step 847: [  7.79598340e-05   3.52201430e-04]\n",
      "Minibatch train loss mean at step 847: 0.00021508062491193414\n",
      "Minibatch train prediction at step 847: [2 1]\n",
      "Ground truth at step 847: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 848: [[-3.05239558 -3.5521729   7.12965441 -1.3058188 ]\n",
      " [-0.38384917  8.15448475 -1.33127546 -1.69323325]]\n",
      "Minibatch train loss at step 848: [ 0.00027784  0.00032455]\n",
      "Minibatch train loss mean at step 848: 0.00030119623988866806\n",
      "Minibatch train prediction at step 848: [2 1]\n",
      "Ground truth at step 848: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 849: [[-3.05280924 -3.55246615  7.13173962 -1.30757856]\n",
      " [-3.06031656 -2.9929595   7.6959095  -2.61050916]]\n",
      "Minibatch train loss at step 849: [  2.76884850e-04   7.74830332e-05]\n",
      "Minibatch train loss mean at step 849: 0.00017718394519761205\n",
      "Minibatch train prediction at step 849: [2 2]\n",
      "Ground truth at step 849: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 850: [[-0.29448992  8.11094761 -1.41833353 -1.74178267]\n",
      " [-3.02138472 -2.29139853  7.68321276 -2.30392313]]\n",
      "Minibatch train loss at step 850: [ 0.00034886  0.00011503]\n",
      "Minibatch train loss mean at step 850: 0.00023194754612632096\n",
      "Minibatch train prediction at step 850: [1 2]\n",
      "Ground truth at step 850: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 851: [[ 8.21660519 -0.52412754 -2.2394762  -1.56588674]\n",
      " [-3.06153703 -2.99379039  7.70073032 -2.61362267]]\n",
      "Minibatch train loss at step 851: [  2.45064264e-04   7.70062325e-05]\n",
      "Minibatch train loss mean at step 851: 0.0001610352483112365\n",
      "Minibatch train prediction at step 851: [0 2]\n",
      "Ground truth at step 851: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 852: [[ 8.22558117 -1.96592629 -2.11488032 -1.55278313]\n",
      " [-3.02325892 -2.29247093  7.68651104 -2.30641437]]\n",
      "Minibatch train loss at step 852: [ 0.00012635  0.00011443]\n",
      "Minibatch train loss mean at step 852: 0.0001203941137646325\n",
      "Minibatch train prediction at step 852: [0 2]\n",
      "Ground truth at step 852: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 853: [[-3.0545783  -3.55383754  7.14120674 -1.31564021]\n",
      " [ 7.65254831 -2.89008784 -2.17283344 -1.75780237]]\n",
      "Minibatch train loss at step 853: [ 0.00027236  0.00016235]\n",
      "Minibatch train loss mean at step 853: 0.00021735300833825022\n",
      "Minibatch train prediction at step 853: [2 0]\n",
      "Ground truth at step 853: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 854: [[-3.0550375  -3.55422354  7.14380455 -1.31787705]\n",
      " [ 8.21934128 -0.52457726 -2.24048138 -1.56651735]]\n",
      "Minibatch train loss at step 854: [ 0.00027116  0.00024423]\n",
      "Minibatch train loss mean at step 854: 0.00025769718922674656\n",
      "Minibatch train prediction at step 854: [2 0]\n",
      "Ground truth at step 854: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 855: [[ 8.22962952 -1.96669054 -2.11521459 -1.55538583]\n",
      " [ 7.98140955 -2.03115511 -2.52714467 -1.96710789]]\n",
      "Minibatch train loss at step 855: [ 0.00012576  0.00011992]\n",
      "Minibatch train loss mean at step 855: 0.00012283763498999178\n",
      "Minibatch train prediction at step 855: [0 0]\n",
      "Ground truth at step 855: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 856: [[ 8.23132896 -1.96697772 -2.11538553 -1.55650389]\n",
      " [-1.21364188 -3.52525949 -0.74642855  7.7463994 ]]\n",
      "Minibatch train loss at step 856: [ 0.0001254  0.000346 ]\n",
      "Minibatch train loss mean at step 856: 0.0002357025077799335\n",
      "Minibatch train prediction at step 856: [0 3]\n",
      "Ground truth at step 856: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 857: [[ 7.98427391 -2.03206611 -2.52783155 -1.96811628]\n",
      " [ 7.6600337  -2.89195323 -2.1744206  -1.76051533]]\n",
      "Minibatch train loss at step 857: [ 0.00011944  0.00016068]\n",
      "Minibatch train loss mean at step 857: 0.00014006090350449085\n",
      "Minibatch train prediction at step 857: [0 0]\n",
      "Ground truth at step 857: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 858: [[-3.05699587 -3.55556846  7.15389347 -1.32644236]\n",
      " [ 7.98650408 -2.03272533 -2.5283072  -1.96880412]]\n",
      "Minibatch train loss at step 858: [ 0.00026664  0.0001192 ]\n",
      "Minibatch train loss mean at step 858: 0.00019291890203021467\n",
      "Minibatch train prediction at step 858: [2 0]\n",
      "Ground truth at step 858: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 859: [[ 7.98943138 -2.03355312 -2.5289185  -1.96962297]\n",
      " [-3.0574944  -3.55584955  7.1563797  -1.3285712 ]]\n",
      "Minibatch train loss at step 859: [ 0.00011873  0.00026544]\n",
      "Minibatch train loss mean at step 859: 0.00019208462617825717\n",
      "Minibatch train prediction at step 859: [0 2]\n",
      "Ground truth at step 859: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 860: [[ 8.12319183 -0.62530679 -2.33214951 -1.52328837]\n",
      " [-3.05801845 -3.55606961  7.15919447 -1.33111346]]\n",
      "Minibatch train loss at step 860: [ 0.0002521   0.00026413]\n",
      "Minibatch train loss mean at step 860: 0.0002581143635325134\n",
      "Minibatch train prediction at step 860: [0 2]\n",
      "Ground truth at step 860: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 861: [[ 8.225564   -0.52684319 -2.242342   -1.56813979]\n",
      " [ 7.99619436 -2.03543139 -2.53033185 -1.97143114]]\n",
      "Minibatch train loss at step 861: [ 0.00024232  0.00011765]\n",
      "Minibatch train loss mean at step 861: 0.00017998788098338991\n",
      "Minibatch train prediction at step 861: [0 0]\n",
      "Ground truth at step 861: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 862: [[ 8.24219227 -1.96851611 -2.11681175 -1.56351554]\n",
      " [-3.03034472 -2.29889822  7.7038784  -2.31820416]]\n",
      "Minibatch train loss at step 862: [ 0.00012361  0.00011157]\n",
      "Minibatch train loss mean at step 862: 0.000117593037430197\n",
      "Minibatch train prediction at step 862: [0 2]\n",
      "Ground truth at step 862: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 863: [[-0.61978924  8.23031712 -1.18272352 -1.72727835]\n",
      " [ 8.22772217 -0.52803105 -2.24273157 -1.56872952]]\n",
      "Minibatch train loss at step 863: [ 0.00027236  0.00024161]\n",
      "Minibatch train loss mean at step 863: 0.0002569821081124246\n",
      "Minibatch train prediction at step 863: [1 0]\n",
      "Ground truth at step 863: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 864: [[-0.6035971  -1.35656118 -0.86005354  7.99492931]\n",
      " [-3.06001496 -3.55674458  7.17017174 -1.34114397]]\n",
      "Minibatch train loss at step 864: [ 0.00041381  0.00025925]\n",
      "Minibatch train loss mean at step 864: 0.00033652782440185547\n",
      "Minibatch train prediction at step 864: [3 2]\n",
      "Ground truth at step 864: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 865: [[ 8.24748898 -1.9693594  -2.11743784 -1.56701219]\n",
      " [-2.9221468  -3.54341674  7.37605906 -1.80587161]]\n",
      "Minibatch train loss at step 865: [ 0.00012266  0.0001546 ]\n",
      "Minibatch train loss mean at step 865: 0.00013863066851627082\n",
      "Minibatch train prediction at step 865: [0 2]\n",
      "Ground truth at step 865: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 866: [[-3.06917453 -2.99798799  7.72942305 -2.63378739]\n",
      " [-3.03255987 -2.30185199  7.71119356 -2.32233143]]\n",
      "Minibatch train loss at step 866: [  7.39070310e-05   1.10262510e-04]\n",
      "Minibatch train loss mean at step 866: 9.208476694766432e-05\n",
      "Minibatch train prediction at step 866: [2 2]\n",
      "Ground truth at step 866: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 867: [[-0.30827034  8.13042355 -1.41877234 -1.74099445]\n",
      " [-3.06969619 -2.99826145  7.73132801 -2.63514638]]\n",
      "Minibatch train loss at step 867: [  3.39212100e-04   7.37878290e-05]\n",
      "Minibatch train loss mean at step 867: 0.0002064999716822058\n",
      "Minibatch train prediction at step 867: [1 2]\n",
      "Ground truth at step 867: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 868: [[-0.60505027 -1.35780203 -0.86148721  7.99952555]\n",
      " [ 8.2326889  -0.53043252 -2.24409437 -1.56974256]]\n",
      "Minibatch train loss at step 868: [ 0.00041131  0.00023982]\n",
      "Minibatch train loss mean at step 868: 0.00032556348014622927\n",
      "Minibatch train prediction at step 868: [3 0]\n",
      "Ground truth at step 868: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 869: [[-3.03460526 -2.30458903  7.71731186 -2.32543898]\n",
      " [ 8.05524445 -0.23682117 -2.40497708 -1.53917646]]\n",
      "Minibatch train loss at step 869: [ 0.00010931  0.00034708]\n",
      "Minibatch train loss mean at step 869: 0.00022819307923782617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train prediction at step 869: [2 0]\n",
      "Ground truth at step 869: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 870: [[ 8.13408947 -0.63050109 -2.33486032 -1.52551055]\n",
      " [-3.0633707  -3.55860949  7.18592501 -1.35365677]]\n",
      "Minibatch train loss at step 870: [ 0.00024828  0.00025257]\n",
      "Minibatch train loss mean at step 870: 0.00025042734341695905\n",
      "Minibatch train prediction at step 870: [0 2]\n",
      "Ground truth at step 870: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 871: [[-0.60660577 -1.35930693 -0.86336583  8.00485516]\n",
      " [-0.39548689  8.17958736 -1.33580184 -1.70091188]]\n",
      "Minibatch train loss at step 871: [ 0.00040845  0.00031347]\n",
      "Minibatch train loss mean at step 871: 0.00036095906398259103\n",
      "Minibatch train prediction at step 871: [3 1]\n",
      "Ground truth at step 871: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 872: [[-3.07241845 -2.99974656  7.74112034 -2.64184833]\n",
      " [-3.06454206 -3.55927253  7.19098282 -1.35745025]]\n",
      "Minibatch train loss at step 872: [  7.27150255e-05   2.50427343e-04]\n",
      "Minibatch train loss mean at step 872: 0.00016157118079718202\n",
      "Minibatch train prediction at step 872: [2 2]\n",
      "Ground truth at step 872: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 873: [[-3.03758502 -2.30868363  7.72614002 -2.32958317]\n",
      " [-1.21921122 -3.52689528 -0.73684108  7.75182295]]\n",
      "Minibatch train loss at step 873: [ 0.00010788  0.00034541]\n",
      "Minibatch train loss mean at step 873: 0.00022664372227154672\n",
      "Minibatch train prediction at step 873: [2 3]\n",
      "Ground truth at step 873: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 874: [[ 8.0622282  -0.24052465 -2.40640044 -1.54033899]\n",
      " [-2.92797947 -3.54522848  7.39692974 -1.82013738]]\n",
      "Minibatch train loss at step 874: [ 0.00034374  0.00014983]\n",
      "Minibatch train loss mean at step 874: 0.00024678767658770084\n",
      "Minibatch train prediction at step 874: [0 2]\n",
      "Ground truth at step 874: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 875: [[ 8.06388569 -0.24158676 -2.40657711 -1.54066217]\n",
      " [-3.03937626 -2.3109107   7.73088884 -2.33193374]]\n",
      "Minibatch train loss at step 875: [ 0.00034291  0.00010728]\n",
      "Minibatch train loss mean at step 875: 0.00022509446716867387\n",
      "Minibatch train prediction at step 875: [0 2]\n",
      "Ground truth at step 875: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 876: [[-3.06726718 -3.56090927  7.20152617 -1.36468792]\n",
      " [-2.92954803 -3.54578924  7.40169382 -1.82294762]]\n",
      "Minibatch train loss at step 876: [ 0.00024626  0.00014876]\n",
      "Minibatch train loss mean at step 876: 0.0001975090999621898\n",
      "Minibatch train prediction at step 876: [2 2]\n",
      "Ground truth at step 876: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 877: [[-0.60985929 -1.36276507 -0.86774629  8.01740265]\n",
      " [ 8.06790161 -0.24443258 -2.40669751 -1.54151118]]\n",
      "Minibatch train loss at step 877: [ 0.00040189  0.00034076]\n",
      "Minibatch train loss mean at step 877: 0.0003713271289598197\n",
      "Minibatch train prediction at step 877: [3 0]\n",
      "Ground truth at step 877: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 878: [[-3.04209018 -2.31479955  7.73877239 -2.3355217 ]\n",
      " [-3.07603669 -3.00145531  7.7535305  -2.65025544]]\n",
      "Minibatch train loss at step 878: [  1.05971441e-04   7.14038179e-05]\n",
      "Minibatch train loss mean at step 878: 8.868762961355969e-05\n",
      "Minibatch train prediction at step 878: [2 2]\n",
      "Ground truth at step 878: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 879: [[-1.22147596 -3.52728939 -0.73434514  7.75745249]\n",
      " [ 8.24682903 -0.54021859 -2.24636555 -1.5718168 ]]\n",
      "Minibatch train loss at step 879: [ 0.00034374  0.00023493]\n",
      "Minibatch train loss mean at step 879: 0.0002893372147809714\n",
      "Minibatch train prediction at step 879: [3 0]\n",
      "Ground truth at step 879: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 880: [[-0.62216389  8.24302673 -1.1847806  -1.73281991]\n",
      " [-0.30945194  8.13696098 -1.42065382 -1.74374092]]\n",
      "Minibatch train loss at step 880: [ 0.00026818  0.00033647]\n",
      "Minibatch train loss mean at step 880: 0.00030232808785513043\n",
      "Minibatch train prediction at step 880: [1 1]\n",
      "Ground truth at step 880: [1 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 881: [[-3.07110572 -3.56306767  7.21485043 -1.37309206]\n",
      " [-0.39493582  8.18447971 -1.33771586 -1.7048347 ]]\n",
      "Minibatch train loss at step 881: [ 0.00024125  0.0003118 ]\n",
      "Minibatch train loss mean at step 881: 0.00027652669814415276\n",
      "Minibatch train prediction at step 881: [2 1]\n",
      "Ground truth at step 881: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 882: [[ 8.07781219 -0.25131544 -2.40722322 -1.54338527]\n",
      " [-2.93459845 -3.54752326  7.41671515 -1.83153772]]\n",
      "Minibatch train loss at step 882: [ 0.00033552  0.00014554]\n",
      "Minibatch train loss mean at step 882: 0.00024053090601228178\n",
      "Minibatch train prediction at step 882: [0 2]\n",
      "Ground truth at step 882: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 883: [[ 8.02829647 -2.04622173 -2.53680611 -1.98263395]\n",
      " [ 8.27018261 -1.97311103 -2.1199398  -1.58147371]]\n",
      "Minibatch train loss at step 883: [ 0.00011277  0.00011908]\n",
      "Minibatch train loss mean at step 883: 0.00011592431110329926\n",
      "Minibatch train prediction at step 883: [0 0]\n",
      "Ground truth at step 883: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 884: [[-2.93616533 -3.54796457  7.42150545 -1.83427656]\n",
      " [ 8.27144718 -1.97333527 -2.12012362 -1.58224154]]\n",
      "Minibatch train loss at step 884: [ 0.00014459  0.00011873]\n",
      "Minibatch train loss mean at step 884: 0.00013165791460778564\n",
      "Minibatch train prediction at step 884: [2 0]\n",
      "Ground truth at step 884: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 885: [[-0.61308903 -1.36725414 -0.87288803  8.03342724]\n",
      " [-0.31063554  8.14087486 -1.42156589 -1.74514818]]\n",
      "Minibatch train loss at step 885: [ 0.00039391  0.0003348 ]\n",
      "Minibatch train loss mean at step 885: 0.0003643559757620096\n",
      "Minibatch train prediction at step 885: [3 1]\n",
      "Ground truth at step 885: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 886: [[-0.61382067 -1.36781061 -0.87368602  8.03564739]\n",
      " [-0.62409377  8.24964333 -1.18566036 -1.73533571]]\n",
      "Minibatch train loss at step 886: [ 0.00039272  0.00026604]\n",
      "Minibatch train loss mean at step 886: 0.0003293786139693111\n",
      "Minibatch train prediction at step 886: [3 1]\n",
      "Ground truth at step 886: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 887: [[-0.61491448 -1.36851752 -0.87486082  8.03857136]\n",
      " [-3.04998755 -2.32376218  7.75863552 -2.34492612]]\n",
      "Minibatch train loss at step 887: [ 0.00039117  0.00010287]\n",
      "Minibatch train loss mean at step 887: 0.00024702036171220243\n",
      "Minibatch train prediction at step 887: [3 2]\n",
      "Ground truth at step 887: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 888: [[ 8.0326128  -2.04804635 -2.53749657 -1.98462737]\n",
      " [-0.39718494  8.191679   -1.33962214 -1.70776057]]\n",
      "Minibatch train loss at step 888: [ 0.00011205  0.00030894]\n",
      "Minibatch train loss mean at step 888: 0.00021049659699201584\n",
      "Minibatch train prediction at step 888: [0 1]\n",
      "Ground truth at step 888: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 889: [[ 8.15934372 -0.64662993 -2.33969998 -1.52918828]\n",
      " [-3.05193543 -2.32556891  7.76265192 -2.34696198]]\n",
      "Minibatch train loss at step 889: [ 0.00023934  0.00010228]\n",
      "Minibatch train loss mean at step 889: 0.0001708099734969437\n",
      "Minibatch train prediction at step 889: [0 2]\n",
      "Ground truth at step 889: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 890: [[ 7.69990873 -2.90171123 -2.18231606 -1.77331555]\n",
      " [-3.07721376 -3.56684756  7.23519087 -1.3844043 ]]\n",
      "Minibatch train loss at step 890: [ 0.00015281  0.00023398]\n",
      "Minibatch train loss mean at step 890: 0.0001933975436259061\n",
      "Minibatch train prediction at step 890: [0 2]\n",
      "Ground truth at step 890: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 891: [[-3.07782197 -3.56728673  7.2373848  -1.38566828]\n",
      " [ 8.28105164 -1.97472572 -2.12178493 -1.58814943]]\n",
      "Minibatch train loss at step 891: [ 0.00023327  0.0001173 ]\n",
      "Minibatch train loss mean at step 891: 0.0001752802199916914\n",
      "Minibatch train prediction at step 891: [2 0]\n",
      "Ground truth at step 891: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 892: [[ 8.28265381 -1.97493756 -2.12202263 -1.58918047]\n",
      " [-0.31524143  8.14906025 -1.42321849 -1.74549937]]\n",
      "Minibatch train loss at step 892: [ 0.00011694  0.00033087]\n",
      "Minibatch train loss mean at step 892: 0.00022390385856851935\n",
      "Minibatch train prediction at step 892: [0 1]\n",
      "Ground truth at step 892: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 893: [[ 8.2623167  -0.54854673 -2.2505939  -1.57383144]\n",
      " [-3.08345246 -3.00473118  7.77732944 -2.66596127]]\n",
      "Minibatch train loss at step 893: [  2.29809099e-04   6.90197994e-05]\n",
      "Minibatch train loss mean at step 893: 0.00014941445260774344\n",
      "Minibatch train prediction at step 893: [0 2]\n",
      "Ground truth at step 893: [0 2]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 894: [[ 8.03921318 -2.0503819  -2.53872991 -1.98728228]\n",
      " [-3.0795145  -3.56838751  7.24445581 -1.39026415]]\n",
      "Minibatch train loss at step 894: [ 0.00011122  0.00023076]\n",
      "Minibatch train loss mean at step 894: 0.0001709893113002181\n",
      "Minibatch train prediction at step 894: [0 2]\n",
      "Ground truth at step 894: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 895: [[-3.08438325 -3.00521088  7.78055    -2.66816401]\n",
      " [-0.62911052  8.2622366  -1.1877377  -1.73845756]]\n",
      "Minibatch train loss at step 895: [  6.87813954e-05   2.61630164e-04]\n",
      "Minibatch train loss mean at step 895: 0.00016520578355994076\n",
      "Minibatch train prediction at step 895: [2 1]\n",
      "Ground truth at step 895: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 896: [[-0.62968796  8.26368237 -1.18800735 -1.73879683]\n",
      " [-0.40174264  8.20176697 -1.3421011  -1.71047676]]\n",
      "Minibatch train loss at step 896: [ 0.00026115  0.00030465]\n",
      "Minibatch train loss mean at step 896: 0.0002829029690474272\n",
      "Minibatch train prediction at step 896: [1 1]\n",
      "Ground truth at step 896: [1 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 897: [[-3.08098984 -3.56921315  7.25143242 -1.39516091]\n",
      " [-2.94479251 -3.55154705  7.44902039 -1.84909952]]\n",
      "Minibatch train loss at step 897: [ 0.00022826  0.00013887]\n",
      "Minibatch train loss mean at step 897: 0.00018356446526013315\n",
      "Minibatch train prediction at step 897: [2 2]\n",
      "Ground truth at step 897: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 898: [[ 8.04452324 -2.05208635 -2.53982759 -1.98912561]\n",
      " [-3.0859046  -3.00614882  7.78618526 -2.67189622]]\n",
      "Minibatch train loss at step 898: [  1.10381712e-04   6.83045873e-05]\n",
      "Minibatch train loss mean at step 898: 8.934314973885193e-05\n",
      "Minibatch train prediction at step 898: [0 2]\n",
      "Ground truth at step 898: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 899: [[-0.63211203  8.2693224  -1.18910241 -1.74001265]\n",
      " [-3.08646894 -3.00651574  7.78832197 -2.67330122]]\n",
      "Minibatch train loss at step 899: [  2.59127410e-04   6.80661906e-05]\n",
      "Minibatch train loss mean at step 899: 0.00016359679284505546\n",
      "Minibatch train prediction at step 899: [1 2]\n",
      "Ground truth at step 899: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 900: [[-1.22775185 -3.52896047 -0.72644478  7.77432108]\n",
      " [ 8.26726723 -0.54871082 -2.25363874 -1.57449055]]\n",
      "Minibatch train loss at step 900: [ 0.00033874  0.00022838]\n",
      "Minibatch train loss mean at step 900: 0.00028355716494843364\n",
      "Minibatch train prediction at step 900: [3 0]\n",
      "Ground truth at step 900: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 901: [[-3.08770466 -3.00737453  7.79284143 -2.67613912]\n",
      " [-1.22796047 -3.52893686 -0.72638196  7.77545261]]\n",
      "Minibatch train loss at step 901: [  6.75893825e-05   3.38377926e-04]\n",
      "Minibatch train loss mean at step 901: 0.00020298364688642323\n",
      "Minibatch train prediction at step 901: [2 3]\n",
      "Ground truth at step 901: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 902: [[ 8.29686546 -1.97708404 -2.1241889  -1.59816408]\n",
      " [ 8.05049229 -2.05401134 -2.54093289 -1.99110413]]\n",
      "Minibatch train loss at step 902: [ 0.00011479  0.00010955]\n",
      "Minibatch train loss mean at step 902: 0.00011216964048799127\n",
      "Minibatch train prediction at step 902: [0 0]\n",
      "Ground truth at step 902: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 903: [[-0.63472706  8.27589703 -1.1904428  -1.74162662]\n",
      " [-3.08903956 -3.00830936  7.79727125 -2.67880344]]\n",
      "Minibatch train loss at step 903: [  2.56863015e-04   6.72317765e-05]\n",
      "Minibatch train loss mean at step 903: 0.00016204739222303033\n",
      "Minibatch train prediction at step 903: [1 2]\n",
      "Ground truth at step 903: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 904: [[-3.06454253 -2.33476424  7.7876277  -2.36228204]\n",
      " [-3.0847528  -3.57136774  7.26641321 -1.40433455]]\n",
      "Minibatch train loss at step 904: [  9.85812221e-05   2.23134892e-04]\n",
      "Minibatch train loss mean at step 904: 0.0001608580641914159\n",
      "Minibatch train prediction at step 904: [2 2]\n",
      "Ground truth at step 904: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 905: [[-0.32549745  8.1659317  -1.42664814 -1.74595344]\n",
      " [-3.08529449 -3.57172966  7.26848841 -1.40552759]]\n",
      "Minibatch train loss at step 905: [ 0.000323    0.00022242]\n",
      "Minibatch train loss mean at step 905: 0.0002727123792283237\n",
      "Minibatch train prediction at step 905: [1 2]\n",
      "Ground truth at step 905: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 906: [[-0.40910426  8.21703434 -1.34606469 -1.7138989 ]\n",
      " [-3.08583403 -3.57202077  7.27090549 -1.4071393 ]]\n",
      "Minibatch train loss at step 906: [ 0.00029834  0.00022159]\n",
      "Minibatch train loss mean at step 906: 0.0002599609433673322\n",
      "Minibatch train prediction at step 906: [1 2]\n",
      "Ground truth at step 906: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 907: [[-3.067137   -2.33636737  7.7925024  -2.3655715 ]\n",
      " [-0.4101229   8.21885014 -1.34645212 -1.71420538]]\n",
      "Minibatch train loss at step 907: [  9.78660391e-05   2.97382969e-04]\n",
      "Minibatch train loss mean at step 907: 0.00019762449664995074\n",
      "Minibatch train prediction at step 907: [2 1]\n",
      "Ground truth at step 907: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 908: [[ 8.3042326  -1.978073   -2.12563777 -1.60247922]\n",
      " [-3.09226823 -3.01063943  7.80897903 -2.68611217]]\n",
      "Minibatch train loss at step 908: [  1.13599999e-04   6.60397636e-05]\n",
      "Minibatch train loss mean at step 908: 8.981987775769085e-05\n",
      "Minibatch train prediction at step 908: [0 2]\n",
      "Ground truth at step 908: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 909: [[ 7.71847296 -2.90597963 -2.18713045 -1.77858233]\n",
      " [-3.06966138 -2.33763051  7.79602194 -2.36859751]]\n",
      "Minibatch train loss at step 909: [  1.49358078e-04   9.72700509e-05]\n",
      "Minibatch train loss mean at step 909: 0.00012331406469456851\n",
      "Minibatch train prediction at step 909: [0 2]\n",
      "Ground truth at step 909: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 910: [[-3.0710125  -2.33854246  7.79803276 -2.37012839]\n",
      " [-0.33110735  8.17404652 -1.42793131 -1.74576235]]\n",
      "Minibatch train loss at step 910: [  9.70316541e-05   3.19191517e-04]\n",
      "Minibatch train loss mean at step 910: 0.0002081115817418322\n",
      "Minibatch train prediction at step 910: [2 1]\n",
      "Ground truth at step 910: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 911: [[ 8.06122684 -2.05761743 -2.54308081 -1.99495852]\n",
      " [-3.09411383 -3.01206374  7.81582785 -2.69032741]]\n",
      "Minibatch train loss at step 911: [  1.07997781e-04   6.55629628e-05]\n",
      "Minibatch train loss mean at step 911: 8.678037556819618e-05\n",
      "Minibatch train prediction at step 911: [0 2]\n",
      "Ground truth at step 911: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 912: [[-0.63113618 -1.37640619 -0.88927513  8.07664204]\n",
      " [ 8.17338371 -0.64682901 -2.34986877 -1.53057992]]\n",
      "Minibatch train loss at step 912: [ 0.00037139  0.00023565]\n",
      "Minibatch train loss mean at step 912: 0.00030351808527484536\n",
      "Minibatch train prediction at step 912: [3 0]\n",
      "Ground truth at step 912: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 913: [[-2.95388198 -3.55506611  7.48188925 -1.86821389]\n",
      " [ 8.27278042 -0.54604715 -2.25887442 -1.57570326]]\n",
      "Minibatch train loss at step 913: [ 0.00013231  0.00022743]\n",
      "Minibatch train loss mean at step 913: 0.00017986950115300715\n",
      "Minibatch train prediction at step 913: [2 0]\n",
      "Ground truth at step 913: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 914: [[ 8.10275269 -0.25131658 -2.421942   -1.54639745]\n",
      " [-0.41725132  8.23083878 -1.34846604 -1.71634555]]\n",
      "Minibatch train loss at step 914: [ 0.0003267   0.00029238]\n",
      "Minibatch train loss mean at step 914: 0.0003095384454354644\n",
      "Minibatch train prediction at step 914: [0 1]\n",
      "Ground truth at step 914: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 915: [[-3.0772624  -2.34342074  7.80812311 -2.37650728]\n",
      " [ 8.10381413 -0.25153518 -2.42230344 -1.54663074]]\n",
      "Minibatch train loss at step 915: [  9.54820789e-05   3.26222595e-04]\n",
      "Minibatch train loss mean at step 915: 0.0002108523331116885\n",
      "Minibatch train prediction at step 915: [2 0]\n",
      "Ground truth at step 915: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 916: [[-0.64471608  8.29743958 -1.19390678 -1.74631965]\n",
      " [-3.09039474 -3.57544494  7.29239464 -1.42097831]]\n",
      "Minibatch train loss at step 916: [ 0.00024971  0.00021443]\n",
      "Minibatch train loss mean at step 916: 0.0002320734056411311\n",
      "Minibatch train prediction at step 916: [1 2]\n",
      "Ground truth at step 916: [1 2]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 917: [[ 8.31482029 -1.97979712 -2.12750769 -1.60872126]\n",
      " [-0.41902024  8.23485279 -1.34908652 -1.71765506]]\n",
      "Minibatch train loss at step 917: [ 0.00011205  0.00029071]\n",
      "Minibatch train loss mean at step 917: 0.000201379822101444\n",
      "Minibatch train prediction at step 917: [0 1]\n",
      "Ground truth at step 917: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 918: [[ 8.27765274 -0.54778469 -2.26034546 -1.5767746 ]\n",
      " [-0.41982192  8.23651981 -1.34934568 -1.71812153]]\n",
      "Minibatch train loss at step 918: [ 0.000226    0.00029023]\n",
      "Minibatch train loss mean at step 918: 0.00025811389787122607\n",
      "Minibatch train prediction at step 918: [0 1]\n",
      "Ground truth at step 918: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 919: [[ 8.18032455 -0.64912426 -2.35223174 -1.53192472]\n",
      " [-0.42075199  8.23839188 -1.34960771 -1.71862495]]\n",
      "Minibatch train loss at step 919: [ 0.00023362  0.00028952]\n",
      "Minibatch train loss mean at step 919: 0.0002615701814647764\n",
      "Minibatch train prediction at step 919: [0 1]\n",
      "Ground truth at step 919: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 920: [[-3.09868765 -3.01506519  7.83189249 -2.70009613]\n",
      " [ 8.31860065 -1.98059762 -2.12817574 -1.61096931]]\n",
      "Minibatch train loss at step 920: [  6.40133367e-05   1.11454472e-04]\n",
      "Minibatch train loss mean at step 920: 8.773390436545014e-05\n",
      "Minibatch train prediction at step 920: [2 0]\n",
      "Ground truth at step 920: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 921: [[ 8.18282318 -0.64997131 -2.35286498 -1.53258288]\n",
      " [ 8.07164764 -2.06129313 -2.54512    -1.99892175]]\n",
      "Minibatch train loss at step 921: [ 0.00023279  0.00010657]\n",
      "Minibatch train loss mean at step 921: 0.00016967803821898997\n",
      "Minibatch train prediction at step 921: [0 0]\n",
      "Ground truth at step 921: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 922: [[-3.09260201 -3.57696176  7.30269098 -1.42735672]\n",
      " [ 8.11321735 -0.25428417 -2.42454624 -1.54912031]]\n",
      "Minibatch train loss at step 922: [ 0.0002111   0.00032241]\n",
      "Minibatch train loss mean at step 922: 0.00026675325352698565\n",
      "Minibatch train prediction at step 922: [2 0]\n",
      "Ground truth at step 922: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 923: [[ 8.32296467 -1.98151267 -2.12900996 -1.61357987]\n",
      " [-3.09295344 -3.57707047  7.30443573 -1.42858589]]\n",
      "Minibatch train loss at step 923: [ 0.00011074  0.0002105 ]\n",
      "Minibatch train loss mean at step 923: 0.00016062037320807576\n",
      "Minibatch train prediction at step 923: [0 2]\n",
      "Ground truth at step 923: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 924: [[-0.65015572  8.31075478 -1.19542027 -1.75034142]\n",
      " [-2.95893216 -3.5568862   7.50012875 -1.8786155 ]]\n",
      "Minibatch train loss at step 924: [ 0.00024542  0.00012898]\n",
      "Minibatch train loss mean at step 924: 0.00018719896615948528\n",
      "Minibatch train prediction at step 924: [1 2]\n",
      "Ground truth at step 924: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 925: [[-0.6506933   8.31229305 -1.19564426 -1.75084424]\n",
      " [-3.08806705 -2.34991956  7.82430792 -2.38738012]]\n",
      "Minibatch train loss at step 925: [  2.44945084e-04   9.30981187e-05]\n",
      "Minibatch train loss mean at step 925: 0.00016902160132303834\n",
      "Minibatch train prediction at step 925: [1 2]\n",
      "Ground truth at step 925: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 926: [[-3.10134339 -3.01645947  7.84102154 -2.70588136]\n",
      " [ 8.19010639 -0.65345359 -2.35393476 -1.53445983]]\n",
      "Minibatch train loss at step 926: [  6.31789298e-05   2.30524194e-04]\n",
      "Minibatch train loss mean at step 926: 0.00014685155474580824\n",
      "Minibatch train prediction at step 926: [2 0]\n",
      "Ground truth at step 926: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 927: [[-3.08995676 -2.3513577   7.8277874  -2.38931537]\n",
      " [-2.960361   -3.55706644  7.50596428 -1.88237083]]\n",
      "Minibatch train loss at step 927: [  9.26213252e-05   1.27784195e-04]\n",
      "Minibatch train loss mean at step 927: 0.00011020275996997952\n",
      "Minibatch train prediction at step 927: [2 2]\n",
      "Ground truth at step 927: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 928: [[-3.09492588 -3.57776403  7.31496143 -1.43611181]\n",
      " [ 8.28979874 -0.55296171 -2.2628777  -1.58013833]]\n",
      "Minibatch train loss at step 928: [ 0.00020705  0.0002223 ]\n",
      "Minibatch train loss mean at step 928: 0.00021467285114340484\n",
      "Minibatch train prediction at step 928: [2 0]\n",
      "Ground truth at step 928: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 929: [[ 8.12412357 -0.25890854 -2.42613292 -1.55215776]\n",
      " [-1.2326827  -3.53082252 -0.71912479  7.79354668]]\n",
      "Minibatch train loss at step 929: [ 0.00031764  0.00033313]\n",
      "Minibatch train loss mean at step 929: 0.00032538839150220156\n",
      "Minibatch train prediction at step 929: [0 3]\n",
      "Ground truth at step 929: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 930: [[ 8.08127213 -2.0647707  -2.54701447 -2.00259733]\n",
      " [ 8.29254913 -0.55467921 -2.2631731  -1.58085108]]\n",
      "Minibatch train loss at step 930: [ 0.00010514  0.00022123]\n",
      "Minibatch train loss mean at step 930: 0.00016318251437041909\n",
      "Minibatch train prediction at step 930: [0 0]\n",
      "Ground truth at step 930: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 931: [[-3.09354949 -2.35494828  7.83570862 -2.39286876]\n",
      " [ 8.08284283 -2.06533456 -2.54728508 -2.00312185]]\n",
      "Minibatch train loss at step 931: [  9.15485434e-05   1.04898674e-04]\n",
      "Minibatch train loss mean at step 931: 9.82236088020727e-05\n",
      "Minibatch train prediction at step 931: [2 0]\n",
      "Ground truth at step 931: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 932: [[-3.09444094 -2.35598183  7.83771658 -2.39364314]\n",
      " [-3.10424113 -3.01777387  7.85045052 -2.71187806]]\n",
      "Minibatch train loss at step 932: [  9.13101467e-05   6.23445158e-05]\n",
      "Minibatch train loss mean at step 932: 7.682733121328056e-05\n",
      "Minibatch train prediction at step 932: [2 2]\n",
      "Ground truth at step 932: [2 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 933: [[-0.6533041   8.32205391 -1.19691908 -1.75494075]\n",
      " [ 8.33606148 -1.98423481 -2.13114929 -1.62156069]]\n",
      "Minibatch train loss at step 933: [ 0.00024197  0.00010871]\n",
      "Minibatch train loss mean at step 933: 0.00017533927166368812\n",
      "Minibatch train prediction at step 933: [1 0]\n",
      "Ground truth at step 933: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 934: [[-0.42711928  8.2567215  -1.35267425 -1.72591007]\n",
      " [-3.10519361 -3.01827908  7.85347128 -2.7137332 ]]\n",
      "Minibatch train loss at step 934: [  2.82605295e-04   6.21061117e-05]\n",
      "Minibatch train loss mean at step 934: 0.0001723557070363313\n",
      "Minibatch train prediction at step 934: [1 2]\n",
      "Ground truth at step 934: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 935: [[-0.65400153  8.32445335 -1.19723761 -1.75593233]\n",
      " [-2.96465421 -3.55803823  7.52048635 -1.89080369]]\n",
      "Minibatch train loss at step 935: [ 0.00024113  0.00012516]\n",
      "Minibatch train loss mean at step 935: 0.00018314662156626582\n",
      "Minibatch train prediction at step 935: [1 2]\n",
      "Ground truth at step 935: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 936: [[-0.63483953 -1.38136721 -0.89392203  8.09544659]\n",
      " [-3.10622573 -3.01894689  7.85690355 -2.71574974]]\n",
      "Minibatch train loss at step 936: [  3.62926425e-04   6.18677077e-05]\n",
      "Minibatch train loss mean at step 936: 0.00021239706256892532\n",
      "Minibatch train prediction at step 936: [3 2]\n",
      "Ground truth at step 936: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 937: [[ 8.09299183 -2.06862712 -2.5490818  -2.00625157]\n",
      " [ 8.30114651 -0.55984932 -2.26429582 -1.58315694]]\n",
      "Minibatch train loss at step 937: [ 0.00010359  0.00021861]\n",
      "Minibatch train loss mean at step 937: 0.00016109673015307635\n",
      "Minibatch train prediction at step 937: [0 0]\n",
      "Ground truth at step 937: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 938: [[ 8.20646381 -0.66240001 -2.35593271 -1.53843486]\n",
      " [-3.10051775 -2.36187816  7.84918594 -2.39876485]]\n",
      "Minibatch train loss at step 938: [  2.25160999e-04   8.97605642e-05]\n",
      "Minibatch train loss mean at step 938: 0.00015746077406220138\n",
      "Minibatch train prediction at step 938: [0 2]\n",
      "Ground truth at step 938: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 939: [[-2.96667719 -3.55889153  7.52664089 -1.89366102]\n",
      " [-0.65572113  8.32989311 -1.1981672  -1.75784397]]\n",
      "Minibatch train loss at step 939: [ 0.00012409  0.00023958]\n",
      "Minibatch train loss mean at step 939: 0.00018183558131568134\n",
      "Minibatch train prediction at step 939: [2 1]\n",
      "Ground truth at step 939: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 940: [[ 8.09835911 -2.07031775 -2.54999065 -2.00786638]\n",
      " [ 8.34391689 -1.98583996 -2.13281155 -1.62602103]]\n",
      "Minibatch train loss at step 940: [ 0.00010287  0.00010752]\n",
      "Minibatch train loss mean at step 940: 0.00010519666830077767\n",
      "Minibatch train prediction at step 940: [0 0]\n",
      "Ground truth at step 940: [0 0]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 941: [[-0.4296276   8.26444244 -1.35461628 -1.72890186]\n",
      " [ 8.10069752 -2.07101059 -2.55040121 -2.0085113 ]]\n",
      "Minibatch train loss at step 941: [ 0.00027963  0.00010263]\n",
      "Minibatch train loss mean at step 941: 0.0001911299186758697\n",
      "Minibatch train prediction at step 941: [1 0]\n",
      "Ground truth at step 941: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 942: [[-0.63670993 -1.38296902 -0.89629489  8.1016798 ]\n",
      " [ 8.34645176 -1.98634565 -2.13338161 -1.62749517]]\n",
      "Minibatch train loss at step 942: [ 0.00035995  0.00010728]\n",
      "Minibatch train loss mean at step 942: 0.0002336149336770177\n",
      "Minibatch train prediction at step 942: [3 0]\n",
      "Ground truth at step 942: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 943: [[ 8.3073988  -0.56254578 -2.26594925 -1.58470201]\n",
      " [ 8.14379692 -0.26810014 -2.42914701 -1.55740845]]\n",
      "Minibatch train loss at step 943: [ 0.0002167   0.00030894]\n",
      "Minibatch train loss mean at step 943: 0.0002628208603709936\n",
      "Minibatch train prediction at step 943: [0 0]\n",
      "Ground truth at step 943: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 944: [[-0.34507167  8.20820045 -1.43523979 -1.75649261]\n",
      " [-1.23463464 -3.5317204  -0.71684819  7.80303764]]\n",
      "Minibatch train loss at step 944: [ 0.00030477  0.00033016]\n",
      "Minibatch train loss mean at step 944: 0.0003174634766764939\n",
      "Minibatch train prediction at step 944: [1 3]\n",
      "Ground truth at step 944: [1 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 945: [[ 8.11109543 -2.07416916 -2.55214262 -2.01135206]\n",
      " [-1.23471475 -3.53169131 -0.71721053  7.80449867]]\n",
      "Minibatch train loss at step 945: [ 0.0001012   0.00032968]\n",
      "Minibatch train loss mean at step 945: 0.00021544106130022556\n",
      "Minibatch train prediction at step 945: [0 3]\n",
      "Ground truth at step 945: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 946: [[-0.65873873  8.33902359 -1.19959521 -1.76093853]\n",
      " [-0.43164691  8.27003288 -1.35581601 -1.73083377]]\n",
      "Minibatch train loss at step 946: [ 0.00023684  0.00027772]\n",
      "Minibatch train loss mean at step 946: 0.0002572799567133188\n",
      "Minibatch train prediction at step 946: [1 1]\n",
      "Ground truth at step 946: [1 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 947: [[ 8.11641788 -2.07592058 -2.55295277 -2.01288915]\n",
      " [-0.3466638   8.21144295 -1.43581128 -1.75712299]]\n",
      "Minibatch train loss at step 947: [ 0.00010061  0.00030334]\n",
      "Minibatch train loss mean at step 947: 0.00020197460253257304\n",
      "Minibatch train prediction at step 947: [0 1]\n",
      "Ground truth at step 947: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 948: [[ 8.11953449 -2.07690239 -2.5534513  -2.01373196]\n",
      " [ 8.35474777 -1.98770571 -2.13586402 -1.63190782]]\n",
      "Minibatch train loss at step 948: [ 0.00010013  0.00010597]\n",
      "Minibatch train loss mean at step 948: 0.00010305111936759204\n",
      "Minibatch train prediction at step 948: [0 0]\n",
      "Ground truth at step 948: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 949: [[ 8.31366348 -0.56509906 -2.26762629 -1.58638668]\n",
      " [-1.23482513 -3.53117537 -0.72012061  7.81245422]]\n",
      "Minibatch train loss at step 949: [ 0.00021491  0.00032646]\n",
      "Minibatch train loss mean at step 949: 0.000270686112344265\n",
      "Minibatch train prediction at step 949: [0 3]\n",
      "Ground truth at step 949: [0 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 950: [[-3.10445356 -3.58377528  7.34519911 -1.44938385]\n",
      " [-0.43490759  8.27647781 -1.35672438 -1.73232186]]\n",
      "Minibatch train loss at step 950: [ 0.00019834  0.00027534]\n",
      "Minibatch train loss mean at step 950: 0.0002368400600971654\n",
      "Minibatch train prediction at step 950: [2 1]\n",
      "Ground truth at step 950: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 951: [[ 7.75368547 -2.91661882 -2.19688773 -1.78770673]\n",
      " [-0.6625883   8.34749031 -1.20063329 -1.76303196]]\n",
      "Minibatch train loss at step 951: [ 0.00014268  0.0002341 ]\n",
      "Minibatch train loss mean at step 951: 0.00018839148106053472\n",
      "Minibatch train prediction at step 951: [0 1]\n",
      "Ground truth at step 951: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 952: [[-2.97247028 -3.56181097  7.53979492 -1.89654601]\n",
      " [ 8.36041927 -1.98851383 -2.13780665 -1.63474715]]\n",
      "Minibatch train loss at step 952: [ 0.00012206  0.00010526]\n",
      "Minibatch train loss mean at step 952: 0.00011365956743247807\n",
      "Minibatch train prediction at step 952: [2 0]\n",
      "Ground truth at step 952: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 953: [[-3.1057415  -3.58466005  7.34813547 -1.44980562]\n",
      " [-0.3523373   8.22042274 -1.43693113 -1.75759768]]\n",
      "Minibatch train loss at step 953: [ 0.00019775  0.00029941]\n",
      "Minibatch train loss mean at step 953: 0.00024857878452166915\n",
      "Minibatch train prediction at step 953: [2 1]\n",
      "Ground truth at step 953: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 954: [[-0.66512161  8.35295105 -1.20142734 -1.76422846]\n",
      " [-3.106179   -3.58486915  7.34965563 -1.45053971]]\n",
      "Minibatch train loss at step 954: [ 0.00023255  0.00019727]\n",
      "Minibatch train loss mean at step 954: 0.00021491109509952366\n",
      "Minibatch train prediction at step 954: [1 2]\n",
      "Ground truth at step 954: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 955: [[ 8.31814957 -0.565395   -2.26973128 -1.58780968]\n",
      " [-3.1131587  -3.02271271  7.87656546 -2.72719312]]\n",
      "Minibatch train loss at step 955: [  2.13719410e-04   6.01988795e-05]\n",
      "Minibatch train loss mean at step 955: 0.00013695914822164923\n",
      "Minibatch train prediction at step 955: [0 2]\n",
      "Ground truth at step 955: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 956: [[-1.23534453 -3.53046942 -0.72380656  7.82493114]\n",
      " [-0.44052318  8.28702736 -1.35843861 -1.73428893]]\n",
      "Minibatch train loss at step 956: [ 0.00032169  0.00027116]\n",
      "Minibatch train loss mean at step 956: 0.00029642926529049873\n",
      "Minibatch train prediction at step 956: [3 1]\n",
      "Ground truth at step 956: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 957: [[ 8.36776257 -1.98951447 -2.13981128 -1.63880944]\n",
      " [-3.11397576 -3.02315307  7.8790803  -2.72872043]]\n",
      "Minibatch train loss at step 957: [  1.04064296e-04   5.99604755e-05]\n",
      "Minibatch train loss mean at step 957: 8.201238233596087e-05\n",
      "Minibatch train prediction at step 957: [0 2]\n",
      "Ground truth at step 957: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 958: [[-3.10799456 -3.58566165  7.35636282 -1.4540602 ]\n",
      " [ 8.14274693 -2.08452868 -2.55737758 -2.02059031]]\n",
      "Minibatch train loss at step 958: [  1.95364933e-04   9.71508489e-05]\n",
      "Minibatch train loss mean at step 958: 0.0001462578948121518\n",
      "Minibatch train prediction at step 958: [2 0]\n",
      "Ground truth at step 958: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 959: [[-3.11492443 -3.02374673  7.88207197 -2.73052406]\n",
      " [-0.66938674  8.36215115 -1.20293117 -1.76612687]]\n",
      "Minibatch train loss at step 959: [  5.96028694e-05   2.29570738e-04]\n",
      "Minibatch train loss mean at step 959: 0.00014458681107498705\n",
      "Minibatch train prediction at step 959: [2 1]\n",
      "Ground truth at step 959: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 960: [[ 7.76380062 -2.91879535 -2.20023775 -1.78964281]\n",
      " [ 8.32117176 -0.56509101 -2.27158689 -1.5887152 ]]\n",
      "Minibatch train loss at step 960: [ 0.0001409   0.00021312]\n",
      "Minibatch train loss mean at step 960: 0.00017700946773402393\n",
      "Minibatch train prediction at step 960: [0 0]\n",
      "Ground truth at step 960: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 961: [[-3.11594367 -3.02443552  7.88544464 -2.73254967]\n",
      " [-0.36068204  8.23306274 -1.43887055 -1.75775135]]\n",
      "Minibatch train loss at step 961: [  5.93644654e-05   2.94046069e-04]\n",
      "Minibatch train loss mean at step 961: 0.00017670527449809015\n",
      "Minibatch train prediction at step 961: [2 1]\n",
      "Ground truth at step 961: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 962: [[ 8.37493515 -1.99046934 -2.14174533 -1.64277482]\n",
      " [-3.11649489 -3.02484417  7.88730574 -2.73363209]]\n",
      "Minibatch train loss at step 962: [  1.03110717e-04   5.93644654e-05]\n",
      "Minibatch train loss mean at step 962: 8.123759471345693e-05\n",
      "Minibatch train prediction at step 962: [0 2]\n",
      "Ground truth at step 962: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 963: [[-0.3628228   8.23617649 -1.43927181 -1.75765288]\n",
      " [-2.9776814  -3.56310844  7.55653143 -1.90478659]]\n",
      "Minibatch train loss at step 963: [ 0.00029274  0.00011932]\n",
      "Minibatch train loss mean at step 963: 0.0002060282859019935\n",
      "Minibatch train prediction at step 963: [1 2]\n",
      "Ground truth at step 963: [1 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 964: [[-2.97814441 -3.56323218  7.55809498 -1.9055928 ]\n",
      " [-3.11770034 -3.02578259  7.8914094  -2.73598504]]\n",
      "Minibatch train loss at step 964: [  1.18963791e-04   5.88876537e-05]\n",
      "Minibatch train loss mean at step 964: 8.892572077456862e-05\n",
      "Minibatch train prediction at step 964: [2 2]\n",
      "Ground truth at step 964: [2 2]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 965: [[ 8.22979164 -0.66785705 -2.3651607  -1.54360688]\n",
      " [ 8.3239851  -0.5646255  -2.27343202 -1.58946061]]\n",
      "Minibatch train loss at step 965: [ 0.00021873  0.00021241]\n",
      "Minibatch train loss mean at step 965: 0.0002155667607439682\n",
      "Minibatch train prediction at step 965: [0 0]\n",
      "Ground truth at step 965: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 966: [[-3.11911392 -2.37055826  7.87979364 -2.41383076]\n",
      " [ 8.23073006 -0.66812944 -2.36544371 -1.5437938 ]]\n",
      "Minibatch train loss at step 966: [  8.59462016e-05   2.18367568e-04]\n",
      "Minibatch train loss mean at step 966: 0.00015215689199976623\n",
      "Minibatch train prediction at step 966: [2 0]\n",
      "Ground truth at step 966: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 967: [[-1.23658872 -3.53026438 -0.72601837  7.83874559]\n",
      " [-0.44972107  8.3042469  -1.36140406 -1.73727679]]\n",
      "Minibatch train loss at step 967: [ 0.00031669  0.00026473]\n",
      "Minibatch train loss mean at step 967: 0.0002907088492065668\n",
      "Minibatch train prediction at step 967: [3 1]\n",
      "Ground truth at step 967: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 968: [[-3.12017655 -3.02751899  7.89948082 -2.74057937]\n",
      " [-0.67584157  8.37613487 -1.2051245  -1.76900482]]\n",
      "Minibatch train loss at step 968: [  5.82916437e-05   2.25399359e-04]\n",
      "Minibatch train loss mean at step 968: 0.00014184550673235208\n",
      "Minibatch train prediction at step 968: [2 1]\n",
      "Ground truth at step 968: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 969: [[ 8.23384857 -0.66910267 -2.36617398 -1.54452837]\n",
      " [ 8.16394901 -0.26664007 -2.44010687 -1.56259716]]\n",
      "Minibatch train loss at step 969: [ 0.00021753  0.00030251]\n",
      "Minibatch train loss mean at step 969: 0.00026002037338912487\n",
      "Minibatch train prediction at step 969: [0 0]\n",
      "Ground truth at step 969: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 970: [[-0.67704779  8.37912655 -1.2054857  -1.76986337]\n",
      " [-0.45167276  8.30839729 -1.36194444 -1.73836768]]\n",
      "Minibatch train loss at step 970: [ 0.00022457  0.0002633 ]\n",
      "Minibatch train loss mean at step 970: 0.00024393186322413385\n",
      "Minibatch train prediction at step 970: [1 1]\n",
      "Ground truth at step 970: [1 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 971: [[-1.23692501 -3.53020191 -0.72756863  7.84435749]\n",
      " [ 8.15716553 -2.08995962 -2.55988193 -2.02599597]]\n",
      "Minibatch train loss at step 971: [  3.14543839e-04   9.53628842e-05]\n",
      "Minibatch train loss mean at step 971: 0.0002049533650279045\n",
      "Minibatch train prediction at step 971: [3 0]\n",
      "Ground truth at step 971: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 972: [[ 8.33109283 -0.56713402 -2.27515769 -1.59165108]\n",
      " [-0.45318058  8.31171894 -1.36240065 -1.73927867]]\n",
      "Minibatch train loss at step 972: [ 0.00021038  0.00026223]\n",
      "Minibatch train loss mean at step 972: 0.00023630415671505034\n",
      "Minibatch train prediction at step 972: [0 1]\n",
      "Ground truth at step 972: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 973: [[ 8.15976048 -2.09099746 -2.56024027 -2.02698326]\n",
      " [-0.37163737  8.25033665 -1.44078064 -1.75859511]]\n",
      "Minibatch train loss at step 973: [  9.50052927e-05   2.86895607e-04]\n",
      "Minibatch train loss mean at step 973: 0.00019095044990535825\n",
      "Minibatch train prediction at step 973: [0 1]\n",
      "Ground truth at step 973: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 974: [[-2.98299432 -3.56488156  7.57067251 -1.91021705]\n",
      " [-0.3727259   8.25208187 -1.44093204 -1.75874233]]\n",
      "Minibatch train loss at step 974: [ 0.00011694  0.0002863 ]\n",
      "Minibatch train loss mean at step 974: 0.00020161860447842628\n",
      "Minibatch train prediction at step 974: [2 1]\n",
      "Ground truth at step 974: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 975: [[ 8.16343307 -2.09229779 -2.56082177 -2.02813554]\n",
      " [ 8.38973045 -1.99296391 -2.1460278  -1.65032423]]\n",
      "Minibatch train loss at step 975: [  9.46476939e-05   1.01203565e-04]\n",
      "Minibatch train loss mean at step 975: 9.792562923394144e-05\n",
      "Minibatch train prediction at step 975: [0 0]\n",
      "Ground truth at step 975: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 976: [[-0.37536961  8.25592899 -1.44115937 -1.75875056]\n",
      " [-0.64874697 -1.38576245 -0.90374422  8.12768459]]\n",
      "Minibatch train loss at step 976: [ 0.00028463  0.00034767]\n",
      "Minibatch train loss mean at step 976: 0.0003161521744914353\n",
      "Minibatch train prediction at step 976: [1 3]\n",
      "Ground truth at step 976: [1 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 977: [[ 8.24336433 -0.67202467 -2.36833143 -1.54698718]\n",
      " [ 8.16793156 -2.0937829  -2.56156278 -2.02942109]]\n",
      "Minibatch train loss at step 977: [  2.15030435e-04   9.41709077e-05]\n",
      "Minibatch train loss mean at step 977: 0.00015460066788364202\n",
      "Minibatch train prediction at step 977: [0 0]\n",
      "Ground truth at step 977: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 978: [[-3.12464595 -3.03042483  7.91263199 -2.74758339]\n",
      " [ 7.78214693 -2.92318368 -2.20654249 -1.79329038]]\n",
      "Minibatch train loss at step 978: [  5.72188219e-05   1.37677256e-04]\n",
      "Minibatch train loss mean at step 978: 9.744803537614644e-05\n",
      "Minibatch train prediction at step 978: [2 0]\n",
      "Ground truth at step 978: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 979: [[ 8.17444611 -0.26738784 -2.44394469 -1.56578445]\n",
      " [ 8.39417267 -1.99377251 -2.14741707 -1.65254688]]\n",
      "Minibatch train loss at step 979: [ 0.00029893  0.00010049]\n",
      "Minibatch train loss mean at step 979: 0.00019971030997112393\n",
      "Minibatch train prediction at step 979: [0 0]\n",
      "Ground truth at step 979: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 980: [[-1.23754239 -3.52964592 -0.7319988   7.85721397]\n",
      " [-3.12649751 -2.37309957  7.89273691 -2.41755843]]\n",
      "Minibatch train loss at step 980: [  3.09657771e-04   8.43966191e-05]\n",
      "Minibatch train loss mean at step 980: 0.0001970271987374872\n",
      "Minibatch train prediction at step 980: [3 2]\n",
      "Ground truth at step 980: [3 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 981: [[-3.12579942 -3.03113961  7.9157505  -2.74924684]\n",
      " [-0.65117294 -1.3866204  -0.90558052  8.13277149]]\n",
      "Minibatch train loss at step 981: [  5.69804179e-05   3.45289678e-04]\n",
      "Minibatch train loss mean at step 981: 0.00020113504433538765\n",
      "Minibatch train prediction at step 981: [2 3]\n",
      "Ground truth at step 981: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 982: [[-0.46193272  8.32812214 -1.36390746 -1.74261248]\n",
      " [ 8.17820835 -0.26861393 -2.44480443 -1.56676233]]\n",
      "Minibatch train loss at step 982: [ 0.00025627  0.00029738]\n",
      "Minibatch train loss mean at step 982: 0.00027682504151016474\n",
      "Minibatch train prediction at step 982: [1 0]\n",
      "Ground truth at step 982: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 983: [[ 8.25063133 -0.67468691 -2.36998248 -1.5486213 ]\n",
      " [ 7.78857756 -2.92470837 -2.20873809 -1.79444265]]\n",
      "Minibatch train loss at step 983: [ 0.000213    0.00013672]\n",
      "Minibatch train loss mean at step 983: 0.0001748640206642449\n",
      "Minibatch train prediction at step 983: [0 0]\n",
      "Ground truth at step 983: [0 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 984: [[-0.38276118  8.26737309 -1.44181085 -1.75898755]\n",
      " [ 7.79032087 -2.9251163  -2.20930624 -1.79480219]]\n",
      "Minibatch train loss at step 984: [ 0.00027998  0.00013637]\n",
      "Minibatch train loss mean at step 984: 0.000208174780709669\n",
      "Minibatch train prediction at step 984: [1 0]\n",
      "Ground truth at step 984: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 985: [[-2.98753023 -3.56721425  7.57949257 -1.91109681]\n",
      " [-0.38358435  8.26870537 -1.44185352 -1.75906181]]\n",
      "Minibatch train loss at step 985: [ 0.00011575  0.00027951]\n",
      "Minibatch train loss mean at step 985: 0.00019762612646445632\n",
      "Minibatch train prediction at step 985: [2 1]\n",
      "Ground truth at step 985: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 986: [[-0.65365511 -1.38840342 -0.90850598  8.14020729]\n",
      " [-0.46429148  8.33319092 -1.36420429 -1.74396908]]\n",
      "Minibatch train loss at step 986: [ 0.00034183  0.0002546 ]\n",
      "Minibatch train loss mean at step 986: 0.0002982162113767117\n",
      "Minibatch train prediction at step 986: [3 1]\n",
      "Ground truth at step 986: [3 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 987: [[-3.1301589  -2.37550235  7.90004396 -2.41908336]\n",
      " [-0.46526167  8.33492184 -1.3643347  -1.74424326]]\n",
      "Minibatch train loss at step 987: [  8.36814215e-05   2.53883569e-04]\n",
      "Minibatch train loss mean at step 987: 0.00016878249880392104\n",
      "Minibatch train prediction at step 987: [2 1]\n",
      "Ground truth at step 987: [2 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 988: [[-3.12862968 -3.0328548   7.92325115 -2.75316024]\n",
      " [-0.69052589  8.4078331  -1.20808756 -1.77652597]]\n",
      "Minibatch train loss at step 988: [  5.63844042e-05   2.16222266e-04]\n",
      "Minibatch train loss mean at step 988: 0.00013630333705805242\n",
      "Minibatch train prediction at step 988: [2 1]\n",
      "Ground truth at step 988: [2 1]\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train out at step 989: [[-0.38852301  8.2759037  -1.44228625 -1.75890481]\n",
      " [-1.2384491  -3.53015471 -0.73599267  7.87009192]]\n",
      "Minibatch train loss at step 989: [ 0.00027665  0.00030489]\n",
      "Minibatch train loss mean at step 989: 0.00029076868668198586\n",
      "Minibatch train prediction at step 989: [1 3]\n",
      "Ground truth at step 989: [1 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 990: [[ 8.40718079 -1.99568892 -2.151721   -1.65875471]\n",
      " [-0.69261593  8.41184616 -1.2085079  -1.77723491]]\n",
      "Minibatch train loss at step 990: [  9.89388136e-05   2.15149616e-04]\n",
      "Minibatch train loss mean at step 990: 0.00015704421093687415\n",
      "Minibatch train prediction at step 990: [0 1]\n",
      "Ground truth at step 990: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 991: [[-0.69372809  8.4140234  -1.20877147 -1.77763116]\n",
      " [ 8.18966579 -2.1016233  -2.56478953 -2.03676772]]\n",
      "Minibatch train loss at step 991: [  2.14315340e-04   9.15485434e-05]\n",
      "Minibatch train loss mean at step 991: 0.00015293194155674428\n",
      "Minibatch train prediction at step 991: [1 0]\n",
      "Ground truth at step 991: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 992: [[-3.13018656 -3.03403616  7.92767334 -2.75532675]\n",
      " [-1.23871279 -3.53029203 -0.73776078  7.87515688]]\n",
      "Minibatch train loss at step 992: [  5.60267981e-05   3.02984117e-04]\n",
      "Minibatch train loss mean at step 992: 0.0001795054558897391\n",
      "Minibatch train prediction at step 992: [2 3]\n",
      "Ground truth at step 992: [2 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 993: [[-0.65891188 -1.39041007 -0.91320831  8.15109444]\n",
      " [ 8.41076279 -1.99614453 -2.15307307 -1.66038573]]\n",
      "Minibatch train loss at step 993: [  3.36709549e-04   9.84620274e-05]\n",
      "Minibatch train loss mean at step 993: 0.0002175857953261584\n",
      "Minibatch train prediction at step 993: [3 0]\n",
      "Ground truth at step 993: [3 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 994: [[-3.12307596 -3.59644198  7.39311743 -1.46207821]\n",
      " [ 8.35107231 -0.57027739 -2.28273106 -1.5972079 ]]\n",
      "Minibatch train loss at step 994: [ 0.00018666  0.00020538]\n",
      "Minibatch train loss mean at step 994: 0.00019602042448241264\n",
      "Minibatch train prediction at step 994: [2 0]\n",
      "Ground truth at step 994: [2 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 995: [[-0.39637122  8.28747177 -1.44370556 -1.75868738]\n",
      " [ 8.41363716 -1.99644709 -2.15411186 -1.66180265]]\n",
      "Minibatch train loss at step 995: [  2.71998608e-04   9.79852339e-05]\n",
      "Minibatch train loss mean at step 995: 0.00018499192083254457\n",
      "Minibatch train prediction at step 995: [1 0]\n",
      "Ground truth at step 995: [1 0]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 996: [[ 8.19468307 -2.10370755 -2.56541443 -2.03889632]\n",
      " [-3.13695884 -2.37840891  7.90957737 -2.42298198]]\n",
      "Minibatch train loss at step 996: [  9.09525552e-05   8.24894232e-05]\n",
      "Minibatch train loss mean at step 996: 8.672098920214921e-05\n",
      "Minibatch train prediction at step 996: [0 2]\n",
      "Ground truth at step 996: [0 2]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 997: [[-0.39873374  8.29088402 -1.44411206 -1.7585212 ]\n",
      " [-0.66219747 -1.3917619  -0.91636366  8.1580162 ]]\n",
      "Minibatch train loss at step 997: [ 0.00027057  0.00033349]\n",
      "Minibatch train loss mean at step 997: 0.0003020302392542362\n",
      "Minibatch train prediction at step 997: [1 3]\n",
      "Ground truth at step 997: [1 3]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 998: [[ 8.19747257 -2.10471535 -2.56584144 -2.03984761]\n",
      " [-0.7002027   8.42719173 -1.21060884 -1.78007674]]\n",
      "Minibatch train loss at step 998: [  9.05949564e-05   2.10739818e-04]\n",
      "Minibatch train loss mean at step 998: 0.00015066738706082106\n",
      "Minibatch train prediction at step 998: [0 1]\n",
      "Ground truth at step 998: [0 1]\n",
      "****************************************************************************************************\n",
      "Minibatch train out at step 999: [[ 8.19221878 -0.2658101  -2.45351171 -1.56925833]\n",
      " [-0.4014273   8.29467773 -1.44457042 -1.7581861 ]]\n",
      "Minibatch train loss at step 999: [ 0.00029357  0.00026914]\n",
      "Minibatch train loss mean at step 999: 0.0002813538594637066\n",
      "Minibatch train prediction at step 999: [0 1]\n",
      "Ground truth at step 999: [0 1]\n",
      "****************************************************************************************************\n",
      "train acc: 0.5\n"
     ]
    }
   ],
   "source": [
    "# set training parameters\n",
    "num_steps = 1000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(num_steps):\n",
    "        # get data for a batch\n",
    "        batch_data = random.sample(train_set[:-2], 2)\n",
    "        batch_imgs = []\n",
    "        batch_labels = []\n",
    "        for i in range(batch_size):\n",
    "            batch_imgs.append(batch_data[i][0])\n",
    "            batch_labels.append(batch_data[i][1])\n",
    "        batch_imgs = np.stack(batch_imgs, axis=0)\n",
    "        batch_labels = np.stack(batch_labels, axis=0)\n",
    "\n",
    "        # run the 'optimizer', 'loss', and 'pred_err' operations in the graph\n",
    "        _, train_loss, train_loss_mean, train_out = sess.run(\n",
    "            [optimizer, loss, loss_mean, out], feed_dict={imgs: batch_imgs, labels: batch_labels})\n",
    "\n",
    "        # print stuff every 50 steps to see how we are doing\n",
    "        if (step % 1 == 0):\n",
    "            #pass\n",
    "            print(\"Minibatch train out at step {}: {}\".format(step, train_out))\n",
    "            print(\"Minibatch train loss at step {}: {}\".format(step, train_loss))\n",
    "            print(\"Minibatch train loss mean at step {}: {}\".format(step, train_loss_mean))\n",
    "            print(\"Minibatch train prediction at step {}: {}\".format(step, np.argmax(train_out, axis=1)))\n",
    "            print(\"Ground truth at step {}: {}\".format(step, batch_labels, axis=1))\n",
    "            print(\"*\"*100)\n",
    "            \n",
    "    # test accuracy\n",
    "    num_correct = 0\n",
    "    test_set = train_set[-2:]\n",
    "    for k in range(len(test_set)//batch_size):\n",
    "        batch_data = test_set[k:k+2]\n",
    "        batch_imgs = []\n",
    "        batch_labels = []\n",
    "        for i in range(batch_size):\n",
    "            batch_imgs.append(batch_data[i][0])\n",
    "            batch_labels.append(batch_data[i][1])\n",
    "        batch_imgs = np.stack(batch_imgs, axis=0)\n",
    "        batch_labels = np.stack(batch_labels, axis=0)\n",
    "\n",
    "        # run the 'optimizer', 'loss', and 'pred_err' operations in the graph\n",
    "        test_out = sess.run(out, feed_dict={imgs: batch_imgs})\n",
    "        num_correct += np.sum(np.argmax(test_out, axis=1) == batch_labels)\n",
    "        \n",
    "    print(\"test acc: {}\".format(num_correct/len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.24512316,  7.95199108, -1.30561435, -1.74505627],\n",
       "       [-3.13974142, -2.38035321,  7.91443634, -2.42449427]], dtype=float32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
